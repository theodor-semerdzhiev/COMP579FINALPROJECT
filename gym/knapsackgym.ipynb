{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, Tuple, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KnapsackEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for the 0-1 Knapsack Problem.\n",
    "    This version maintains a state vector of size (2*n + 4) for n items:\n",
    "      - The first 2*n slots hold pairs of (value_i, weight_i) for each item, left-aligned.\n",
    "      - The last 4 slots hold [capacity, sum(values), sum(weights), n_items].\n",
    "    After an item is chosen (whether it fits or not), it is removed from the state by\n",
    "    'shifting' all subsequent items left by 2, and placing zeros at the end.\n",
    "\n",
    "    Reward is normalized:\n",
    "      - v_r_i = v_i / (w_i * W_P) if the item fits.\n",
    "      - -w_r_i = - (w_i / W_P) if the item does not fit.\n",
    "      - -W_P if the action is out of range.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, problem_instance: dict[str, Any], N=100):\n",
    "        \"\"\"\n",
    "        Initialize the Knapsack environment with a single problem instance.\n",
    "\n",
    "        Args:\n",
    "            problem_instance (Dict): Must have:\n",
    "                - 'values': List[float] of item values\n",
    "                - 'weights': List[float] of item weights\n",
    "                - 'capacity': float (max knapsack capacity)\n",
    "            \n",
    "                N: Max number of items that a knapsack can have\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = N\n",
    "        self.change_problem_instance(problem_instance)\n",
    "\n",
    "        # Internal tracking\n",
    "        # We'll store (value, weight, idx) so we know which normalized entry to use.\n",
    "        self.items: List[Tuple[float, float, int]] = []\n",
    "        self.remaining_capacity = 0.0\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "        self.best_value = 0.0\n",
    "\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def change_problem_instance(self, problem_instance:dict[str, Any]) -> None:\n",
    "        assert len(problem_instance['values']) ==  len(problem_instance['weights']), \"Weights and Values must match\"\n",
    "        assert len(problem_instance['values']) != 0, \"KnapsackEnv canont take empty KP instances\"\n",
    "        assert self.N >= len(problem_instance['values']), f\"KnapsackEnv cannot take KP instances larger than {self.N}. Expected: <= {self.N}, got {len(problem_instance['values'])}\"\n",
    "\n",
    "        self.problem_instance = problem_instance\n",
    "\n",
    "        # Extract basic info\n",
    "        self.values = list(problem_instance['values'])\n",
    "        self.weights = list(problem_instance['weights'])\n",
    "        self.capacity = float(problem_instance['capacity'])\n",
    "        self.n_items = len(self.values)\n",
    "\n",
    "        self.total_value = sum(self.values)\n",
    "        self.total_weight = sum(self.weights)\n",
    "\n",
    "        # Precompute normalized values and weights for the reward function\n",
    "        # v_r_i = v_i / (w_i * capacity), w_r_i = w_i / capacity\n",
    "        self.normalized_values = []\n",
    "        self.normalized_weights = []\n",
    "        for v, w in zip(self.values, self.weights):\n",
    "            if w == 0.0:\n",
    "                # Avoid division by zero; you can choose an alternate scheme\n",
    "                # for zero-weight items (e.g., treat as v / (1 * capacity))\n",
    "                self.normalized_values.append(0.0)\n",
    "            else:\n",
    "                self.normalized_values.append(v / (w * self.capacity))\n",
    "            self.normalized_weights.append(w / self.capacity)\n",
    "\n",
    "        # Define the action space: pick an index [0, n_items - 1]\n",
    "        self.action_space = spaces.Discrete(self.n_items)\n",
    "\n",
    "        # Observation space: 2*n for (value, weight) pairs + 4\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(2 * self.n_items + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reset the environment state and return the initial observation.\n",
    "        \"\"\"\n",
    "        # Re-initialize the item list with all items + their original indices\n",
    "        self.items = [\n",
    "            (v, w, i) for i, (v, w) in enumerate(zip(self.values, self.weights))\n",
    "        ]\n",
    "        self.remaining_capacity = self.capacity\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "        # self.best_value = 0.0  # Uncomment if you want to reset best_value each episode\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build the state vector of length (2*n_items + 4).\n",
    "          - The first 2*len(self.items) slots: (value_i, weight_i) for each remaining item (left-aligned).\n",
    "          - The next 2*(n_items - len(self.items)) slots: 0.0\n",
    "          - The last 4 slots: [capacity, total_value, total_weight, n_items].\n",
    "        \"\"\"\n",
    "        # state = np.zeros(2 * self.n_items + 4, dtype=np.float32)\n",
    "        state = np.zeros(2 * self.N + 4, dtype=np.float32)\n",
    "\n",
    "        # Fill in item data for remaining items (left-aligned)\n",
    "        for i, (v, w, idx) in enumerate(self.items):\n",
    "            pos = 2 * i\n",
    "            state[pos] = v\n",
    "            state[pos + 1] = w\n",
    "\n",
    "        # The last 4 features\n",
    "        state[-4] = self.capacity\n",
    "        state[-3] = self.total_value\n",
    "        state[-2] = self.total_weight\n",
    "        state[-1] = self.n_items\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Take an action (select an item index) and return (new_state, reward, done, info).\n",
    "        Reward logic:\n",
    "          1) If action is out of range => reward = -W_P.\n",
    "          2) Else pick the item; if it fits => reward = v_r_i; else => reward = - w_r_i.\n",
    "          3) Remove the item from self.items in all cases (shifting the state).\n",
    "        End the episode if no items remain or if capacity is exhausted.\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "\n",
    "        # Out-of-range or invalid action => large negative penalty\n",
    "        if action < 0 or action >= len(self.items):\n",
    "            reward = -self.capacity\n",
    "            self.done = True\n",
    "        else:\n",
    "            # Retrieve the chosen item\n",
    "            item_value, item_weight, item_idx = self.items[action]\n",
    "\n",
    "            # Check if it fits in the *remaining* capacity\n",
    "            if item_weight <= self.remaining_capacity:\n",
    "                # Positive reward: v_r_i = v_i / (w_i * W_P)\n",
    "                reward = self.normalized_values[item_idx]\n",
    "\n",
    "                # Update current knapsack usage\n",
    "                self.current_value += item_value\n",
    "                self.current_weight += item_weight\n",
    "                self.remaining_capacity -= item_weight\n",
    "            else:\n",
    "                # Negative reward: - w_r_i = - (w_i / W_P)\n",
    "                reward = -self.normalized_weights[item_idx]\n",
    "\n",
    "            # Remove this item from the state (pop shifts everything to the left).\n",
    "            self.items.pop(action)\n",
    "\n",
    "        # Check if we should end the episode\n",
    "        if len(self.items) == 0 or self.remaining_capacity == 0:\n",
    "            self.done = True\n",
    "            # Update best_value if the current solution is better\n",
    "            if self.current_value > self.best_value:\n",
    "                self.best_value = self.current_value\n",
    "\n",
    "        # Build the new state\n",
    "        new_state = self._get_state()\n",
    "\n",
    "        info = {\n",
    "            'current_value': self.current_value,\n",
    "            'current_weight': self.current_weight,\n",
    "            'remaining_capacity': self.remaining_capacity,\n",
    "            'best_value': self.best_value,\n",
    "            'items_remaining': len(self.items),\n",
    "        }\n",
    "\n",
    "        return new_state, reward, self.done, info\n",
    "\n",
    "    def render(self, mode='human') -> None:\n",
    "        \"\"\"\n",
    "        Print out the environment state for debugging.\n",
    "        \"\"\"\n",
    "        if mode == 'human':\n",
    "            print(\"Knapsack Environment\")\n",
    "            print(f\"Capacity: {self.capacity}, Remaining: {self.remaining_capacity}\")\n",
    "            print(f\"Current value: {self.current_value}, Current weight: {self.current_weight}\")\n",
    "            print(\"Items left (value, weight, idx):\")\n",
    "            for it in self.items:\n",
    "                print(\"  \", it)\n",
    "            print(f\"Best value so far: {self.best_value}\")\n",
    "            print(f\"Done: {self.done}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 50.0\n",
      "Current value: 0.0, Current weight: 0.0\n",
      "Items left (value, weight, idx):\n",
      "   (60, 10, 0)\n",
      "   (100, 20, 1)\n",
      "   (120, 40, 2)\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Action: 0 State: [ 60.  10. 100.  20. 120.  40.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  50. 280.  70.   3.]\n",
      "Reward: 0.12\n",
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 40.0\n",
      "Current value: 60.0, Current weight: 10.0\n",
      "Items left (value, weight, idx):\n",
      "   (100, 20, 1)\n",
      "   (120, 40, 2)\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Action: 0 State: [100.  20. 120.  40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  50. 280.  70.   3.]\n",
      "Reward: 0.1\n",
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 20.0\n",
      "Current value: 160.0, Current weight: 30.0\n",
      "Items left (value, weight, idx):\n",
      "   (120, 40, 2)\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Action: 2 State: [120.  40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  50. 280.  70.   3.]\n",
      "Reward: -50.0\n",
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 20.0\n",
      "Current value: 160.0, Current weight: 30.0\n",
      "Items left (value, weight, idx):\n",
      "   (120, 40, 2)\n",
      "Best value so far: 0.0\n",
      "Done: True\n",
      "--------------------------------------------------\n",
      "Episode finished with total reward: -49.78\n",
      "Final info: {'current_value': 160.0, 'current_weight': 30.0, 'remaining_capacity': 20.0, 'best_value': 0.0, 'items_remaining': 1}\n"
     ]
    }
   ],
   "source": [
    "def run_episode_test(env:KnapsackEnv, policy=None, render=False, max_ite = None):\n",
    "    \"\"\"\n",
    "    Run a single episode in a Gym environment.\n",
    "\n",
    "    Args:\n",
    "        env: An instance of a Gym environment.\n",
    "        policy: Optional function that takes the current state and returns an action.\n",
    "                If None, actions are chosen randomly.\n",
    "        render: Boolean flag to indicate whether to render the environment at each step.\n",
    "\n",
    "    Returns:\n",
    "        total_reward (float): The total accumulated reward from the episode.\n",
    "        episode_info (dict): The final info dictionary returned by the environment.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    ite = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        # Choose an action: use the policy if provided, otherwise sample randomly.\n",
    "        action = policy(state) if policy is not None else env.action_space.sample()\n",
    "        print(\"Action:\" , action, \"State:\", state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        print(\"Reward:\", reward)\n",
    "\n",
    "\n",
    "        total_reward += reward\n",
    "        ite += 1\n",
    "        if max_ite is not None and ite < max_ite:\n",
    "            break\n",
    "\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    return total_reward, info\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple problem instance\n",
    "    problem_instance = {\n",
    "        'values': [60, 100, 120],\n",
    "        'weights': [10, 20, 40],\n",
    "        'capacity': 50\n",
    "    }\n",
    "\n",
    "    # Create the Knapsack environment (assuming KnapsackEnv is already defined)\n",
    "    env = KnapsackEnv(problem_instance)\n",
    "\n",
    "    # Run an episode using a random policy (since no policy is provided)\n",
    "    total_reward, final_info = run_episode_test(env, render=True)\n",
    "    print(\"Episode finished with total reward:\", total_reward)\n",
    "    print(\"Final info:\", final_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
