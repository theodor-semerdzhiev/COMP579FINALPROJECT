# COMP579 Final Project ‚Äì Deep Reinforcement Learning for 0-1 Knapsack Optimization

This project applies Deep Reinforcement Learning (DRL) techniques to solve the classical **0-1 Knapsack Problem**, a well-known NP-hard combinatorial optimization task. We re-implemented and extended the research by **Afshar et al. (2019)**, who proposed a state aggregation method for improving DRL efficiency on the knapsack problem. Our objective was to reproduce, analyze, and enhance their methodology by integrating and comparing several RL architectures including **A2C**, **DQN**, and **PPO** within a custom OpenAI Gym environment.

---

## üìå Project Highlights

- ‚úÖ Custom Gym environment for simulating knapsack dynamics  
- ‚úÖ Modular design using an abstract policy interface  
- ‚úÖ Implementations of DRL algorithms: A2C, PPO, and DQN  
- ‚úÖ Baseline solvers: Dynamic Programming and Greedy Heuristic  
- ‚úÖ Support for state aggregation to reduce state dimensionality  
- ‚úÖ Extensive experiment suite across diverse knapsack instance types  

---

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ environment/
‚îÇ   ‚îî‚îÄ‚îÄ knapsackgym.py             # Custom Gym environment for 0-1 KP
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ AbstractKnapsackPolicy.py  # Base class for all policy implementations
‚îÇ   ‚îú‚îÄ‚îÄ KnapsackA2C.py             # Advantage Actor-Critic
‚îÇ   ‚îú‚îÄ‚îÄ KnapsackPPO.py             # Proximal Policy Optimization
‚îÇ   ‚îú‚îÄ‚îÄ KnapsackQLearning.py       # Deep Q-Network (DQN)
‚îÇ   ‚îú‚îÄ‚îÄ DP_Knapsack.py             # Dynamic Programming baseline
‚îÇ   ‚îú‚îÄ‚îÄ Greedy_Knapsack.py         # Greedy heuristic baseline
‚îÇ   ‚îî‚îÄ‚îÄ StateAggregator.py         # Q-learning based state aggregation
‚îÇ
‚îú‚îÄ‚îÄ solver/
‚îÇ   ‚îú‚îÄ‚îÄ KnapsackDRLSolver.py       # Solver that coordinates training + evaluation
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îî‚îÄ‚îÄ experiment.ipynb           # Final experiments and result analysis
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îî‚îÄ‚îÄ README.md                      # This file
```

---

## üîÑ System Flow Overview

This diagram illustrates the overall flow of the training pipeline:

![Flow Diagram](docs/flowchart.png)

> Components:
> - `knapsackEnv(knapsackgym.py)` defines the environment.
> - `DRLSolver(KnapsackDRLSolver.py)` manages training and evaluation.
> - `A2C/DQN/PPO(AbstractKnapsackPolicy.py)` is extended by each RL agent (A2C, PPO, DQN).
> - Each model logs checkpoints and performance.

---

## üß± UML Class Diagram

The following UML diagram shows how the main components and classes interact:

![UML Diagram](docs/uml_diagram.png)

> Arrows indicate class dependency and usage relationships.

---

## üöÄ How to Run the Code

### 1. Clone the Repository
```bash
git clone https://github.com/theodor-semerdzhiev/COMP579FINALPROJECT.git
cd COMP579FINALPROJECT
```

### 2. Create a Python Virtual Environment

#### Option A: VSCode
1. Open the project folder in VSCode.
2. Press `Ctrl + Shift + P` (or `Cmd + Shift + P` on macOS).
3. Choose "Python: Create Environment".

#### Option B: Manual Setup
```bash
python -m venv .venv
```

- **Windows**:  
  ```bash
  .venv\Scripts\activate
  ```

- **macOS/Linux**:  
  ```bash
  source .venv/bin/activate
  ```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Run Training or Experiments

You can explore experiments interactively:
- `solver/KnapsackDRLSolverTest.ipynb`
- `experiments/experiment_table1.ipynb`

Or run training directly:
```bash
python solver/KnapsackDRLSolver.py
```

---

## üìö Reference

Afshar, R. R., Zhang, Y., Firat, M., & Kaymak, U. (2019).  
*A State Aggregation Approach for Solving Knapsack Problem with Deep Reinforcement Learning.*  
**Asian Conference on Machine Learning (ACML)**, PMLR Vol. 129, pp. 81‚Äì96.  
[Link](https://proceedings.mlr.press/v129/afshar20a.html)

---

## üë®‚Äçüíª Authors

- **Theodor Semerdzhiev** ‚Äì 261118892 ‚Äì [theodor.semerdzhiev@mail.mcgill.ca](mailto:theodor.semerdzhiev@mail.mcgill.ca)  
- **Bohan Wang** ‚Äì 261023725 ‚Äì [bohan.wang@mail.mcgill.ca](mailto:bohan.wang@mail.mcgill.ca)  
- **Mikhail Lavrenov** ‚Äì 261096558 ‚Äì [mikhail.lavrenov@mail.mcgill.ca](mailto:mikhail.lavrenov@mail.mcgill.ca)
