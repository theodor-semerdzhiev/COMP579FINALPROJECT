# COMP579 Final Project â€“ Deep Reinforcement Learning for 0-1 Knapsack Optimization

This project applies Deep Reinforcement Learning (DRL) techniques to solve the classical **0-1 Knapsack Problem**, a well-known NP-hard combinatorial optimization task. We re-implemented and extended the research by **Afshar et al. (2019)**, who proposed a state aggregation method for improving DRL efficiency on the knapsack problem. Our objective was to reproduce, analyze, and enhance their methodology by integrating and comparing several RL architectures including **A2C**, **DQN**, and **PPO** within a custom OpenAI Gym environment.

---

## ğŸ“Œ Project Highlights

- âœ… Custom Gym environment for simulating knapsack dynamics  
- âœ… Modular design using an abstract policy interface  
- âœ… Implementations of DRL algorithms: A2C, PPO, and DQN  
- âœ… Baseline solvers: Dynamic Programming and Greedy Heuristic  
- âœ… Support for state aggregation to reduce state dimensionality  
- âœ… Extensive experiment suite across diverse knapsack instance types  

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ environment/
â”‚   â””â”€â”€ knapsackgym.py             # Custom Gym environment for 0-1 KP
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ AbstractKnapsackPolicy.py  # Base class for all policy implementations
â”‚   â”œâ”€â”€ KnapsackA2C.py             # Advantage Actor-Critic
â”‚   â”œâ”€â”€ KnapsackPPO.py             # Proximal Policy Optimization
â”‚   â”œâ”€â”€ KnapsackQLearning.py       # Deep Q-Network (DQN)
â”‚   â”œâ”€â”€ DP_Knapsack.py             # Dynamic Programming baseline
â”‚   â”œâ”€â”€ Greedy_Knapsack.py         # Greedy heuristic baseline
â”‚   â””â”€â”€ StateAggregator.py         # Q-learning based state aggregation
â”‚
â”œâ”€â”€ solver/
â”‚   â”œâ”€â”€ KnapsackDRLSolver.py       # Solver that coordinates training + evaluation
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ experiment.ipynb           # Final experiments and result analysis
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸ”„ System Flow Overview

This diagram illustrates the overall flow of the training pipeline:

<img src="docs/flowchart.png" width="200" alt="Flow Diagram">

> Components:
> - `knapsackEnv(knapsackgym.py)` defines the environment.
> - `DRLSolver(KnapsackDRLSolver.py)` manages training and evaluation.
> - `A2C/DQN/PPO(AbstractKnapsackPolicy.py)` is extended by each RL agent (A2C, PPO, DQN).
> - Each model logs checkpoints and performance.

---

## ğŸ§± UML Class Diagram

The following UML diagram shows how the main components and classes interact:

<img src="docs/uml_diagram.png" width="600" alt="UML Diagram">

> Arrows indicate class dependency and usage relationships.

---

## ğŸš€ How to Run the Code

### 1. Clone the Repository
```bash
git clone https://github.com/theodor-semerdzhiev/COMP579FINALPROJECT.git
cd COMP579FINALPROJECT
```

### 2. Create a Python Virtual Environment

#### Option A: VSCode
1. Open the project folder in VSCode.
2. Press `Ctrl + Shift + P` (or `Cmd + Shift + P` on macOS).
3. Choose "Python: Create Environment".

#### Option B: Manual Setup
```bash
python -m venv .venv
```

- **Windows**:  
  ```bash
  .venv\Scripts\activate
  ```

- **macOS/Linux**:  
  ```bash
  source .venv/bin/activate
  ```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Run Training or Experiments

You can explore experiments interactively:
- `solver/KnapsackDRLSolverTest.ipynb`
- `experiments/experiment_table1.ipynb`

Or run training directly:
```bash
python solver/KnapsackDRLSolver.py
```

---

## ğŸ“š Reference

Afshar, R. R., Zhang, Y., Firat, M., & Kaymak, U. (2019).  
*A State Aggregation Approach for Solving Knapsack Problem with Deep Reinforcement Learning.*  
**Asian Conference on Machine Learning (ACML)**, PMLR Vol. 129, pp. 81â€“96.  
[Link](https://proceedings.mlr.press/v129/afshar20a.html)

---

## ğŸ‘¨â€ğŸ’» Authors

- **Theodor Semerdzhiev** â€“ 261118892 â€“ [theodor.semerdzhiev@mail.mcgill.ca](mailto:theodor.semerdzhiev@mail.mcgill.ca)  
- **Bohan Wang** â€“ 261023725 â€“ [bohan.wang@mail.mcgill.ca](mailto:bohan.wang@mail.mcgill.ca)  
- **Mikhail Lavrenov** â€“ 261096558 â€“ [mikhail.lavrenov@mail.mcgill.ca](mailto:mikhail.lavrenov@mail.mcgill.ca)
