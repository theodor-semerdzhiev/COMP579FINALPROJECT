{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knapsackgym import KnapsackEnv\n",
    "from DP_Knapsack import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "from AbstractKnapsackPolicy import AbstractKnapsackPolicy\n",
    "from A2C import KnapsackA2C\n",
    "from StateAggregator import StateAggregator\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main KnapsackDRLSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KnapsackDRLSolver:\n",
    "    \"\"\"DRL-based Knapsack Solver using A2C algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, env, KPsolver:AbstractKnapsackPolicy, use_state_aggregation=False, gamma=0.99, lr_policy=0.001, lr_value=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize the DRL solver.\n",
    "        \n",
    "        Args:\n",
    "            env: Gym environment for the knapsack problem\n",
    "            N (int): Maximum number of items in a problem instance\n",
    "            use_state_aggregation (bool): Whether to use state aggregation\n",
    "            gamma (float): Discount factor\n",
    "            lr_policy (float): Learning rate for policy network\n",
    "            lr_value (float): Learning rate for value network\n",
    "        \"\"\"\n",
    "        self.env:KnapsackEnv = env\n",
    "        self.N = KPsolver.N\n",
    "        self.use_state_aggregation = use_state_aggregation\n",
    "        \n",
    "        \n",
    "        # Initialize state aggregator if needed\n",
    "        self.state_aggregator = StateAggregator() if use_state_aggregation else None\n",
    "        self.KPsolver = KPsolver\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def process_state(self, state):\n",
    "        \"\"\"\n",
    "        Process state with optional aggregation.\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Original state\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Processed state\n",
    "        \"\"\"\n",
    "        if self.use_state_aggregation and self.state_aggregator is not None:\n",
    "            return self.state_aggregator.aggregate(state)\n",
    "        return state\n",
    "    \n",
    "    def train(self, problem_instances, t_max=None):\n",
    "        \"\"\"\n",
    "        Train the DRL solver on multiple problem instances with progress tracking.\n",
    "        \n",
    "        Args:\n",
    "            problem_instances (List[Dict]): List of problem instances\n",
    "            t_max (int): Maximum number of training steps\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: Values of solutions for each problem instance\n",
    "        \"\"\"\n",
    "        assert len(problem_instances) is not None or len(problem_instances) > 0\n",
    "\n",
    "        if t_max is None:\n",
    "            t_max = 3 * self.N * 10000  # As specified in the pseudocode\n",
    "        \n",
    "        val = [0] * len(problem_instances)  # Initialize solution values\n",
    "        \n",
    "        print(f\"Training on {len(problem_instances)} KP Instances, with N={self.N}, t_max={t_max}\")\n",
    "        for t in range(t_max):\n",
    "\n",
    "            # Select a problem instance (line 6 in pseudocode)\n",
    "            # P_idx = np.random.randint(0, len(problem_instances))\n",
    "            P_idx = t % len(problem_instances)\n",
    "            P = problem_instances[P_idx]\n",
    "            \n",
    "            assert len(P['values'] )<= self.N, f\"Problem Instance has too many items. KnapsackEnv is configuered to accept no more than <= {self.N}.\"\n",
    "\n",
    "            # Change the environment to use this problem instance\n",
    "            self.env.change_problem_instance(P)\n",
    "            \n",
    "            # Reset environment and get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # Initialize for this episode\n",
    "            done = False\n",
    "            ow = 0  # Total weight of selected items\n",
    "            ov = 0  # Total value of selected items\n",
    "            \n",
    "            # Create a copy of the problem instance P for modification\n",
    "            n_P_prime = len(P['values'])\n",
    "            W_P_prime = P['capacity']\n",
    "            \n",
    "            # Store states, actions, rewards for batch update\n",
    "            states:List[np.ndarray] = []\n",
    "            actions:List[int] = []\n",
    "            rewards:List[float] = []\n",
    "            next_states:List[np.ndarray] = []\n",
    "            dones:List[bool] = []\n",
    "            \n",
    "            # Track episode rewards\n",
    "            episode_rewards:List[float] = []\n",
    "            \n",
    "            # Solve the knapsack problem for this instance\n",
    "            while ow < W_P_prime and n_P_prime > 0 and not done:\n",
    "                # Process state if needed\n",
    "                processed_state = self.process_state(state)\n",
    "                states.append(processed_state)\n",
    "                \n",
    "                # Get available actions (indices of remaining items)\n",
    "                available_actions = list(range(len(self.env.items)))\n",
    "                \n",
    "                # Get action according to policy (line 12 in pseudocode)\n",
    "                action = self.KPsolver.get_action(processed_state, available_actions)\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Take action and observe reward and next state\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                episode_rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "                \n",
    "                # Check if item fits in knapsack (line 13 in pseudocode)\n",
    "                if action < n_P_prime and info['current_weight'] - ow <= W_P_prime:\n",
    "                    # Update totals (line 14 in pseudocode)\n",
    "                    ow = info['current_weight']\n",
    "                    ov = info['current_value']\n",
    "                    W_P_prime = P['capacity'] - ow\n",
    "                \n",
    "                # Update P_prime (line 16 in pseudocode)\n",
    "                n_P_prime -= 1\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "            \n",
    "            # Update parameters using collected trajectories\n",
    "            self.KPsolver.update_parameters(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            if self.verbose and t % 1000 == 0:\n",
    "                print(f\"Iteration [{t}/{t_max}], Training KP Instance {P_idx}, Reward: {sum(rewards) / len(rewards)}\")\n",
    "\n",
    "\n",
    "            # Update best value if needed (lines 20-22 in pseudocode)\n",
    "            if ov > val[P_idx]:\n",
    "                val[P_idx] = ov\n",
    "            \n",
    "        return val\n",
    "    \n",
    "    \n",
    "    def solve(self, problem_instance):\n",
    "        \"\"\"\n",
    "        Solve a single knapsack problem instance using the trained policy.\n",
    "        \n",
    "        Args:\n",
    "            problem_instance (Dict): A problem instance\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, List[int]]: Total value and list of selected item indices\n",
    "        \"\"\"\n",
    "        # Set environment to use this problem instance\n",
    "        self.env.change_problem_instance(problem_instance)\n",
    "        \n",
    "        # Reset environment and get initial state\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        done = False\n",
    "        total_value = 0\n",
    "        total_weight = 0\n",
    "        selected_items = []\n",
    "        \n",
    "        while not done:\n",
    "            # Process state if needed\n",
    "            processed_state = self.process_state(state)\n",
    "            \n",
    "            # Get available actions (indices of remaining items)\n",
    "            available_actions = list(range(len(self.env.items)))\n",
    "            \n",
    "            if not available_actions:\n",
    "                break\n",
    "                \n",
    "            # Get action according to policy\n",
    "            action = self.KPsolver.get_action(processed_state, available_actions)\n",
    "            \n",
    "            # Get original item index (before any removal)\n",
    "            original_item_idx = self.env.items[action][2]\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # If item was added (positive reward means item fit)\n",
    "            if reward > 0:\n",
    "                selected_items.append(original_item_idx)\n",
    "            \n",
    "            # Update value and weight\n",
    "            total_value = info['current_value']\n",
    "            total_weight = info['current_weight']\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        return total_value, selected_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_knapsack_solver(env, problem_instances:List[Dict[str, Any]], KPsolver, use_state_aggregation=False, t_max=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a DRL-based knapsack solver on multiple problem instances.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment for knapsack problem\n",
    "        problem_instances (List[Dict]): List of problem instances\n",
    "        N (int): Maximum number of items in a problem instance\n",
    "        use_state_aggregation (bool): Whether to use state aggregation\n",
    "        gamma (float): Discount factor\n",
    "        lr_policy (float): Learning rate for policy network\n",
    "        lr_value (float): Learning rate for value network\n",
    "        t_max (int): Maximum number of training steps\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[KnapsackDRLSolver, List[float]]: Trained solver and solution values\n",
    "    \"\"\"\n",
    "    # Initialize solver\n",
    "    solver = KnapsackDRLSolver(\n",
    "        env=env,\n",
    "        KPsolver=KPsolver,\n",
    "        use_state_aggregation=use_state_aggregation,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Train solver\n",
    "    solution_values = solver.train(problem_instances, t_max)\n",
    "\n",
    "    return solver, solution_values\n",
    "\n",
    "\n",
    "def evaluate_knapsack_solver(solver:KnapsackDRLSolver, test_instances:list[dict]):\n",
    "    \"\"\"\n",
    "    Evaluate the trained solver on test instances.\n",
    "    \n",
    "    Args:\n",
    "        solver (KnapsackDRLSolver): Trained solver\n",
    "        test_instances (List[Dict]): List of test problem instances\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Evaluation results for each test instance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, instance in enumerate(test_instances):\n",
    "        # Solve instance\n",
    "        value, selected_items = solver.solve(instance)\n",
    "        \n",
    "        # Calculate actual total weight\n",
    "        total_weight = sum(instance['weights'][idx] for idx in selected_items)\n",
    "        \n",
    "        # Check if solution respects capacity constraint\n",
    "        is_valid = total_weight <= instance['capacity']\n",
    "\n",
    "        optimal_value, optimal_items = solve_knapsack_dp(instance)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'instance_idx': i,\n",
    "            'total_value': value,\n",
    "            'total_weight': total_weight,\n",
    "            'capacity': instance['capacity'],\n",
    "            'selected_items': selected_items,\n",
    "            'is_valid': is_valid,\n",
    "            \"optimal_sol\": optimal_value,\n",
    "            \"optimal_items\": optimal_items,\n",
    "            \"optimality_ratio\": value /  optimal_value \n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.19999999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsemerdz/School/COMP579_Assignments/A2C.py:233: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  states_tensor = torch.FloatTensor(states)\n",
      "/Users/tsemerdz/School/COMP579_Assignments/A2C.py:259: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(state_values, returns_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.29166666666666663\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Trained solution values: [54.0, 64.0]\n",
      "Optimal solution values: [np.float64(59.0), np.float64(64.0)]\n",
      "Evaluation results:\n",
      "{'instance_idx': 0, 'total_value': 40.0, 'total_weight': 20, 'capacity': 20, 'selected_items': [2, 0], 'is_valid': True, 'optimal_sol': np.float64(40.0), 'optimal_items': [0, 2], 'optimality_ratio': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create problem instances\n",
    "    problem_instances = [\n",
    "        {\n",
    "            'values': [10, 5, 15, 7, 6, 18, 3, 20],\n",
    "            'weights': [2, 3, 5, 7, 1, 4, 1, 5],\n",
    "            'capacity': 15\n",
    "        },\n",
    "        {\n",
    "            'values': [20, 30, 15, 25, 10, 14, 2, 6],\n",
    "            'weights': [5, 10, 8, 12, 4, 2, 5, 20],\n",
    "            'capacity': 20\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    N = 20\n",
    "    gamma = 0.99\n",
    "    t_max = 10000\n",
    "    env:KnapsackEnv = KnapsackEnv(problem_instance=None, N=N)\n",
    "\n",
    "    KPSolver = KnapsackA2C(N=N, gamma=gamma, lr_policy=0.001, lr_value=0.001, verbose=True)\n",
    "    # Train solver\n",
    "    solver, solution_values = train_knapsack_solver(\n",
    "        env=env,\n",
    "        problem_instances=problem_instances,\n",
    "        KPsolver=KPSolver,\n",
    "        use_state_aggregation=False,\n",
    "        t_max=t_max, # Set a smaller value for testing\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    optimal_sol = solve_KP_instances_with_DP(problem_instances)\n",
    "    \n",
    "    print(\"Trained solution values:\", solution_values)\n",
    "    print(\"Optimal solution values:\", optimal_sol)\n",
    "    \n",
    "    # Evaluate on test instances\n",
    "    test_instances = [\n",
    "        {\n",
    "            'values': [10, 20, 30],\n",
    "            'weights': [5, 10, 15],\n",
    "            'capacity': 20\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    solver.verbose = False\n",
    "    results = evaluate_knapsack_solver(solver, test_instances)\n",
    "    print(\"Evaluation results:\")\n",
    "    for res in results:\n",
    "        print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
