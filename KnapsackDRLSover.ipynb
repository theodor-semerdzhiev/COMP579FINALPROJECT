{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knapsackgym import KnapsackEnv\n",
    "from AbstractKnapsackPolicy import AbstractKnapsackPolicy\n",
    "from StateAggregator import StateAggregator\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from DP_Knapsack import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main KnapsackDRLSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackDRLSolver:\n",
    "    \"\"\"DRL-based Knapsack Solver using A2C algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, env, KPsolver:AbstractKnapsackPolicy, use_state_aggregation=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize the DRL solver.\n",
    "        \n",
    "        Args:\n",
    "            env: Gym environment for the knapsack problem\n",
    "            N (int): Maximum number of items in a problem instance\n",
    "            use_state_aggregation (bool): Whether to use state aggregation\n",
    "            gamma (float): Discount factor\n",
    "            lr_policy (float): Learning rate for policy network\n",
    "            lr_value (float): Learning rate for value network\n",
    "        \"\"\"\n",
    "        self.env:KnapsackEnv = env\n",
    "        self.N = KPsolver.N\n",
    "        self.use_state_aggregation = use_state_aggregation\n",
    "        \n",
    "        \n",
    "        # Initialize state aggregator if needed\n",
    "        self.state_aggregator = StateAggregator() if use_state_aggregation else None\n",
    "        self.KPsolver = KPsolver\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def process_state(self, state):\n",
    "        \"\"\"\n",
    "        Process state with optional aggregation.\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Original state\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Processed state\n",
    "        \"\"\"\n",
    "        if self.use_state_aggregation and self.state_aggregator is not None:\n",
    "            return self.state_aggregator.aggregate(state)\n",
    "        return state\n",
    "    \n",
    "    def train(self, problem_instances, t_max=None):\n",
    "        \"\"\"\n",
    "        Train the DRL solver on multiple problem instances with progress tracking.\n",
    "        \n",
    "        Args:\n",
    "            problem_instances (List[Dict]): List of problem instances\n",
    "            t_max (int): Maximum number of training steps\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: Values of solutions for each problem instance\n",
    "        \"\"\"\n",
    "        assert len(problem_instances) is not None or len(problem_instances) > 0\n",
    "\n",
    "        if t_max is None:\n",
    "            t_max = 3 * self.N * 10000  # As specified in the pseudocode\n",
    "        \n",
    "        val = [0] * len(problem_instances)  # Initialize solution values\n",
    "        \n",
    "        print(f\"Training on {len(problem_instances)} KP Instances, with N={self.N}, t_max={t_max}\")\n",
    "        for t in range(t_max):\n",
    "\n",
    "            # Select a problem instance (line 6 in pseudocode)\n",
    "            # P_idx = np.random.randint(0, len(problem_instances))\n",
    "            P_idx = t % len(problem_instances)\n",
    "            P = problem_instances[P_idx]\n",
    "            \n",
    "            assert len(P['values'] )<= self.N, f\"Problem Instance has too many items. KnapsackEnv is configuered to accept no more than <= {self.N}.\"\n",
    "\n",
    "            # Change the environment to use this problem instance\n",
    "            self.env.change_problem_instance(P)\n",
    "            \n",
    "            # Reset environment and get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # Initialize for this episode\n",
    "            done = False\n",
    "            ow = 0  # Total weight of selected items\n",
    "            ov = 0  # Total value of selected items\n",
    "            \n",
    "            # Create a copy of the problem instance P for modification\n",
    "            n_P_prime = len(P['values'])\n",
    "            W_P_prime = P['capacity']\n",
    "            \n",
    "            # Store states, actions, rewards for batch update\n",
    "            states:List[np.ndarray] = []\n",
    "            actions:List[int] = []\n",
    "            rewards:List[float] = []\n",
    "            next_states:List[np.ndarray] = []\n",
    "            dones:List[bool] = []\n",
    "            \n",
    "            # Track episode rewards\n",
    "            episode_rewards:List[float] = []\n",
    "            \n",
    "            # Solve the knapsack problem for this instance\n",
    "            while ow < W_P_prime and n_P_prime > 0 and not done:\n",
    "                # Process state if needed\n",
    "                processed_state = self.process_state(state)\n",
    "                states.append(processed_state)\n",
    "                \n",
    "                # Get available actions (indices of remaining items)\n",
    "                available_actions = list(range(len(self.env.items)))\n",
    "                \n",
    "                # Get action according to policy (line 12 in pseudocode)\n",
    "                action = self.KPsolver.get_action(processed_state, available_actions)\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Take action and observe reward and next state\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                episode_rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "                \n",
    "                # Check if item fits in knapsack (line 13 in pseudocode)\n",
    "                if action < n_P_prime and info['current_weight'] - ow <= W_P_prime:\n",
    "                    # Update totals (line 14 in pseudocode)\n",
    "                    ow = info['current_weight']\n",
    "                    ov = info['current_value']\n",
    "                    W_P_prime = P['capacity'] - ow\n",
    "                \n",
    "                # Update P_prime (line 16 in pseudocode)\n",
    "                n_P_prime -= 1\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "            \n",
    "            # Update parameters using collected trajectories\n",
    "            self.KPsolver.update_parameters(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            if self.verbose and t % 1000 == 0:\n",
    "                print(f\"Iteration [{t}/{t_max}], Training KP Instance {P_idx}, Reward: {sum(rewards) / len(rewards)}\")\n",
    "\n",
    "\n",
    "            # Update best value if needed (lines 20-22 in pseudocode)\n",
    "            if ov > val[P_idx]:\n",
    "                val[P_idx] = ov\n",
    "            \n",
    "        return val\n",
    "    \n",
    "    \n",
    "    def solve(self, problem_instance):\n",
    "        \"\"\"\n",
    "        Solve a single knapsack problem instance using the trained policy.\n",
    "        \n",
    "        Args:\n",
    "            problem_instance (Dict): A problem instance\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, List[int]]: Total value and list of selected item indices\n",
    "        \"\"\"\n",
    "        # Set environment to use this problem instance\n",
    "        self.env.change_problem_instance(problem_instance)\n",
    "        \n",
    "        # Reset environment and get initial state\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        done = False\n",
    "        total_value = 0\n",
    "        total_weight = 0\n",
    "        selected_items = []\n",
    "        \n",
    "        while not done:\n",
    "            # Process state if needed\n",
    "            processed_state = self.process_state(state)\n",
    "            \n",
    "            # Get available actions (indices of remaining items)\n",
    "            available_actions = list(range(len(self.env.items)))\n",
    "            \n",
    "            if not available_actions:\n",
    "                break\n",
    "                \n",
    "            # Get action according to policy\n",
    "            action = self.KPsolver.get_action(processed_state, available_actions)\n",
    "            \n",
    "            # Get original item index (before any removal)\n",
    "            original_item_idx = self.env.items[action][2]\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # If item was added (positive reward means item fit)\n",
    "            if reward > 0:\n",
    "                selected_items.append(original_item_idx)\n",
    "            \n",
    "            # Update value and weight\n",
    "            total_value = info['current_value']\n",
    "            total_weight = info['current_weight']\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        return total_value, selected_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Greedy_Knapsack import greedy_knapsack, solve_problem_instances_greedy\n",
    "\n",
    "\n",
    "def train_knapsack_solver(env, problem_instances:List[Dict[str, Any]], KPsolver, use_state_aggregation=False, t_max=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a DRL-based knapsack solver on multiple problem instances.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment for knapsack problem\n",
    "        problem_instances (List[Dict]): List of problem instances\n",
    "        N (int): Maximum number of items in a problem instance\n",
    "        use_state_aggregation (bool): Whether to use state aggregation\n",
    "        gamma (float): Discount factor\n",
    "        lr_policy (float): Learning rate for policy network\n",
    "        lr_value (float): Learning rate for value network\n",
    "        t_max (int): Maximum number of training steps\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[KnapsackDRLSolver, List[float]]: Trained solver and solution values\n",
    "    \"\"\"\n",
    "    # Initialize solver\n",
    "    solver = KnapsackDRLSolver(\n",
    "        env=env,\n",
    "        KPsolver=KPsolver,\n",
    "        use_state_aggregation=use_state_aggregation,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Train solver\n",
    "    solution_values = solver.train(problem_instances, t_max)\n",
    "\n",
    "    return solver, solution_values\n",
    "\n",
    "\n",
    "def evaluate_knapsack_solver(solver:KnapsackDRLSolver, test_instances:list[dict]):\n",
    "    \"\"\"\n",
    "    Evaluate the trained solver on test instances.\n",
    "    \n",
    "    Args:\n",
    "        solver (KnapsackDRLSolver): Trained solver\n",
    "        test_instances (List[Dict]): List of test problem instances\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Evaluation results for each test instance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, instance in enumerate(test_instances):\n",
    "        # Solve instance\n",
    "        value, selected_items = solver.solve(instance)\n",
    "        \n",
    "        # Calculate actual total weight\n",
    "        total_weight = sum(instance['weights'][idx] for idx in selected_items)\n",
    "        \n",
    "        # Check if solution respects capacity constraint\n",
    "        is_valid = total_weight <= instance['capacity']\n",
    "\n",
    "        optimal_value, optimal_items = solve_knapsack_dp(instance)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'instance_idx': i,\n",
    "            'total_value': value,\n",
    "            'total_weight': total_weight,\n",
    "            'capacity': instance['capacity'],\n",
    "            'selected_items': selected_items,\n",
    "            'is_valid': is_valid,\n",
    "            \"optimal_sol\": optimal_value,\n",
    "            \"optimal_items\": optimal_items,\n",
    "            \"optimality_ratio\": value /  optimal_value \n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_KPSolver(env:KnapsackEnv, KPSolver:KnapsackDRLSolver, \n",
    "                 training_problem_instances: List[Dict[str, Any]], testing_problems_instances:List[Dict[str, Any]], t_max:int=10000):\n",
    "    \n",
    "    print(f\"Running Model {KPSolver.__class__}\")\n",
    "    # Train solver\n",
    "    solver, solution_values = train_knapsack_solver(\n",
    "        env=env,\n",
    "        problem_instances=training_problem_instances,\n",
    "        KPsolver=KPSolver,\n",
    "        use_state_aggregation=False,\n",
    "        t_max=t_max,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    optimal_sol, optimal_sol_items = solve_KP_instances_with_DP(training_problem_instances)\n",
    "    \n",
    "    training_selected, training_value_total, training_weight_total = solve_problem_instances_greedy(training_problem_instances)\n",
    "\n",
    "    print(\"Trained solution values:\", solution_values)\n",
    "    print(\"Optimal solution values:\", optimal_sol)\n",
    "    print(\"Greedy solution values:\", training_selected)\n",
    "    \n",
    "    \n",
    "    solver.verbose = False\n",
    "    results = evaluate_knapsack_solver(solver, testing_problems_instances)\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results:list):\n",
    "    print(\"Evaluation results:\")\n",
    "    for res in results: \n",
    "        print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model <class 'A2C.KnapsackA2C'>\n",
      "Training on 5 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Trained solution values: [84.0, 69.0, 267.0, 155.0, 247.0]\n",
      "Optimal solution values: [84.0, 69.0, 368.0, 203.0, 358.0]\n",
      "Greedy solution values: [84.0, 69.0, 368.0, 190.0, 353.0]\n",
      "Running Model <class 'KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 5 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Trained solution values: [84.0, 69.0, 280.0, 149.0, 250.0]\n",
      "Optimal solution values: [84.0, 69.0, 368.0, 203.0, 358.0]\n",
      "Greedy solution values: [84.0, 69.0, 368.0, 190.0, 353.0]\n",
      "Running Model <class 'KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 5 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.01075050390526581\n",
      "Trained solution values: [84.0, 69.0, 280.0, 169.0, 244.0]\n",
      "Optimal solution values: [84.0, 69.0, 368.0, 203.0, 358.0]\n",
      "Greedy solution values: [84.0, 69.0, 368.0, 190.0, 353.0]\n"
     ]
    }
   ],
   "source": [
    "from KnapsackPPO import KnapsackPPOSolver\n",
    "from A2C import KnapsackA2C\n",
    "from KnapsackQLearning import KnapsackDQN\n",
    "from problem_gen import create_knapsack_problem_instances\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    N = 20\n",
    "    gamma = 0.99\n",
    "    t_max = 10000\n",
    "\n",
    "    env:KnapsackEnv = KnapsackEnv(problem_instance=None, N=N)\n",
    "\n",
    "    problem_instances = create_knapsack_problem_instances(num_instances=5, N=N, seed=42)\n",
    "\n",
    "    # Evaluate on test instances\n",
    "    test_instances = create_knapsack_problem_instances(num_instances=5, N=N, seed=40)\n",
    "\n",
    "    KPSolver_A2C = KnapsackA2C(N=N, gamma=gamma, lr_policy=0.001, lr_value=0.001, verbose=False)\n",
    "\n",
    "    KPSolver_PPO = KnapsackPPOSolver(N=N, gamma=gamma, policy_lr=0.001, value_lr=0.001, verbose=False)\n",
    "\n",
    "    KPSolver_DQN = KnapsackDQN(N=N, gamma=gamma, lr=0.001, verbose=False)\n",
    "\n",
    "    A2C_Results = run_KPSolver(env=env, KPSolver=KPSolver_A2C, training_problem_instances=problem_instances, testing_problems_instances=test_instances, t_max=t_max)\n",
    "    PPO_Results = run_KPSolver(env=env, KPSolver=KPSolver_PPO, training_problem_instances=problem_instances, testing_problems_instances=test_instances, t_max=t_max)\n",
    "    DQN_Results = run_KPSolver(env=env, KPSolver=KPSolver_DQN, training_problem_instances=problem_instances, testing_problems_instances=test_instances, t_max=t_max)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
