{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knapsackgym import KnapsackEnv\n",
    "from AbstractKnapsackPolicy import AbstractKnapsackPolicy\n",
    "from StateAggregator import StateAggregator\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from DP_Knapsack import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main KnapsackDRLSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackDRLSolver:\n",
    "    \"\"\"DRL-based Knapsack Solver using A2C algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, env, KPsolver:AbstractKnapsackPolicy, use_state_aggregation=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize the DRL solver.\n",
    "        \n",
    "        Args:\n",
    "            env: Gym environment for the knapsack problem\n",
    "            N (int): Maximum number of items in a problem instance\n",
    "            use_state_aggregation (bool): Whether to use state aggregation\n",
    "            gamma (float): Discount factor\n",
    "            lr_policy (float): Learning rate for policy network\n",
    "            lr_value (float): Learning rate for value network\n",
    "        \"\"\"\n",
    "        self.env:KnapsackEnv = env\n",
    "        self.N = KPsolver.N\n",
    "        self.use_state_aggregation = use_state_aggregation\n",
    "        \n",
    "        \n",
    "        # Initialize state aggregator if needed\n",
    "        self.state_aggregator = StateAggregator() if use_state_aggregation else None\n",
    "        self.KPsolver = KPsolver\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def process_state(self, state):\n",
    "        \"\"\"\n",
    "        Process state with optional aggregation.\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Original state\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Processed state\n",
    "        \"\"\"\n",
    "        if self.use_state_aggregation and self.state_aggregator is not None:\n",
    "            return self.state_aggregator.aggregate(state)\n",
    "        return state\n",
    "    \n",
    "    def train(self, problem_instances, t_max=None):\n",
    "        \"\"\"\n",
    "        Train the DRL solver on multiple problem instances with progress tracking.\n",
    "        \n",
    "        Args:\n",
    "            problem_instances (List[Dict]): List of problem instances\n",
    "            t_max (int): Maximum number of training steps\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: Values of solutions for each problem instance\n",
    "        \"\"\"\n",
    "        assert len(problem_instances) is not None or len(problem_instances) > 0\n",
    "\n",
    "        if t_max is None:\n",
    "            t_max = 3 * self.N * 10000  # As specified in the pseudocode\n",
    "        \n",
    "        val = [0] * len(problem_instances)  # Initialize solution values\n",
    "        \n",
    "        print(f\"Training on {len(problem_instances)} KP Instances, with N={self.N}, t_max={t_max}\")\n",
    "        for t in range(t_max):\n",
    "\n",
    "            # Select a problem instance (line 6 in pseudocode)\n",
    "            # P_idx = np.random.randint(0, len(problem_instances))\n",
    "            P_idx = t % len(problem_instances)\n",
    "            P = problem_instances[P_idx]\n",
    "            \n",
    "            assert len(P['values'] )<= self.N, f\"Problem Instance has too many items. KnapsackEnv is configuered to accept no more than <= {self.N}.\"\n",
    "\n",
    "            # Change the environment to use this problem instance\n",
    "            self.env.change_problem_instance(P)\n",
    "            \n",
    "            # Reset environment and get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # Initialize for this episode\n",
    "            done = False\n",
    "            ow = 0  # Total weight of selected items\n",
    "            ov = 0  # Total value of selected items\n",
    "            \n",
    "            # Create a copy of the problem instance P for modification\n",
    "            n_P_prime = len(P['values'])\n",
    "            W_P_prime = P['capacity']\n",
    "            \n",
    "            # Store states, actions, rewards for batch update\n",
    "            states:List[np.ndarray] = []\n",
    "            actions:List[int] = []\n",
    "            rewards:List[float] = []\n",
    "            next_states:List[np.ndarray] = []\n",
    "            dones:List[bool] = []\n",
    "            \n",
    "            # Track episode rewards\n",
    "            episode_rewards:List[float] = []\n",
    "            \n",
    "            # Solve the knapsack problem for this instance\n",
    "            while ow < W_P_prime and n_P_prime > 0 and not done:\n",
    "                # Process state if needed\n",
    "                processed_state = self.process_state(state)\n",
    "                states.append(processed_state)\n",
    "                \n",
    "                # Get available actions (indices of remaining items)\n",
    "                available_actions = list(range(len(self.env.items)))\n",
    "                \n",
    "                # Get action according to policy (line 12 in pseudocode)\n",
    "                action = self.KPsolver.get_action(processed_state, available_actions)\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Take action and observe reward and next state\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                episode_rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "                \n",
    "                # Check if item fits in knapsack (line 13 in pseudocode)\n",
    "                if action < n_P_prime and info['current_weight'] - ow <= W_P_prime:\n",
    "                    # Update totals (line 14 in pseudocode)\n",
    "                    ow = info['current_weight']\n",
    "                    ov = info['current_value']\n",
    "                    W_P_prime = P['capacity'] - ow\n",
    "                \n",
    "                # Update P_prime (line 16 in pseudocode)\n",
    "                n_P_prime -= 1\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "            \n",
    "            # Update parameters using collected trajectories\n",
    "            self.KPsolver.update_parameters(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            if self.verbose and t % 1000 == 0:\n",
    "                print(f\"Iteration [{t}/{t_max}], Training KP Instance {P_idx}, Reward: {sum(rewards) / len(rewards)}\")\n",
    "\n",
    "\n",
    "            # Update best value if needed (lines 20-22 in pseudocode)\n",
    "            if ov > val[P_idx]:\n",
    "                val[P_idx] = ov\n",
    "            \n",
    "        return val\n",
    "    \n",
    "    \n",
    "    def solve(self, problem_instance):\n",
    "        \"\"\"\n",
    "        Solve a single knapsack problem instance using the trained policy.\n",
    "        \n",
    "        Args:\n",
    "            problem_instance (Dict): A problem instance\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, List[int]]: Total value and list of selected item indices\n",
    "        \"\"\"\n",
    "        # Set environment to use this problem instance\n",
    "        self.env.change_problem_instance(problem_instance)\n",
    "        \n",
    "        # Reset environment and get initial state\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        done = False\n",
    "        total_value = 0\n",
    "        total_weight = 0\n",
    "        selected_items = []\n",
    "        \n",
    "        while not done:\n",
    "            # Process state if needed\n",
    "            processed_state = self.process_state(state)\n",
    "            \n",
    "            # Get available actions (indices of remaining items)\n",
    "            available_actions = list(range(len(self.env.items)))\n",
    "            \n",
    "            if not available_actions:\n",
    "                break\n",
    "                \n",
    "            # Get action according to policy\n",
    "            action = self.KPsolver.get_action(processed_state, available_actions)\n",
    "            \n",
    "            # Get original item index (before any removal)\n",
    "            original_item_idx = self.env.items[action][2]\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # If item was added (positive reward means item fit)\n",
    "            if reward > 0:\n",
    "                selected_items.append(original_item_idx)\n",
    "            \n",
    "            # Update value and weight\n",
    "            total_value = info['current_value']\n",
    "            total_weight = info['current_weight']\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        return total_value, selected_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Greedy_Knapsack import greedy_knapsack, solve_problem_instances_greedy\n",
    "\n",
    "\n",
    "def train_knapsack_solver(env, problem_instances:List[Dict[str, Any]], KPsolver, use_state_aggregation=False, t_max=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a DRL-based knapsack solver on multiple problem instances.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment for knapsack problem\n",
    "        problem_instances (List[Dict]): List of problem instances\n",
    "        N (int): Maximum number of items in a problem instance\n",
    "        use_state_aggregation (bool): Whether to use state aggregation\n",
    "        gamma (float): Discount factor\n",
    "        lr_policy (float): Learning rate for policy network\n",
    "        lr_value (float): Learning rate for value network\n",
    "        t_max (int): Maximum number of training steps\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[KnapsackDRLSolver, List[float]]: Trained solver and solution values\n",
    "    \"\"\"\n",
    "    # Initialize solver\n",
    "    solver = KnapsackDRLSolver(\n",
    "        env=env,\n",
    "        KPsolver=KPsolver,\n",
    "        use_state_aggregation=use_state_aggregation,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Train solver\n",
    "    solution_values = solver.train(problem_instances, t_max)\n",
    "\n",
    "    return solver, solution_values\n",
    "\n",
    "\n",
    "def evaluate_knapsack_solver(solver:KnapsackDRLSolver, test_instances:list[dict]):\n",
    "    \"\"\"\n",
    "    Evaluate the trained solver on test instances.\n",
    "    \n",
    "    Args:\n",
    "        solver (KnapsackDRLSolver): Trained solver\n",
    "        test_instances (List[Dict]): List of test problem instances\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Evaluation results for each test instance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, instance in enumerate(test_instances):\n",
    "        # Solve instance\n",
    "        value, selected_items = solver.solve(instance)\n",
    "        \n",
    "        # Calculate actual total weight\n",
    "        total_weight = sum(instance['weights'][idx] for idx in selected_items)\n",
    "        \n",
    "        # Check if solution respects capacity constraint\n",
    "        is_valid = total_weight <= instance['capacity']\n",
    "\n",
    "        optimal_value, optimal_items = solve_knapsack_dp(instance)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'instance_idx': i,\n",
    "            'total_value': value,\n",
    "            'total_weight': total_weight,\n",
    "            'capacity': instance['capacity'],\n",
    "            'selected_items': selected_items,\n",
    "            'is_valid': is_valid,\n",
    "            \"optimal_sol\": optimal_value,\n",
    "            \"optimal_items\": optimal_items,\n",
    "            \"optimality_ratio\": value /  optimal_value \n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_KPSolver(env:KnapsackEnv, KPSolver:KnapsackDRLSolver, \n",
    "                 training_problem_instances: List[Dict[str, Any]], testing_problems_instances:List[Dict[str, Any]], t_max:int=10000):\n",
    "    \n",
    "    print(f\"Running Model {KPSolver.__class__}\")\n",
    "    # Train solver\n",
    "    solver, solution_values = train_knapsack_solver(\n",
    "        env=env,\n",
    "        problem_instances=training_problem_instances,\n",
    "        KPsolver=KPSolver,\n",
    "        use_state_aggregation=False,\n",
    "        t_max=t_max,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    optimal_sol, optimal_sol_items = solve_KP_instances_with_DP(training_problem_instances)\n",
    "    \n",
    "    training_selected, training_value_total, training_weight_total = solve_problem_instances_greedy(training_problem_instances)\n",
    "\n",
    "    print(\"Trained solution values:\", solution_values)\n",
    "    print(\"Optimal solution values:\", optimal_sol)\n",
    "    print(\"Greedy solution values:\", training_selected)\n",
    "    \n",
    "    \n",
    "    solver.verbose = False\n",
    "    results = evaluate_knapsack_solver(solver, testing_problems_instances)\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results:list):\n",
    "    print(\"Evaluation results:\")\n",
    "    for res in results: \n",
    "        print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model <class 'A2C.KnapsackA2C'>\n",
      "Training on 2 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsemerdz/School/COMP579_Assignments/A2C.py:259: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(state_values, returns_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.0996078431372549\n",
      "Trained solution values: [202.0, 152.0]\n",
      "Optimal solution values: [210.0, 152.0]\n",
      "Greedy solution values: [202.0, 152.0]\n",
      "Running Model <class 'KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 2 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m KPSolver_DQN \u001b[38;5;241m=\u001b[39m KnapsackDQN(N\u001b[38;5;241m=\u001b[39mN, gamma\u001b[38;5;241m=\u001b[39mgamma, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m A2C_Results \u001b[38;5;241m=\u001b[39m run_KPSolver(env\u001b[38;5;241m=\u001b[39menv, KPSolver\u001b[38;5;241m=\u001b[39mKPSolver_A2C, training_problem_instances\u001b[38;5;241m=\u001b[39mproblem_instances, testing_problems_instances\u001b[38;5;241m=\u001b[39mtest_instances, t_max\u001b[38;5;241m=\u001b[39mt_max)\n\u001b[0;32m---> 42\u001b[0m PPO_Results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_KPSolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKPSolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKPSolver_PPO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_problem_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_instances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_problems_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_instances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m DQN_Results \u001b[38;5;241m=\u001b[39m run_KPSolver(env\u001b[38;5;241m=\u001b[39menv, KPSolver\u001b[38;5;241m=\u001b[39mKPSolver_DQN, training_problem_instances\u001b[38;5;241m=\u001b[39mproblem_instances, testing_problems_instances\u001b[38;5;241m=\u001b[39mtest_instances, t_max\u001b[38;5;241m=\u001b[39mt_max)\n",
      "Cell \u001b[0;32mIn[4], line 80\u001b[0m, in \u001b[0;36mrun_KPSolver\u001b[0;34m(env, KPSolver, training_problem_instances, testing_problems_instances, t_max)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKPSolver\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Train solver\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m solver, solution_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_knapsack_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_instances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_problem_instances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKPsolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKPSolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_state_aggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m optimal_sol, optimal_sol_items \u001b[38;5;241m=\u001b[39m solve_KP_instances_with_DP(training_problem_instances)\n\u001b[1;32m     91\u001b[0m training_selected, training_value_total, training_weight_total \u001b[38;5;241m=\u001b[39m solve_problem_instances_greedy(training_problem_instances)\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mtrain_knapsack_solver\u001b[0;34m(env, problem_instances, KPsolver, use_state_aggregation, t_max, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m solver \u001b[38;5;241m=\u001b[39m KnapsackDRLSolver(\n\u001b[1;32m     23\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m     24\u001b[0m     KPsolver\u001b[38;5;241m=\u001b[39mKPsolver,\n\u001b[1;32m     25\u001b[0m     use_state_aggregation\u001b[38;5;241m=\u001b[39muse_state_aggregation,\n\u001b[1;32m     26\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train solver\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m solution_values \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_instances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solver, solution_values\n",
      "Cell \u001b[0;32mIn[3], line 127\u001b[0m, in \u001b[0;36mKnapsackDRLSolver.train\u001b[0;34m(self, problem_instances, t_max)\u001b[0m\n\u001b[1;32m    124\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Update parameters using collected trajectories\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKPsolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Training KP Instance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mP_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(rewards)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/School/COMP579_Assignments/KnapsackPPO.py:280\u001b[0m, in \u001b[0;36mKnapsackPPOSolver.update_parameters\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# --- PPO Update loop (K_epochs) ---\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_epochs):\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# 2.1) Evaluate log_probs under *new* policy\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     log_probs, dist_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# 2.2) Compute ratio = exp(new_log_prob - old_log_prob)\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_probs \u001b[38;5;241m-\u001b[39m old_log_probs)\n",
      "File \u001b[0;32m~/School/COMP579_Assignments/KnapsackPPO.py:101\u001b[0m, in \u001b[0;36mPolicyNetworkPPO.evaluate_actions\u001b[0;34m(self, states, actions)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, states: torch\u001b[38;5;241m.\u001b[39mTensor, actions: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Evaluate the log probabilities of given actions under the current policy.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    Also returns the entropy of the distribution for regularization.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m     action_log_probs \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/School/COMP579_Assignments/KnapsackPPO.py:57\u001b[0m, in \u001b[0;36mPolicyNetworkPPO.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass that returns raw logits for each possible action.\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Last layer (logits)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[0;32m~/School/COMP579_Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/COMP579_Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/School/COMP579_Assignments/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from KnapsackPPO import KnapsackPPOSolver\n",
    "from A2C import KnapsackA2C\n",
    "from KnapsackQLearning import KnapsackDQN\n",
    "\n",
    "\n",
    "# TODO integrate the instance generator in this code\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    N = 20\n",
    "    gamma = 0.99\n",
    "    t_max = 10000\n",
    "\n",
    "    env:KnapsackEnv = KnapsackEnv(problem_instance=None, N=N)\n",
    "\n",
    "    problem_instances = [\n",
    "        {\n",
    "        'values': [60, 100, 120, 23, 10, 42, 50],\n",
    "        'weights': [10, 20, 40, 20, 30, 7, 17],\n",
    "        'capacity': 50\n",
    "        },\n",
    "        {\n",
    "        'values': [60, 10, 10, 23, 11, 42, 50],\n",
    "        'weights': [10, 21, 45, 20, 30, 7, 17],\n",
    "        'capacity': 50\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Evaluate on test instances\n",
    "    test_instances = [{\n",
    "        'values': [60, 100, 120],\n",
    "        'weights': [10, 20, 40],\n",
    "        'capacity': 40\n",
    "    }]\n",
    "\n",
    "    KPSolver_A2C = KnapsackA2C(N=N, gamma=gamma, lr_policy=0.001, lr_value=0.001, verbose=False)\n",
    "\n",
    "    KPSolver_PPO = KnapsackPPOSolver(N=N, gamma=gamma, policy_lr=0.001, value_lr=0.001, verbose=False)\n",
    "\n",
    "    KPSolver_DQN = KnapsackDQN(N=N, gamma=gamma, lr=0.001, verbose=False)\n",
    "\n",
    "    A2C_Results = run_KPSolver(env=env, KPSolver=KPSolver_A2C, training_problem_instances=problem_instances, testing_problems_instances=test_instances, t_max=t_max)\n",
    "    PPO_Results = run_KPSolver(env=env, KPSolver=KPSolver_PPO, training_problem_instances=problem_instances, testing_problems_instances=test_instances, t_max=t_max)\n",
    "    DQN_Results = run_KPSolver(env=env, KPSolver=KPSolver_DQN, training_problem_instances=problem_instances, testing_problems_instances=test_instances, t_max=t_max)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
