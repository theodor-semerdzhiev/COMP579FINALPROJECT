{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from DP_Knapsack import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "from Greedy_Knapsack import solve_problem_instances_greedy\n",
    "from KnapsackPPO import KnapsackPPOSolver\n",
    "from KnapsackA2C import KnapsackA2C\n",
    "from KnapsackQLearning import KnapsackDQN\n",
    "from util.instance_gen import KnapsackInstanceGenerator\n",
    "\n",
    "from util.metrics import evaluate_knapsack_performance\n",
    "from KnapsackDRLSolver import KnapsackDRLSolver, run_KPSolver\n",
    "from environment.knapsackgym import KnapsackEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook is purely for testing and experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'values': [9, 70, 21, 10, 53], 'weights': [78, 66, 44, 44, 86], 'capacity': 293}, {'values': [76, 20, 37, 47, 50, 5, 55, 16, 75, 69, 93, 75, 37, 97, 42, 33, 91, 38, 8, 47, 80, 19, 47, 13, 69, 48, 34, 23, 57, 67, 95, 44, 17, 84, 63, 71, 10], 'weights': [77, 72, 79, 52, 13, 84, 46, 51, 38, 19, 93, 79, 65, 41, 83, 55, 45, 46, 23, 10, 56, 89, 7, 86, 83, 28, 64, 17, 76, 71, 36, 7, 98, 45, 90, 68, 78], 'capacity': 100}, {'values': [58, 64, 57, 56, 10, 56, 80, 31, 61, 4, 35, 44, 99, 22, 28, 41, 100, 86, 4, 24, 83, 6, 86, 29, 92, 30, 44, 67, 13, 56, 51, 79, 100, 67, 41, 41, 42, 82, 33], 'weights': [84, 44, 81, 85, 39, 90, 29, 24, 69, 64, 14, 84, 20, 81, 1, 80, 79, 79, 67, 48, 71, 28, 79, 56, 46, 51, 57, 4, 14, 25, 12, 44, 67, 66, 48, 86, 57, 8, 77], 'capacity': 58}, {'values': [17, 39, 24, 31, 69, 64, 61, 37, 96, 9, 35, 12, 34, 97, 37, 91, 50], 'weights': [3, 11, 10, 78, 73, 70, 47, 72, 17, 91, 51, 94, 16, 50, 70, 50, 45], 'capacity': 213}, {'values': [55, 18, 47, 86, 2, 76, 50, 72, 67, 44, 31, 63, 15, 59, 10, 65, 63, 9, 76, 42, 79, 5, 18], 'weights': [27, 77, 97, 27, 78, 27, 72, 79, 45, 74, 28, 8, 10, 45, 91, 13, 46, 71, 21, 73, 31, 81, 58], 'capacity': 153}, {'values': [93, 99, 59, 45, 35, 62, 60, 30, 3, 16], 'weights': [33, 67, 15, 68, 11, 18, 59, 80, 18, 30], 'capacity': 288}, {'values': [85, 44, 29, 3, 37, 83, 1, 90, 55, 15, 31, 56, 32, 11, 67, 68, 12, 29, 25, 66, 74], 'weights': [49, 47, 79, 7, 9, 28, 49, 68, 50, 45, 94, 17, 58, 55, 48, 81, 27, 96, 34, 55, 53], 'capacity': 221}, {'values': [47, 32, 3, 78, 34, 67, 42, 38, 89, 10, 74, 75, 38, 27, 1, 94, 95, 25, 4, 13, 7, 84, 90, 16, 75, 18, 37, 60, 72, 88, 5, 20, 29, 32, 100, 78, 33, 98, 50, 51, 44, 15, 94, 2, 18, 23, 28], 'weights': [77, 33, 11, 62, 92, 98, 24, 24, 4, 15, 56, 55, 38, 31, 83, 47, 81, 97, 32, 21, 96, 64, 30, 63, 52, 91, 26, 100, 94, 19, 17, 54, 5, 30, 44, 97, 100, 16, 90, 10, 75, 85, 90, 23, 90, 12, 52], 'capacity': 48}, {'values': [97], 'weights': [68], 'capacity': 45}, {'values': [44, 13, 57, 93, 9, 40, 98, 31, 14, 49, 51, 67, 32, 96, 53], 'weights': [51, 93, 70, 77, 59, 42, 20, 87, 81, 3, 72, 9, 74, 83, 14], 'capacity': 93}]\n",
      "Running Model <class 'KnapsackA2C.KnapsackA2C'>\n",
      "Training on 10 KP Instances, with N=50, t_max=20000\n",
      "Iteration [0/20000], Training KP Instance 0, Reward: -0.004517794822580917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsemerdz/School/COMP579_Assignments/models/KnapsackA2C.py:233: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  states_tensor = torch.FloatTensor(states)\n",
      "/Users/tsemerdz/School/COMP579_Assignments/models/KnapsackA2C.py:260: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(state_values, returns_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [1000/20000], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Iteration [2000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [3000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [4000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [5000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [6000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [7000/20000], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Iteration [8000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [9000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [10000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [11000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [12000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [13000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [14000/20000], Training KP Instance 0, Reward: -0.005276762817043793\n",
      "Iteration [15000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [16000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [17000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [18000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [19000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Running Model <class 'KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 10 KP Instances, with N=50, t_max=20000\n",
      "Iteration [0/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [1000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [2000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [3000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [4000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [5000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [6000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [7000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [8000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [9000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [10000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [11000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [12000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [13000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [14000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [15000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [16000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [17000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [18000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [19000/20000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Running Model <class 'KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 10 KP Instances, with N=50, t_max=20000\n",
      "Iteration [0/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [1000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [2000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [3000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [4000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [5000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [6000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [7000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [8000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [9000/20000], Training KP Instance 0, Reward: -0.005276762817043793\n",
      "Iteration [10000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [11000/20000], Training KP Instance 0, Reward: -0.005276762817043793\n",
      "Iteration [12000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [13000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [14000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [15000/20000], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Iteration [16000/20000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [17000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [18000/20000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [19000/20000], Training KP Instance 0, Reward: -0.004517794822580917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO integrate the instance generator in this code\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    N = 50\n",
    "    M = 10\n",
    "    gamma = 0.99\n",
    "    t_max = 40000\n",
    "    # t_max = None\n",
    "\n",
    "    env:KnapsackEnv = KnapsackEnv(problem_instance=None, N=N)\n",
    "    gen = KnapsackInstanceGenerator(seed=42)\n",
    "\n",
    "    problem_instances = gen.generate('RI', M=M, N=N, R=100)\n",
    "    print(problem_instances)\n",
    "\n",
    "    KPSolver_A2C = KnapsackA2C(N=N, gamma=gamma, lr_policy=0.001, lr_value=0.001, verbose=False)\n",
    "    KPSolver_PPO = KnapsackPPOSolver(N=N, gamma=gamma, policy_lr=0.001, value_lr=0.001, verbose=False)\n",
    "    KPSolver_DQN = KnapsackDQN(N=N, gamma=gamma, lr=0.001, verbose=False)\n",
    "\n",
    "    DP_sol_items, DP_value, DP_weight = solve_KP_instances_with_DP(problem_instances)\n",
    "    Greedy_value_total, Greedy_selected, Greedy_weight_total = solve_problem_instances_greedy(problem_instances)\n",
    "    \n",
    "    \n",
    "    _, A2C_Results = run_KPSolver(env=env, KPSolver=KPSolver_A2C, training_problem_instances=problem_instances, t_max=t_max)\n",
    "    _, PPO_Results = run_KPSolver(env=env, KPSolver=KPSolver_PPO, training_problem_instances=problem_instances, t_max=t_max)\n",
    "    _, DQN_Results = run_KPSolver(env=env, KPSolver=KPSolver_DQN, training_problem_instances=problem_instances, t_max=t_max)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution values: [154.0, 354.0, 334.0, 459.0, 460.0, 453.0, 418.0, 304.0, 0.0, 307.0]\n",
      "Greedy solution values: [154.0, 352.0, 327.0, 459.0, 460.0, 453.0, 405.0, 290.0, 0.0, 307.0]\n",
      "A2C Trained solution values: [154. 249. 327. 459. 450. 453. 355. 290.   0. 307.]\n",
      "PPO Trained solution values: {'instance_best_values': array([154., 302., 292., 459., 404., 453., 375., 304.,   0., 307.]), 'best_values_over_time': array([[142.,   0.,   0., ...,   0.,   0.,   0.],\n",
      "       [142., 140.,   0., ...,   0.,   0.,   0.],\n",
      "       [142., 140., 151., ...,   0.,   0.,   0.],\n",
      "       ...,\n",
      "       [154., 302., 292., ..., 304.,   0., 307.],\n",
      "       [154., 302., 292., ..., 304.,   0., 307.],\n",
      "       [154., 302., 292., ..., 304.,   0., 307.]]), 'best_sum_over_time': array([ 142.,  282.,  433., ..., 3050., 3050., 3050.]), 'avg_rewards_over_time': array([-1.29559437e-02, -4.59702381e-01, -8.01014957e-01, ...,\n",
      "       -8.68762089e-01, -2.15555556e+00, -8.00034754e+01])}\n",
      "DQN Trained solution values: {'instance_best_values': array([154., 308., 334., 459., 460., 453., 404., 304.,   0., 307.]), 'best_values_over_time': array([[142.,   0.,   0., ...,   0.,   0.,   0.],\n",
      "       [142.,  65.,   0., ...,   0.,   0.,   0.],\n",
      "       [142.,  65., 134., ...,   0.,   0.,   0.],\n",
      "       ...,\n",
      "       [154., 308., 334., ..., 304.,   0., 307.],\n",
      "       [154., 308., 334., ..., 304.,   0., 307.],\n",
      "       [154., 308., 334., ..., 304.,   0., 307.]]), 'best_sum_over_time': array([ 142.,  207.,  341., ..., 3183., 3183., 3183.]), 'avg_rewards_over_time': array([-0.01295594, -0.55214103, -0.808687  , ..., -0.18913966,\n",
      "       -2.15555556, -0.41087643])}\n",
      "{'N': 10, 'AveragedVal': np.float64(304.4), '#opt': 5, '#highest': 5, 'ValOptRatio': np.float64(93.86370644465002), 'mean_absolute_error': np.float64(19.9), 'min_absolute_error': np.float64(0.0), 'max_absolute_error': np.float64(105.0), 'mean_percentage_error': np.float64(0.0536077718686873), 'mean_improvement_over_greedy': np.float64(-0.04323168072925665)}\n",
      "{'N': 10, 'AveragedVal': np.float64(305.0), '#opt': 6, '#highest': 6, 'ValOptRatio': np.float64(94.04872032069072), 'mean_absolute_error': np.float64(19.3), 'min_absolute_error': np.float64(0.0), 'max_absolute_error': np.float64(56.0), 'mean_percentage_error': np.float64(0.04972511021931554), 'mean_improvement_over_greedy': np.float64(-0.03934901907988489)}\n",
      "{'N': 10, 'AveragedVal': np.float64(318.3), '#opt': 8, '#highest': 8, 'ValOptRatio': np.float64(98.14986123959298), 'mean_absolute_error': np.float64(6.0), 'min_absolute_error': np.float64(0.0), 'max_absolute_error': np.float64(46.0), 'mean_percentage_error': np.float64(0.016343632579136593), 'mean_improvement_over_greedy': np.float64(-0.005967541439705943)}\n"
     ]
    }
   ],
   "source": [
    " # print(A2C_Results, DP_value,  Greedy_value_total)\n",
    "A2C_metric_results = evaluate_knapsack_performance(A2C_Results[\"instance_best_values\"], DP_value, Greedy_value_total)\n",
    "PPO_metric_results = evaluate_knapsack_performance(PPO_Results[\"instance_best_values\"], DP_value, Greedy_value_total)\n",
    "DQN_metric_results = evaluate_knapsack_performance(DQN_Results[\"instance_best_values\"], DP_value, Greedy_value_total)\n",
    "\n",
    "print(\"Optimal solution values:\", DP_value)\n",
    "print(\"Greedy solution values:\", Greedy_value_total)\n",
    "print(\"A2C Trained solution values:\", A2C_Results[\"instance_best_values\"])\n",
    "print(\"PPO Trained solution values:\", PPO_Results)\n",
    "print(\"DQN Trained solution values:\", DQN_Results)\n",
    "\n",
    "print(A2C_metric_results)\n",
    "print(PPO_metric_results)\n",
    "print(DQN_metric_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instance_best_values': [133.0, 186.0, 201.0, 134.0, 88.0, 242.0, 188.0, 93.0, 0, 258.0, 245.0, 121.0, 166.0, 172.0, 161.0, 259.0, 193.0, 126.0, 148.0, 142.0, 103.0, 294.0, 71.0, 123.0, 30.0, 185.0, 94.0, 94.0, 116.0, 200.0, 176.0, 119.0, 167.0, 163.0, 82.0, 244.0, 380.0, 198.0, 150.0, 98.0, 165.0, 104.0, 314.0, 344.0, 23.0, 152.0, 114.0, 178.0, 154.0, 169.0, 51.0, 204.0, 203.0, 211.0, 132.0, 241.0, 160.0, 225.0, 203.0, 164.0, 165.0, 147.0, 216.0, 135.0, 95.0, 307.0, 114.0, 309.0, 156.0, 167.0, 110.0, 325.0, 149.0, 111.0, 194.0, 107.0, 138.0, 171.0, 195.0, 74.0, 87.0, 237.0, 182.0, 132.0, 202.0, 90.0, 195.0, 95.0, 230.0, 134.0, 221.0, 278.0, 230.0, 215.0, 83.0, 123.0, 175.0, 138.0, 90.0, 118.0, 221.0, 174.0, 150.0, 280.0, 163.0, 98.0, 370.0, 159.0, 332.0, 134.0, 76.0, 124.0, 78.0, 78.0, 213.0, 288.0, 163.0, 134.0, 72.0, 162.0, 213.0, 129.0, 308.0, 164.0, 80.0, 149.0, 157.0, 218.0, 245.0, 146.0, 89.0, 83.0, 122.0, 218.0, 92.0, 68.0, 122.0, 295.0, 64.0, 76.0, 191.0, 129.0, 205.0, 87.0, 110.0, 226.0, 0, 62.0, 188.0, 122.0, 174.0, 244.0, 112.0, 189.0, 287.0, 228.0, 11.0, 219.0, 159.0, 244.0, 135.0, 295.0, 225.0, 88.0, 180.0, 227.0, 210.0, 125.0, 102.0, 183.0, 159.0, 70.0, 186.0, 245.0, 100.0, 192.0, 210.0, 172.0, 149.0, 95.0, 163.0, 130.0, 275.0, 127.0, 137.0, 313.0, 157.0, 161.0, 190.0, 159.0, 115.0, 103.0, 198.0, 147.0, 182.0, 152.0, 83.0, 265.0, 321.0, 91.0, 112.0, 178.0, 169.0, 132.0, 201.0, 141.0, 214.0, 256.0, 320.0, 77.0, 216.0, 171.0, 118.0, 218.0, 132.0, 203.0, 125.0, 238.0, 277.0, 383.0, 0, 180.0, 248.0, 74.0, 151.0, 329.0, 377.0, 284.0, 290.0, 203.0, 156.0, 313.0, 201.0, 196.0, 239.0, 200.0, 220.0, 214.0, 86.0, 227.0, 143.0, 140.0, 147.0, 230.0, 158.0, 261.0, 240.0, 206.0, 84.0, 321.0, 165.0, 227.0, 335.0, 98.0, 222.0, 162.0, 77.0, 190.0, 99.0, 94.0, 116.0, 65.0, 177.0, 99.0, 155.0, 119.0, 139.0, 124.0, 407.0, 171.0, 74.0, 144.0, 94.0, 209.0, 162.0, 293.0, 124.0, 103.0, 140.0, 186.0, 273.0, 237.0, 266.0, 333.0, 314.0, 81.0, 142.0, 301.0, 256.0, 348.0, 170.0, 290.0, 149.0, 271.0, 83.0, 228.0, 188.0, 150.0, 259.0, 198.0, 154.0, 114.0, 174.0, 161.0, 73.0, 170.0, 86.0, 232.0, 247.0, 162.0, 306.0, 267.0, 105.0, 132.0, 254.0, 59.0, 143.0, 107.0, 171.0, 253.0, 221.0, 259.0, 207.0, 163.0, 119.0, 384.0, 236.0, 124.0, 174.0, 169.0, 316.0, 29.0, 141.0, 252.0, 256.0, 285.0, 277.0, 79.0, 134.0, 100.0, 302.0, 160.0, 251.0, 1.0, 156.0, 67.0, 262.0, 158.0, 186.0, 174.0, 92.0, 195.0, 149.0, 291.0, 90.0, 148.0, 154.0, 180.0, 97.0, 210.0, 371.0, 83.0, 196.0, 65.0, 194.0, 238.0, 151.0, 241.0, 63.0, 219.0, 230.0, 169.0, 317.0, 365.0, 109.0, 186.0, 274.0, 113.0, 127.0, 212.0, 178.0, 84.0, 108.0, 152.0, 138.0, 73.0, 184.0, 99.0, 250.0, 173.0, 174.0, 62.0, 230.0, 97.0, 189.0, 208.0, 195.0, 118.0, 120.0, 147.0, 166.0, 112.0, 247.0, 145.0, 228.0, 97.0, 187.0, 280.0, 10.0, 269.0, 260.0, 136.0, 295.0, 159.0, 619.0, 155.0, 229.0, 247.0, 228.0, 264.0, 180.0, 229.0, 118.0, 282.0, 95.0, 197.0, 136.0, 194.0, 349.0, 183.0, 283.0, 146.0, 193.0, 240.0, 90.0, 116.0, 294.0, 125.0, 225.0, 65.0, 154.0, 97.0, 71.0, 268.0, 113.0, 236.0, 182.0, 69.0, 188.0, 203.0, 408.0, 119.0, 9.0, 113.0, 151.0, 113.0, 268.0, 187.0, 154.0, 68.0, 252.0, 199.0, 95.0, 138.0, 163.0, 151.0, 119.0, 94.0, 94.0, 133.0, 177.0, 164.0, 263.0, 161.0, 237.0, 222.0, 225.0, 105.0, 542.0, 85.0, 225.0, 81.0, 128.0, 312.0, 63.0, 156.0, 244.0, 131.0, 145.0, 262.0, 173.0, 198.0, 237.0, 51.0, 131.0, 42.0, 220.0, 220.0, 232.0, 241.0, 319.0, 96.0, 201.0, 165.0, 200.0, 158.0, 11.0, 157.0, 102.0, 165.0, 164.0, 199.0, 291.0, 267.0, 130.0, 213.0, 167.0, 70.0, 165.0, 272.0, 136.0, 250.0, 140.0, 180.0, 200.0, 142.0, 239.0, 189.0, 232.0, 160.0, 105.0, 185.0, 182.0, 131.0, 122.0, 206.0, 95.0, 307.0, 161.0, 184.0, 62.0, 122.0, 152.0, 261.0, 84.0, 18.0, 74.0, 298.0, 292.0, 274.0, 97.0, 173.0, 392.0, 224.0, 126.0, 142.0, 154.0, 76.0, 180.0, 57.0, 303.0, 185.0, 73.0, 205.0, 62.0, 152.0, 202.0, 241.0, 186.0, 200.0, 136.0, 84.0, 108.0, 225.0, 169.0, 208.0, 156.0, 163.0, 85.0, 172.0, 300.0, 221.0, 207.0, 237.0, 245.0, 172.0, 317.0, 283.0, 323.0, 274.0, 166.0, 361.0, 177.0, 247.0, 274.0, 263.0, 143.0, 204.0, 120.0, 173.0, 159.0, 306.0, 91.0, 209.0, 243.0, 166.0, 98.0, 121.0, 208.0, 101.0, 102.0, 151.0, 251.0, 223.0, 69.0, 105.0, 169.0, 178.0, 364.0, 173.0, 259.0, 229.0, 191.0, 104.0, 191.0, 192.0, 208.0, 19.0, 193.0, 134.0, 202.0, 213.0, 0, 172.0, 220.0, 181.0, 137.0, 78.0, 168.0, 101.0, 193.0, 83.0, 134.0, 201.0, 167.0, 90.0, 227.0, 66.0, 156.0, 94.0, 75.0, 228.0, 46.0, 64.0, 206.0, 134.0, 147.0, 165.0, 217.0, 286.0, 180.0, 246.0, 230.0, 172.0, 163.0, 274.0, 151.0, 295.0, 19.0, 138.0, 84.0, 96.0, 122.0, 184.0, 232.0, 211.0, 216.0, 147.0, 86.0, 179.0, 234.0, 174.0, 123.0, 144.0, 131.0, 132.0, 177.0, 127.0, 141.0, 147.0, 137.0, 247.0, 63.0, 182.0, 196.0, 206.0, 73.0, 215.0, 222.0, 180.0, 164.0, 0, 139.0, 105.0, 40.0, 147.0, 119.0, 170.0, 122.0, 196.0, 152.0, 145.0, 204.0, 366.0, 401.0, 179.0, 228.0, 129.0, 182.0, 212.0, 79.0, 71.0, 107.0, 193.0, 158.0, 135.0, 95.0, 67.0, 108.0, 276.0, 132.0, 175.0, 191.0, 80.0, 107.0, 149.0, 303.0, 190.0, 281.0, 166.0, 100.0, 189.0, 378.0, 263.0, 68.0, 82.0, 287.0, 242.0, 80.0, 207.0, 248.0, 120.0, 321.0, 248.0, 120.0, 377.0, 80.0, 121.0, 345.0, 151.0, 289.0, 78.0, 319.0, 82.0, 165.0, 190.0, 137.0, 313.0, 196.0, 272.0, 139.0, 163.0, 256.0, 304.0, 354.0, 166.0, 91.0, 144.0, 188.0, 70.0, 325.0, 173.0, 130.0, 166.0, 156.0, 310.0, 111.0, 41.0, 178.0, 152.0, 199.0, 128.0, 79.0, 176.0, 248.0, 110.0, 140.0, 149.0, 186.0, 136.0, 161.0, 90.0, 167.0, 128.0, 167.0, 90.0, 241.0, 339.0, 50.0, 176.0, 155.0, 167.0, 138.0, 148.0, 59.0, 103.0, 112.0, 138.0, 182.0, 194.0, 171.0, 326.0, 180.0, 424.0, 263.0, 223.0, 97.0, 104.0, 238.0, 178.0, 0, 311.0, 79.0, 214.0, 152.0, 76.0, 79.0, 169.0, 94.0, 284.0, 247.0, 189.0, 97.0, 294.0, 187.0, 135.0, 162.0, 186.0, 201.0, 183.0, 58.0, 81.0, 108.0, 257.0, 85.0, 110.0, 303.0, 245.0, 108.0, 201.0, 210.0, 140.0, 192.0, 284.0, 212.0, 72.0, 242.0, 125.0, 341.0, 70.0, 180.0, 184.0, 118.0, 21.0, 221.0, 160.0, 120.0, 181.0, 228.0, 115.0, 183.0, 268.0, 270.0, 169.0, 184.0, 68.0, 256.0, 206.0, 234.0, 304.0, 88.0, 212.0, 318.0, 84.0, 243.0, 141.0, 100.0, 267.0, 87.0, 278.0, 163.0, 306.0, 149.0, 201.0, 328.0, 219.0, 134.0, 302.0, 210.0, 197.0, 180.0, 97.0, 206.0, 260.0, 298.0, 71.0, 193.0, 183.0, 98.0, 88.0, 210.0, 116.0, 314.0, 136.0, 394.0, 217.0, 190.0, 204.0, 99.0, 144.0, 134.0, 245.0, 176.0, 388.0, 320.0, 139.0, 186.0, 181.0, 71.0, 175.0, 83.0, 99.0, 262.0, 178.0, 228.0, 240.0, 155.0, 98.0, 188.0, 107.0, 224.0, 88.0, 89.0, 220.0, 87.0, 199.0, 86.0, 158.0, 107.0, 134.0, 154.0, 193.0, 80.0, 47.0, 179.0, 113.0, 103.0, 413.0, 202.0, 116.0, 171.0, 42.0, 100.0, 157.0, 126.0, 264.0, 167.0, 135.0, 214.0, 255.0, 157.0, 235.0, 176.0, 158.0, 232.0, 193.0, 145.0, 125.0, 173.0, 109.0, 176.0, 136.0, 161.0, 276.0, 137.0, 138.0, 139.0, 93.0, 228.0, 129.0], 'best_values_over_time': array([[101.,   0.,   0., ...,   0.,   0.,   0.],\n",
      "       [101.,  93.,   0., ...,   0.,   0.,   0.],\n",
      "       [101.,  93., 201., ...,   0.,   0.,   0.],\n",
      "       ...,\n",
      "       [133., 186., 201., ...,  93., 228., 129.],\n",
      "       [133., 186., 201., ...,  93., 228., 129.],\n",
      "       [133., 186., 201., ...,  93., 228., 129.]]), 'best_sum_over_time': array([1.01000e+02, 1.94000e+02, 3.95000e+02, ..., 1.75536e+05,\n",
      "       1.75536e+05, 1.75536e+05]), 'avg_rewards_over_time': array([ 0.00200814,  0.01      , -0.24051724, ...,  0.00920063,\n",
      "        0.00200505,  0.00223355])}\n"
     ]
    }
   ],
   "source": [
    "print(A2C_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained solution values: [144.0, 294.0, 283.0]\n",
      "Optimal solution values: [154.0, 354.0, 334.0]\n",
      "Greedy solution values: [154.0, 352.0, 327.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained solution values:\", A2C_Results)\n",
    "print(\"Optimal solution values:\", DP_value)\n",
    "print(\"Greedy solution values:\", Greedy_value_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
