{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for the 0-1 Knapsack Problem using reinforcement learning.\n",
    "    \n",
    "    The environment models the sequential decision process of selecting items\n",
    "    for a knapsack with limited capacity to maximize total value.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, max_items=10, \n",
    "                 value_range=(1, 10), \n",
    "                 weight_range=(1, 10), \n",
    "                 capacity_ratio=0.5,\n",
    "                 undefined_action_penalty=-10.0,\n",
    "                 heavy_item_penalty=-1.0,\n",
    "                 reward_scale=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Knapsack environment.\n",
    "        \n",
    "        Args:\n",
    "            max_items (int): Maximum number of items in any problem instance (N)\n",
    "            value_range (tuple): Range of possible item values (min, max)\n",
    "            weight_range (tuple): Range of possible item weights (min, max)\n",
    "            capacity_ratio (float): Ratio to determine knapsack capacity relative to total item weight\n",
    "            undefined_action_penalty (float): Penalty for selecting a non-existent item\n",
    "            heavy_item_penalty (float): Penalty for selecting an item that exceeds remaining capacity\n",
    "            reward_scale (float): Scaling factor for the reward\n",
    "        \"\"\"\n",
    "        super(KnapsackEnv, self).__init__()\n",
    "        \n",
    "        self.max_items = max_items  # N in the problem description\n",
    "        self.value_range = value_range\n",
    "        self.weight_range = weight_range\n",
    "        self.capacity_ratio = capacity_ratio\n",
    "        self.undefined_action_penalty = undefined_action_penalty\n",
    "        self.heavy_item_penalty = heavy_item_penalty\n",
    "        self.reward_scale = reward_scale\n",
    "        \n",
    "        # Action space: selecting one of the N items\n",
    "        self.action_space = spaces.Discrete(max_items)\n",
    "        \n",
    "        # State space: 2N + 4 features\n",
    "        # - n_P' (number of remaining items)\n",
    "        # - v_i and w_i for each item (2*N features)\n",
    "        # - W_P' (remaining capacity)\n",
    "        # - Total value of items\n",
    "        # - Total weight of items\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, \n",
    "            high=float('inf'),\n",
    "            shape=(2 * max_items + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize problem instance\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment with a new problem instance.\n",
    "        \n",
    "        Returns:\n",
    "            observation (np.array): Initial state vector\n",
    "            info (dict): Additional information\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Generate a random number of items (up to max_items)\n",
    "        self.current_items = random.randint(1, self.max_items)\n",
    "        \n",
    "        # Generate random values and weights for items\n",
    "        self.values = np.random.uniform(\n",
    "            self.value_range[0], \n",
    "            self.value_range[1], \n",
    "            self.current_items\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        self.weights = np.random.uniform(\n",
    "            self.weight_range[0], \n",
    "            self.weight_range[1], \n",
    "            self.current_items\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Set the knapsack capacity based on the total weight and capacity ratio\n",
    "        total_weight = np.sum(self.weights)\n",
    "        self.initial_capacity = float(total_weight * self.capacity_ratio)\n",
    "        self.remaining_capacity = self.initial_capacity\n",
    "        \n",
    "        # Track which items are still available\n",
    "        self.available_items = np.ones(self.max_items, dtype=bool)\n",
    "        self.available_items[self.current_items:] = False\n",
    "        \n",
    "        # Track the current solution\n",
    "        self.selected_items = []\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        \n",
    "        # Create and return the initial observation\n",
    "        observation = self._get_state()\n",
    "        info = {\n",
    "            'num_items': self.current_items,\n",
    "            'capacity': self.initial_capacity,\n",
    "            'selected_items': [],\n",
    "            'current_value': 0.0,\n",
    "            'current_weight': 0.0\n",
    "        }\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action (select an item) and return the next state, reward, etc.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The index of the item to select (0 to N-1)\n",
    "            \n",
    "        Returns:\n",
    "            observation (np.array): The next state\n",
    "            reward (float): The reward for the action\n",
    "            terminated (bool): Whether the episode is done\n",
    "            truncated (bool): Whether the episode was truncated\n",
    "            info (dict): Additional information\n",
    "        \"\"\"\n",
    "        # Initialize reward\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Check if the action is valid (item exists)\n",
    "        if action >= self.max_items or not self.available_items[action]:\n",
    "            # Penalty for undefined action\n",
    "            reward = self.undefined_action_penalty\n",
    "        else:\n",
    "            # Check if the item fits in the knapsack\n",
    "            item_weight = self.weights[action] if action < len(self.weights) else 0\n",
    "            \n",
    "            if item_weight <= self.remaining_capacity:\n",
    "                # Item fits, add it to the knapsack\n",
    "                self.selected_items.append(action)\n",
    "                item_value = self.values[action] if action < len(self.values) else 0\n",
    "                \n",
    "                # Update current solution\n",
    "                self.current_value += item_value\n",
    "                self.current_weight += item_weight\n",
    "                self.remaining_capacity -= item_weight\n",
    "                \n",
    "                # Reward is the value of the item\n",
    "                reward = item_value * self.reward_scale\n",
    "            else:\n",
    "                # Item doesn't fit, apply penalty\n",
    "                reward = self.heavy_item_penalty\n",
    "        \n",
    "        # Remove the item from available items regardless of whether it was added\n",
    "        if action < self.max_items:\n",
    "            self.available_items[action] = False\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        terminated = not np.any(self.available_items) or self.remaining_capacity <= min(self.weights[self.available_items[:len(self.weights)]]) if any(self.available_items[:len(self.weights)]) else True\n",
    "        \n",
    "        # Create the next state\n",
    "        observation = self._get_state()\n",
    "        \n",
    "        # Information about the current state\n",
    "        info = {\n",
    "            'selected_items': self.selected_items.copy(),\n",
    "            'current_value': float(self.current_value),\n",
    "            'current_weight': float(self.current_weight),\n",
    "            'remaining_capacity': float(self.remaining_capacity)\n",
    "        }\n",
    "        \n",
    "        return observation, reward, terminated, False, info\n",
    "        \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Create the state representation as described in the problem.\n",
    "        \n",
    "        Returns:\n",
    "            state (np.array): The state vector of size 2N + 4\n",
    "        \"\"\"\n",
    "        # Initialize state vector with zeros\n",
    "        state = np.zeros(2 * self.max_items + 4, dtype=np.float32)\n",
    "        \n",
    "        # Number of remaining items\n",
    "        remaining_items = np.sum(self.available_items)\n",
    "        state[0] = remaining_items\n",
    "        \n",
    "        # Values and weights of available items\n",
    "        v_idx = 1\n",
    "        w_idx = 1 + self.max_items\n",
    "        \n",
    "        for i in range(self.max_items):\n",
    "            if i < len(self.values) and self.available_items[i]:\n",
    "                state[v_idx + i] = self.values[i]\n",
    "                state[w_idx + i] = self.weights[i]\n",
    "        \n",
    "        # Remaining capacity\n",
    "        state[2 * self.max_items + 1] = self.remaining_capacity\n",
    "        \n",
    "        # Total value and weight of all available items\n",
    "        if remaining_items > 0:\n",
    "            available_indices = np.where(self.available_items[:len(self.values)])[0]\n",
    "            state[2 * self.max_items + 2] = np.sum(self.values[available_indices]) if len(available_indices) > 0 else 0\n",
    "            state[2 * self.max_items + 3] = np.sum(self.weights[available_indices]) if len(available_indices) > 0 else 0\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Render the current state of the environment.\n",
    "        \n",
    "        Args:\n",
    "            mode (str): The rendering mode\n",
    "        \"\"\"\n",
    "        if mode == 'human':\n",
    "            print(\"\\n==== Knapsack Environment State ====\")\n",
    "            print(f\"Capacity: {self.remaining_capacity:.2f}/{self.initial_capacity:.2f}\")\n",
    "            print(f\"Current solution value: {self.current_value:.2f}\")\n",
    "            print(f\"Current solution weight: {self.current_weight:.2f}\")\n",
    "            print(f\"Selected items: {self.selected_items}\")\n",
    "            print(f\"Available items: {np.where(self.available_items)[0]}\")\n",
    "            print(\"====================================\\n\")\n",
    "    \n",
    "    def get_optimal_solution(self):\n",
    "        \"\"\"\n",
    "        Compute the optimal solution for the current problem instance using dynamic programming.\n",
    "        This can be used to evaluate the RL agent's performance.\n",
    "        \n",
    "        Returns:\n",
    "            optimal_value (float): The maximum possible value\n",
    "            optimal_items (list): The indices of items in the optimal solution\n",
    "        \"\"\"\n",
    "        n = len(self.values)\n",
    "        capacity = int(self.initial_capacity * 100)  # Scale to integers for DP\n",
    "        weights = [int(w * 100) for w in self.weights]  # Scale to integers for DP\n",
    "        \n",
    "        # Initialize DP table\n",
    "        dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
    "        \n",
    "        # Fill the DP table\n",
    "        for i in range(1, n + 1):\n",
    "            for w in range(capacity + 1):\n",
    "                if weights[i - 1] <= w:\n",
    "                    dp[i][w] = max(\n",
    "                        self.values[i - 1] + dp[i - 1][w - weights[i - 1]],\n",
    "                        dp[i - 1][w]\n",
    "                    )\n",
    "                else:\n",
    "                    dp[i][w] = dp[i - 1][w]\n",
    "        \n",
    "        # Backtrack to find the items in optimal solution\n",
    "        optimal_value = dp[n][capacity]\n",
    "        optimal_items = []\n",
    "        \n",
    "        w = capacity\n",
    "        for i in range(n, 0, -1):\n",
    "            if dp[i][w] != dp[i - 1][w]:\n",
    "                optimal_items.append(i - 1)\n",
    "                w -= weights[i - 1]\n",
    "        \n",
    "        return optimal_value, optimal_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new episode with random actions\n",
      "\n",
      "==== Knapsack Environment State ====\n",
      "Capacity: 9.70/9.70\n",
      "Current solution value: 0.00\n",
      "Current solution weight: 0.00\n",
      "Selected items: []\n",
      "Available items: [0 1 2 3 4]\n",
      "====================================\n",
      "\n",
      "Action: 3, Reward: 8.97\n",
      "\n",
      "==== Knapsack Environment State ====\n",
      "Capacity: 0.70/9.70\n",
      "Current solution value: 8.97\n",
      "Current solution weight: 9.00\n",
      "Selected items: [np.int64(3)]\n",
      "Available items: [0 1 2 4]\n",
      "====================================\n",
      "\n",
      "Episode finished with total reward: 8.97\n",
      "Optimal solution value: 19.80\n",
      "Optimal solution items: [4, 2, 0]\n",
      "Agent performance ratio: 0.45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of creating the environment and running an episode with random actions\n",
    "def example_random_episode():\n",
    "    # Create environment\n",
    "    env = KnapsackEnv(max_items=5)\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"Starting a new episode with random actions\")\n",
    "    env.render()\n",
    "    \n",
    "    while not done:\n",
    "        # Take a random action\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        total_reward += reward\n",
    "        print(f\"Action: {action}, Reward: {reward:.2f}\")\n",
    "        env.render()\n",
    "    \n",
    "    print(f\"Episode finished with total reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Compare with optimal solution\n",
    "    optimal_value, optimal_items = env.get_optimal_solution()\n",
    "    print(f\"Optimal solution value: {optimal_value:.2f}\")\n",
    "    print(f\"Optimal solution items: {optimal_items}\")\n",
    "    print(f\"Agent performance ratio: {env.current_value/optimal_value if optimal_value > 0 else 0:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_random_episode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
