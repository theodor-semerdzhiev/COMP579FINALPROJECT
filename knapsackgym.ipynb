{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class KnapsackEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for solving 0-1 Knapsack Problem instances using Reinforcement Learning.\n",
    "    This version handles one problem instance at a time.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, problem_instance: Dict):\n",
    "        \"\"\"\n",
    "        Initialize the Knapsack environment with a single problem instance.\n",
    "\n",
    "        Args:\n",
    "            problem_instance (Dict): A dictionary with:\n",
    "                - 'values': List[float] of item values\n",
    "                - 'weights': List[float] of item weights\n",
    "                - 'capacity': float indicating the knapsack capacity\n",
    "        \"\"\"\n",
    "        super(KnapsackEnv, self).__init__()\n",
    "\n",
    "        # Store the current problem instance\n",
    "        self.problem_instance = problem_instance\n",
    "\n",
    "        # Precompute the maximum number of items based on the instance\n",
    "        self.n_items = len(problem_instance['values'])\n",
    "\n",
    "        # Define the action space: select an item index in [0, n_items-1]\n",
    "        self.action_space = spaces.Discrete(self.n_items)\n",
    "\n",
    "        # Define the observation space (2*N + 4 features):\n",
    "        # 1. Number of items (1)\n",
    "        # 2. Normalized values for all items (N)\n",
    "        # 3. Normalized weights for all items (N)\n",
    "        # 4. Capacity (1)\n",
    "        # 5. Total value of all items (1)\n",
    "        # 6. Total weight of all items (1)\n",
    "        # Total = 2*N + 4\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(2 * self.n_items + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Internal state variables (initialized in reset)\n",
    "        self.remaining_capacity = 0.0\n",
    "        self.selected_items = []\n",
    "        self.available_items = []\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        # Keep track of the best value found so far (can be used for logging or analysis)\n",
    "        self.best_value = 0.0\n",
    "\n",
    "        # Initialize environment for the given problem instance\n",
    "        self.reset()\n",
    "\n",
    "    def set_problem_instance(self, new_instance: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Replace the current problem instance with a new one.\n",
    "        You should call `env.reset()` afterwards to start fresh.\n",
    "\n",
    "        Args:\n",
    "            new_instance (Dict): Dictionary with the same keys as before:\n",
    "                - 'values': List[float]\n",
    "                - 'weights': List[float]\n",
    "                - 'capacity': float\n",
    "        \"\"\"\n",
    "        self.problem_instance = new_instance\n",
    "        self.n_items = len(new_instance['values'])\n",
    "\n",
    "        # Update the action and observation spaces if necessary\n",
    "        self.action_space = spaces.Discrete(self.n_items)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(2 * self.n_items + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reset the environment state to start a new episode with the current problem instance.\n",
    "\n",
    "        Returns:\n",
    "            state (np.ndarray): The initial observation (2*N + 4 features).\n",
    "        \"\"\"\n",
    "        # Extract the current problem instance\n",
    "        values = self.problem_instance['values']\n",
    "        weights = self.problem_instance['weights']\n",
    "        capacity = self.problem_instance['capacity']\n",
    "\n",
    "        # Reset internal state\n",
    "        self.remaining_capacity = capacity\n",
    "        self.selected_items = []\n",
    "        self.available_items = list(range(self.n_items))\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        # Normalize values and weights (as in the referenced Equations (4) and (5))\n",
    "        self.normalized_values = self._normalize_values(values, weights, capacity)\n",
    "        self.normalized_weights = self._normalize_weights(weights, capacity)\n",
    "\n",
    "        # Compute total value and total weight\n",
    "        self.total_value = sum(values)\n",
    "        self.total_weight = sum(weights)\n",
    "\n",
    "        # Return the initial state representation\n",
    "        return self._get_state()\n",
    "\n",
    "    def _normalize_values(self, values: List[float], weights: List[float], capacity: float) -> List[float]:\n",
    "        \"\"\"\n",
    "        Normalize values according to Equation (4): vr_i = v_i / (w_i * W_P).\n",
    "        If w_i = 0, handle division by zero carefully (you might define a custom rule).\n",
    "        \"\"\"\n",
    "        normalized = []\n",
    "        for v, w in zip(values, weights):\n",
    "            if w == 0:\n",
    "                # If weight is zero, define a fallback (could be 0 or a large number)\n",
    "                normalized.append(0.0)\n",
    "            else:\n",
    "                normalized.append(v / (w * capacity))\n",
    "        return normalized\n",
    "\n",
    "    def _normalize_weights(self, weights: List[float], capacity: float) -> List[float]:\n",
    "        \"\"\"\n",
    "        Normalize weights according to Equation (5): wr_i = w_i / W_P.\n",
    "        \"\"\"\n",
    "        return [w / capacity for w in weights]\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct the state vector with (2*N + 4) features.\n",
    "        1. Number of items (always total number, N)\n",
    "        2. Normalized values for items (N features; 0 for removed items)\n",
    "        3. Normalized weights for items (N features; 0 for removed items)\n",
    "        4. Capacity (1)\n",
    "        5. Total value (1)\n",
    "        6. Total weight (1)\n",
    "        \"\"\"\n",
    "        state = np.zeros(2 * self.n_items + 4, dtype=np.float32)\n",
    "\n",
    "        # 1) Number of items (total items, not just available)\n",
    "        state[0] = self.n_items\n",
    "\n",
    "        # 2) Normalized values: if the item is not available, set to 0.\n",
    "        for i in range(self.n_items):\n",
    "            if i in self.available_items:\n",
    "                state[1 + i] = self.normalized_values[i]\n",
    "            else:\n",
    "                state[1 + i] = 0.0\n",
    "\n",
    "        # 3) Normalized weights: if the item is not available, set to 0.\n",
    "        for i in range(self.n_items):\n",
    "            if i in self.available_items:\n",
    "                state[1 + self.n_items + i] = self.normalized_weights[i]\n",
    "            else:\n",
    "                state[1 + self.n_items + i] = 0.0\n",
    "\n",
    "        # 4) Capacity\n",
    "        state[1 + 2 * self.n_items] = self.problem_instance['capacity']\n",
    "        # 5) Total value of all items\n",
    "        state[2 + 2 * self.n_items] = self.total_value\n",
    "        # 6) Total weight of all items\n",
    "        state[3 + 2 * self.n_items] = self.total_weight\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Take an action (select an item index) and return:\n",
    "        (new_state, reward, done, info)\n",
    "\n",
    "        If the item cannot fit in the knapsack, it is removed from the available items so that\n",
    "        its corresponding features become 0 in the state vector.\n",
    "        \"\"\"\n",
    "        values = self.problem_instance['values']\n",
    "        weights = self.problem_instance['weights']\n",
    "        capacity = self.problem_instance['capacity']\n",
    "        reward = 0.0\n",
    "\n",
    "        # Check if action is out of bounds.\n",
    "        if action < 0 or action >= self.n_items:\n",
    "            reward = -capacity\n",
    "            self.done = True\n",
    "\n",
    "        # Check if the item is already removed (not available)\n",
    "        elif action not in self.available_items:\n",
    "            reward = -self.normalized_weights[action]\n",
    "\n",
    "        else:\n",
    "            # Attempt to select the item.\n",
    "            item_value = values[action]\n",
    "            item_weight = weights[action]\n",
    "\n",
    "            if item_weight <= self.remaining_capacity:\n",
    "                # Item fits in the knapsack: add it.\n",
    "                self.current_value += item_value\n",
    "                self.current_weight += item_weight\n",
    "                self.remaining_capacity -= item_weight\n",
    "\n",
    "                self.selected_items.append(action)\n",
    "                self.available_items.remove(action)\n",
    "                reward = self.normalized_values[action]\n",
    "            else:\n",
    "                # Item does not fit: apply negative reward and remove it from state.\n",
    "                reward = -self.normalized_weights[action]\n",
    "                self.available_items.remove(action)\n",
    "\n",
    "        # Check if the episode should end.\n",
    "        if len(self.available_items) == 0 or self.remaining_capacity == 0:\n",
    "            self.done = True\n",
    "            if self.current_value > self.best_value:\n",
    "                self.best_value = self.current_value\n",
    "\n",
    "        # Construct the new state.\n",
    "        new_state = self._get_state()\n",
    "\n",
    "        # Prepare the info dictionary.\n",
    "        info = {\n",
    "            'current_value': self.current_value,\n",
    "            'current_weight': self.current_weight,\n",
    "            'remaining_capacity': self.remaining_capacity,\n",
    "            'selected_items': self.selected_items,\n",
    "            'best_value': self.best_value\n",
    "        }\n",
    "\n",
    "        return new_state, reward, self.done, info\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Print out the environment's current state.\n",
    "        \"\"\"\n",
    "        if mode == 'human':\n",
    "            print(\"Knapsack Environment\")\n",
    "            print(f\"Capacity: {self.problem_instance['capacity']}, Remaining: {self.remaining_capacity}\")\n",
    "            print(f\"Selected items: {self.selected_items}\")\n",
    "            print(f\"Current value: {self.current_value}, Current weight: {self.current_weight}\")\n",
    "            print(f\"Available items: {self.available_items}\")\n",
    "            print(f\"Best value so far: {self.best_value}\")\n",
    "            print(f\"Done: {self.done}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knapsack Environment\n",
      "Capacity: 50, Remaining: 50\n",
      "Selected items: []\n",
      "Current value: 0.0, Current weight: 0.0\n",
      "Available items: [0, 1, 2]\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Knapsack Environment\n",
      "Capacity: 50, Remaining: 40\n",
      "Selected items: [np.int64(0)]\n",
      "Current value: 60.0, Current weight: 10.0\n",
      "Available items: [1, 2]\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Knapsack Environment\n",
      "Capacity: 50, Remaining: 0\n",
      "Selected items: [np.int64(0), np.int64(2)]\n",
      "Current value: 180.0, Current weight: 50.0\n",
      "Available items: [1]\n",
      "Best value so far: 180.0\n",
      "Done: True\n",
      "--------------------------------------------------\n",
      "Episode finished with total reward: 0.18\n",
      "Final info: {'current_value': 180.0, 'current_weight': 50.0, 'remaining_capacity': 0, 'selected_items': [np.int64(0), np.int64(2)], 'best_value': 180.0}\n"
     ]
    }
   ],
   "source": [
    "def run_episode(env:KnapsackEnv, policy=None, render=False, max_ite = None):\n",
    "    \"\"\"\n",
    "    Run a single episode in a Gym environment.\n",
    "\n",
    "    Args:\n",
    "        env: An instance of a Gym environment.\n",
    "        policy: Optional function that takes the current state and returns an action.\n",
    "                If None, actions are chosen randomly.\n",
    "        render: Boolean flag to indicate whether to render the environment at each step.\n",
    "\n",
    "    Returns:\n",
    "        total_reward (float): The total accumulated reward from the episode.\n",
    "        episode_info (dict): The final info dictionary returned by the environment.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    ite = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        # Choose an action: use the policy if provided, otherwise sample randomly.\n",
    "        action = policy(state) if policy is not None else env.action_space.sample()\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        ite += 1\n",
    "        if max_ite is not None and ite < max_ite:\n",
    "            break\n",
    "\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    return total_reward, info\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple problem instance\n",
    "    problem_instance = {\n",
    "        'values': [60, 100, 120],\n",
    "        'weights': [10, 20, 40],\n",
    "        'capacity': 50\n",
    "    }\n",
    "\n",
    "    # Create the Knapsack environment (assuming KnapsackEnv is already defined)\n",
    "    env = KnapsackEnv(problem_instance)\n",
    "\n",
    "    # Run an episode using a random policy (since no policy is provided)\n",
    "    total_reward, final_info = run_episode(env, render=True)\n",
    "    print(\"Episode finished with total reward:\", total_reward)\n",
    "    print(\"Final info:\", final_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
