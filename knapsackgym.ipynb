{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KnapsackEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for the 0-1 Knapsack Problem.\n",
    "    This version maintains a state vector of size (2*n + 4) for n items:\n",
    "      - The first 2*n slots hold pairs of (value_i, weight_i) for each item, left-aligned.\n",
    "      - The last 4 slots hold [capacity, sum(values), sum(weights), n_items].\n",
    "    After an item is chosen (whether it fits or not), it is removed from the state by\n",
    "    'shifting' all subsequent items left by 2, and placing zeros at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, problem_instance: Dict):\n",
    "        \"\"\"\n",
    "        Initialize the Knapsack environment with a single problem instance.\n",
    "\n",
    "        Args:\n",
    "            problem_instance (Dict): Must have:\n",
    "                - 'values': List[float] of item values\n",
    "                - 'weights': List[float] of item weights\n",
    "                - 'capacity': float (max knapsack capacity)\n",
    "        \"\"\"\n",
    "        super(KnapsackEnv, self).__init__()\n",
    "\n",
    "        self.problem_instance = problem_instance\n",
    "\n",
    "        # Extract basic info\n",
    "        self.values = list(problem_instance['values'])\n",
    "        self.weights = list(problem_instance['weights'])\n",
    "        self.capacity = float(problem_instance['capacity'])\n",
    "        self.n_items = len(self.values)\n",
    "\n",
    "        # For convenience\n",
    "        self.total_value = sum(self.values)\n",
    "        self.total_weight = sum(self.weights)\n",
    "\n",
    "        # Define the action space: pick an index [0, n_items - 1]\n",
    "        self.action_space = spaces.Discrete(self.n_items)\n",
    "\n",
    "        # The observation space is always (2*n + 4).\n",
    "        #  - 2*n for (value_i, weight_i) pairs\n",
    "        #  - +4 for [capacity, total_value, total_weight, n_items]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(2 * self.n_items + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Internal tracking\n",
    "        self.items = []  # Will hold (value, weight) for each remaining item\n",
    "        self.remaining_capacity = 0.0\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "        self.best_value = 0.0  # Track best solution found\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reset the environment state and return the initial observation.\n",
    "        \"\"\"\n",
    "        # Re-initialize the item list with all items\n",
    "        self.items = [(v, w) for v, w in zip(self.values, self.weights)]\n",
    "        self.remaining_capacity = self.capacity\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        # Reset best_value if desired, or leave it as a persistent stat\n",
    "        # self.best_value = 0.0\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build the state vector of length (2*n_items + 4).\n",
    "          - The first 2*len(self.items) slots: (value_i, weight_i) for each remaining item (left-aligned).\n",
    "          - The next 2*(n_items - len(self.items)) slots: 0.0\n",
    "          - The last 4 slots: [capacity, total_value, total_weight, n_items].\n",
    "        \"\"\"\n",
    "        state = np.zeros(2 * self.n_items + 4, dtype=np.float32)\n",
    "\n",
    "        # Fill in item data for remaining items (left-aligned)\n",
    "        for i, (v, w) in enumerate(self.items):\n",
    "            idx = 2 * i\n",
    "            state[idx] = v\n",
    "            state[idx + 1] = w\n",
    "\n",
    "        # The last 4 features\n",
    "        state[-4] = self.capacity\n",
    "        state[-3] = self.total_value\n",
    "        state[-2] = self.total_weight\n",
    "        state[-1] = self.n_items\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Take an action (select an item index) and return (new_state, reward, done, info).\n",
    "        - If action is invalid (out of range), penalize with -capacity and end the episode.\n",
    "        - Otherwise, remove the item from the 'items' list. If it fits, add its value/weight;\n",
    "          if not, penalize. In both cases, that item is no longer in the state.\n",
    "        - End the episode if no items remain or capacity is exhausted.\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "\n",
    "        # If action is out of range or no items left\n",
    "        if action < 0 or action >= len(self.items):\n",
    "            # Large negative penalty\n",
    "            reward = -self.capacity\n",
    "            self.done = True\n",
    "        else:\n",
    "            # Retrieve the chosen item\n",
    "            item_value, item_weight = self.items[action]\n",
    "\n",
    "            # Check if it fits\n",
    "            if item_weight <= self.remaining_capacity:\n",
    "                # It fits\n",
    "                self.current_value += item_value\n",
    "                self.current_weight += item_weight\n",
    "                self.remaining_capacity -= item_weight\n",
    "\n",
    "                # Positive reward can be the raw item_value, or normalized, etc.\n",
    "                # For now, let's just use the raw item_value\n",
    "                reward = item_value\n",
    "            else:\n",
    "                # Does not fit -> negative reward\n",
    "                reward = -self.capacity\n",
    "\n",
    "            # Remove this item from the state (shifting logic):\n",
    "            # We simply pop from self.items so that in _get_state(),\n",
    "            # everything to the right shifts left automatically.\n",
    "            self.items.pop(action)\n",
    "\n",
    "        # Check if we should end the episode\n",
    "        if len(self.items) == 0 or self.remaining_capacity == 0:\n",
    "            self.done = True\n",
    "            if self.current_value > self.best_value:\n",
    "                self.best_value = self.current_value\n",
    "\n",
    "        # Build the new state\n",
    "        new_state = self._get_state()\n",
    "\n",
    "        info = {\n",
    "            'current_value': self.current_value,\n",
    "            'current_weight': self.current_weight,\n",
    "            'remaining_capacity': self.remaining_capacity,\n",
    "            'best_value': self.best_value,\n",
    "            'items_remaining': len(self.items),\n",
    "        }\n",
    "\n",
    "        return new_state, reward, self.done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Print out the environment state for debugging.\n",
    "        \"\"\"\n",
    "        if mode == 'human':\n",
    "            print(\"Knapsack Environment\")\n",
    "            print(f\"Capacity: {self.capacity}, Remaining: {self.remaining_capacity}\")\n",
    "            print(f\"Current value: {self.current_value}, Current weight: {self.current_weight}\")\n",
    "            print(f\"Items left (value, weight): {self.items}\")\n",
    "            print(f\"Best value so far: {self.best_value}\")\n",
    "            print(f\"Done: {self.done}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KnapsackEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for the 0-1 Knapsack Problem.\n",
    "    This version maintains a state vector of size (2*n + 4) for n items:\n",
    "      - The first 2*n slots hold pairs of (value_i, weight_i) for each item, left-aligned.\n",
    "      - The last 4 slots hold [capacity, sum(values), sum(weights), n_items].\n",
    "    After an item is chosen (whether it fits or not), it is removed from the state by\n",
    "    'shifting' all subsequent items left by 2, and placing zeros at the end.\n",
    "\n",
    "    Reward is normalized:\n",
    "      - v_r_i = v_i / (w_i * W_P) if the item fits.\n",
    "      - -w_r_i = - (w_i / W_P) if the item does not fit.\n",
    "      - -W_P if the action is out of range.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, problem_instance: Dict):\n",
    "        \"\"\"\n",
    "        Initialize the Knapsack environment with a single problem instance.\n",
    "\n",
    "        Args:\n",
    "            problem_instance (Dict): Must have:\n",
    "                - 'values': List[float] of item values\n",
    "                - 'weights': List[float] of item weights\n",
    "                - 'capacity': float (max knapsack capacity)\n",
    "        \"\"\"\n",
    "        super(KnapsackEnv, self).__init__()\n",
    "\n",
    "        self.problem_instance = problem_instance\n",
    "\n",
    "        # Extract basic info\n",
    "        self.values = list(problem_instance['values'])\n",
    "        self.weights = list(problem_instance['weights'])\n",
    "        self.capacity = float(problem_instance['capacity'])\n",
    "        self.n_items = len(self.values)\n",
    "\n",
    "        self.total_value = sum(self.values)\n",
    "        self.total_weight = sum(self.weights)\n",
    "\n",
    "        # Precompute normalized values and weights for the reward function\n",
    "        # v_r_i = v_i / (w_i * capacity), w_r_i = w_i / capacity\n",
    "        self.normalized_values = []\n",
    "        self.normalized_weights = []\n",
    "        for v, w in zip(self.values, self.weights):\n",
    "            if w == 0.0:\n",
    "                # Avoid division by zero; you can choose an alternate scheme\n",
    "                # for zero-weight items (e.g., treat as v / (1 * capacity))\n",
    "                self.normalized_values.append(0.0)\n",
    "            else:\n",
    "                self.normalized_values.append(v / (w * self.capacity))\n",
    "            self.normalized_weights.append(w / self.capacity)\n",
    "\n",
    "        # Define the action space: pick an index [0, n_items - 1]\n",
    "        self.action_space = spaces.Discrete(self.n_items)\n",
    "\n",
    "        # Observation space: 2*n for (value, weight) pairs + 4\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(2 * self.n_items + 4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Internal tracking\n",
    "        # We'll store (value, weight, idx) so we know which normalized entry to use.\n",
    "        self.items: List[Tuple[float, float, int]] = []\n",
    "        self.remaining_capacity = 0.0\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "        self.best_value = 0.0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reset the environment state and return the initial observation.\n",
    "        \"\"\"\n",
    "        # Re-initialize the item list with all items + their original indices\n",
    "        self.items = [\n",
    "            (v, w, i) for i, (v, w) in enumerate(zip(self.values, self.weights))\n",
    "        ]\n",
    "        self.remaining_capacity = self.capacity\n",
    "        self.current_value = 0.0\n",
    "        self.current_weight = 0.0\n",
    "        self.done = False\n",
    "        # self.best_value = 0.0  # Uncomment if you want to reset best_value each episode\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build the state vector of length (2*n_items + 4).\n",
    "          - The first 2*len(self.items) slots: (value_i, weight_i) for each remaining item (left-aligned).\n",
    "          - The next 2*(n_items - len(self.items)) slots: 0.0\n",
    "          - The last 4 slots: [capacity, total_value, total_weight, n_items].\n",
    "        \"\"\"\n",
    "        state = np.zeros(2 * self.n_items + 4, dtype=np.float32)\n",
    "\n",
    "        # Fill in item data for remaining items (left-aligned)\n",
    "        for i, (v, w, idx) in enumerate(self.items):\n",
    "            pos = 2 * i\n",
    "            state[pos] = v\n",
    "            state[pos + 1] = w\n",
    "\n",
    "        # The last 4 features\n",
    "        state[-4] = self.capacity\n",
    "        state[-3] = self.total_value\n",
    "        state[-2] = self.total_weight\n",
    "        state[-1] = self.n_items\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Take an action (select an item index) and return (new_state, reward, done, info).\n",
    "        Reward logic:\n",
    "          1) If action is out of range => reward = -W_P.\n",
    "          2) Else pick the item; if it fits => reward = v_r_i; else => reward = - w_r_i.\n",
    "          3) Remove the item from self.items in all cases (shifting the state).\n",
    "        End the episode if no items remain or if capacity is exhausted.\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "\n",
    "        # Out-of-range or invalid action => large negative penalty\n",
    "        if action < 0 or action >= len(self.items):\n",
    "            reward = -self.capacity\n",
    "            self.done = True\n",
    "        else:\n",
    "            # Retrieve the chosen item\n",
    "            item_value, item_weight, item_idx = self.items[action]\n",
    "\n",
    "            # Check if it fits in the *remaining* capacity\n",
    "            if item_weight <= self.remaining_capacity:\n",
    "                # Positive reward: v_r_i = v_i / (w_i * W_P)\n",
    "                reward = self.normalized_values[item_idx]\n",
    "\n",
    "                # Update current knapsack usage\n",
    "                self.current_value += item_value\n",
    "                self.current_weight += item_weight\n",
    "                self.remaining_capacity -= item_weight\n",
    "            else:\n",
    "                # Negative reward: - w_r_i = - (w_i / W_P)\n",
    "                reward = -self.normalized_weights[item_idx]\n",
    "\n",
    "            # Remove this item from the state (pop shifts everything to the left).\n",
    "            self.items.pop(action)\n",
    "\n",
    "        # Check if we should end the episode\n",
    "        if len(self.items) == 0 or self.remaining_capacity == 0:\n",
    "            self.done = True\n",
    "            # Update best_value if the current solution is better\n",
    "            if self.current_value > self.best_value:\n",
    "                self.best_value = self.current_value\n",
    "\n",
    "        # Build the new state\n",
    "        new_state = self._get_state()\n",
    "\n",
    "        info = {\n",
    "            'current_value': self.current_value,\n",
    "            'current_weight': self.current_weight,\n",
    "            'remaining_capacity': self.remaining_capacity,\n",
    "            'best_value': self.best_value,\n",
    "            'items_remaining': len(self.items),\n",
    "        }\n",
    "\n",
    "        return new_state, reward, self.done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Print out the environment state for debugging.\n",
    "        \"\"\"\n",
    "        if mode == 'human':\n",
    "            print(\"Knapsack Environment\")\n",
    "            print(f\"Capacity: {self.capacity}, Remaining: {self.remaining_capacity}\")\n",
    "            print(f\"Current value: {self.current_value}, Current weight: {self.current_weight}\")\n",
    "            print(\"Items left (value, weight, idx):\")\n",
    "            for it in self.items:\n",
    "                print(\"  \", it)\n",
    "            print(f\"Best value so far: {self.best_value}\")\n",
    "            print(f\"Done: {self.done}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 50.0\n",
      "Current value: 0.0, Current weight: 0.0\n",
      "Items left (value, weight, idx):\n",
      "   (60, 10, 0)\n",
      "   (100, 20, 1)\n",
      "   (120, 40, 2)\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 40.0\n",
      "Current value: 60.0, Current weight: 10.0\n",
      "Items left (value, weight, idx):\n",
      "   (100, 20, 1)\n",
      "   (120, 40, 2)\n",
      "Best value so far: 0.0\n",
      "Done: False\n",
      "--------------------------------------------------\n",
      "Knapsack Environment\n",
      "Capacity: 50.0, Remaining: 0.0\n",
      "Current value: 180.0, Current weight: 50.0\n",
      "Items left (value, weight, idx):\n",
      "   (100, 20, 1)\n",
      "Best value so far: 180.0\n",
      "Done: True\n",
      "--------------------------------------------------\n",
      "Episode finished with total reward: 0.18\n",
      "Final info: {'current_value': 180.0, 'current_weight': 50.0, 'remaining_capacity': 0.0, 'best_value': 180.0, 'items_remaining': 1}\n"
     ]
    }
   ],
   "source": [
    "def run_episode(env:KnapsackEnv, policy=None, render=False, max_ite = None):\n",
    "    \"\"\"\n",
    "    Run a single episode in a Gym environment.\n",
    "\n",
    "    Args:\n",
    "        env: An instance of a Gym environment.\n",
    "        policy: Optional function that takes the current state and returns an action.\n",
    "                If None, actions are chosen randomly.\n",
    "        render: Boolean flag to indicate whether to render the environment at each step.\n",
    "\n",
    "    Returns:\n",
    "        total_reward (float): The total accumulated reward from the episode.\n",
    "        episode_info (dict): The final info dictionary returned by the environment.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    ite = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        # Choose an action: use the policy if provided, otherwise sample randomly.\n",
    "        action = policy(state) if policy is not None else env.action_space.sample()\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        ite += 1\n",
    "        if max_ite is not None and ite < max_ite:\n",
    "            break\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    return total_reward, info\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple problem instance\n",
    "    problem_instance = {\n",
    "        'values': [60, 100, 120],\n",
    "        'weights': [10, 20, 40],\n",
    "        'capacity': 50\n",
    "    }\n",
    "\n",
    "    # Create the Knapsack environment (assuming KnapsackEnv is already defined)\n",
    "    env = KnapsackEnv(problem_instance)\n",
    "\n",
    "    # Run an episode using a random policy (since no policy is provided)\n",
    "    total_reward, final_info = run_episode(env, render=True)\n",
    "    print(\"Episode finished with total reward:\", total_reward)\n",
    "    print(\"Final info:\", final_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
