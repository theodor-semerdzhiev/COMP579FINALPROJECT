{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knapsackgym import KnapsackEnv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network (default: 64 x 64 (2 hidden layers), sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for the A2C algorithm to solve the Knapsack Problem\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        output_size: int, \n",
    "        hidden_size: int = 64, \n",
    "        num_layers: int = 2, \n",
    "        activation: str = \"sigmoid\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the policy network with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of the input vector (2N + 4)\n",
    "            output_size (int): Size of the output vector (N)\n",
    "            hidden_size (int): Size of the hidden layers (default: 64)\n",
    "            num_layers (int): Number of hidden layers (default: 2)\n",
    "            activation (str): Activation function to use (default: \"sigmoid\")\n",
    "                              Options: \"sigmoid\", \"relu\", \"tanh\", \"leaky_relu\"\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.activation_funcs:Dict[str, Callable] = {\n",
    "            \"sigmoid\":F.sigmoid, \n",
    "            \"relu\": F.relu, \n",
    "            \"tanh\": F.tanh, \n",
    "            \"leaky_relu\": F.leaky_relu\n",
    "        }\n",
    "\n",
    "        # Set activation function\n",
    "        if activation not in self.activation_funcs:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.activation = self.activation_funcs[activation]\n",
    "        \n",
    "        # Create layers dynamically\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits\n",
    "        \"\"\"\n",
    "        # Pass through all layers except the last one with activation\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        \n",
    "        # No activation on the output layer\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def get_action(self, state: Union[List[float], torch.Tensor, np.ndarray], \n",
    "                  available_actions: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Get action according to policy.\n",
    "        \n",
    "        Args:\n",
    "            state (Union[List[float], torch.Tensor, numpy.ndarray]): Current state\n",
    "            available_actions (List[int]): List of available actions\n",
    "            \n",
    "        Returns:\n",
    "            int: Chosen action index\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Convert state to tensor if it's not already\n",
    "            if not isinstance(state, torch.Tensor):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            else:\n",
    "                state_tensor = state.unsqueeze(0) if state.dim() == 1 else state\n",
    "                \n",
    "            action_probs = F.softmax(self.forward(state_tensor), dim=1)\n",
    "            \n",
    "            # Mask unavailable actions\n",
    "            action_mask = torch.zeros_like(action_probs)\n",
    "            for action in available_actions:\n",
    "                if action < action_mask.shape[1]:\n",
    "                    action_mask[0, action] = 1\n",
    "            \n",
    "            masked_probs = action_probs * action_mask\n",
    "            \n",
    "            # If all actions are masked, choose randomly from available actions\n",
    "            if torch.sum(masked_probs) == 0:\n",
    "                return random.choice(available_actions)\n",
    "            \n",
    "            # Normalize probabilities\n",
    "            masked_probs = masked_probs / torch.sum(masked_probs)\n",
    "            \n",
    "            # Sample action from the masked probability distribution\n",
    "            action = torch.multinomial(masked_probs, 1).item()\n",
    "            \n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Network (default: 64 x 64 (2 hidden layers), sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for the A2C algorithm to estimate state values\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        hidden_size: int = 64, \n",
    "        num_layers: int = 2, \n",
    "        activation: str = \"sigmoid\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the value network with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of the input vector (2N + 4)\n",
    "            hidden_size (int): Size of the hidden layers (default: 64)\n",
    "            num_layers (int): Number of hidden layers (default: 2)\n",
    "            activation (str): Activation function to use (default: \"sigmoid\")\n",
    "                             Options: \"sigmoid\", \"relu\", \"tanh\", \"leaky_relu\"\n",
    "        \"\"\"\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Dictionary mapping activation function names to their implementations\n",
    "        self.activation_functions: Dict[str, Callable] = {\n",
    "            \"sigmoid\": F.sigmoid,\n",
    "            \"relu\": F.relu,\n",
    "            \"tanh\": F.tanh,\n",
    "            \"leaky_relu\": F.leaky_relu\n",
    "        }\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}. \"\n",
    "                            f\"Supported options are: {list(self.activation_functions.keys())}\")\n",
    "        \n",
    "        self.activation = self.activation_functions[activation]\n",
    "        \n",
    "        # Create layers dynamically\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            \n",
    "        # Output layer - value networks output a single scalar value\n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Estimated state value\n",
    "        \"\"\"\n",
    "        # Pass through all layers except the last one with activation\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        \n",
    "        # No activation on the output layer for value estimation\n",
    "        return self.layers[-1](x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Aggregator (Placeholder for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Just temporary for now, should be moved to seperate file\n",
    "class StateAggregator:\n",
    "    \"\"\"\n",
    "    State aggregator to reduce state space dimension.\n",
    "    Implementation placeholder as mentioned in requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the state aggregator\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def aggregate(self, state):\n",
    "        \"\"\"\n",
    "        Aggregate state according to equation (7) as mentioned in pseudocode.\n",
    "        This is a placeholder implementation.\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Original state\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Aggregated state\n",
    "        \"\"\"\n",
    "        # For now, simply return the original state (no aggregation)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main KnapsackDRLSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KnapsackDRLSolver:\n",
    "    \"\"\"DRL-based Knapsack Solver using A2C algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, env, N=100, use_state_aggregation=False, gamma=0.99, lr_policy=0.001, lr_value=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize the DRL solver.\n",
    "        \n",
    "        Args:\n",
    "            env: Gym environment for the knapsack problem\n",
    "            N (int): Maximum number of items in a problem instance\n",
    "            use_state_aggregation (bool): Whether to use state aggregation\n",
    "            gamma (float): Discount factor\n",
    "            lr_policy (float): Learning rate for policy network\n",
    "            lr_value (float): Learning rate for value network\n",
    "        \"\"\"\n",
    "        self.env:KnapsackEnv = env\n",
    "        self.N = N\n",
    "        self.use_state_aggregation = use_state_aggregation\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Define input and output sizes for networks\n",
    "        input_size = 2 * N + 4  # As specified in the requirements\n",
    "        output_size = N  # One output per potential item\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.policy_net = PolicyNetwork(input_size, output_size)\n",
    "        self.value_net = ValueNetwork(input_size)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        # Initialize state aggregator if needed\n",
    "        self.state_aggregator = StateAggregator() if use_state_aggregation else None\n",
    "\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def process_state(self, state):\n",
    "        \"\"\"\n",
    "        Process state with optional aggregation.\n",
    "        \n",
    "        Args:\n",
    "            state (numpy.ndarray): Original state\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Processed state\n",
    "        \"\"\"\n",
    "        if self.use_state_aggregation and self.state_aggregator is not None:\n",
    "            return self.state_aggregator.aggregate(state)\n",
    "        return state\n",
    "    \n",
    "    def train(self, problem_instances, t_max=None):\n",
    "        \"\"\"\n",
    "        Train the DRL solver on multiple problem instances with progress tracking.\n",
    "        \n",
    "        Args:\n",
    "            problem_instances (List[Dict]): List of problem instances\n",
    "            t_max (int): Maximum number of training steps\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: Values of solutions for each problem instance\n",
    "        \"\"\"\n",
    "        assert len(problem_instances) is not None or len(problem_instances) > 0\n",
    "\n",
    "        if t_max is None:\n",
    "            t_max = 3 * self.N * 10000  # As specified in the pseudocode\n",
    "        \n",
    "        val = [0] * len(problem_instances)  # Initialize solution values\n",
    "        \n",
    "        print(f\"Training on {len(problem_instances)} KP Instances, with N={self.N}, t_max={t_max}\")\n",
    "        for t in range(t_max):\n",
    "\n",
    "            # Select a problem instance (line 6 in pseudocode)\n",
    "            # P_idx = np.random.randint(0, len(problem_instances))\n",
    "            P_idx = t % len(problem_instances)\n",
    "            P = problem_instances[P_idx]\n",
    "            \n",
    "            assert len(P['values'] )<= self.N, f\"Problem Instance has too many items. KnapsackEnv is configuered to accept no more than <= {self.N}.\"\n",
    "\n",
    "            # Change the environment to use this problem instance\n",
    "            self.env.change_problem_instance(P)\n",
    "            \n",
    "            # Reset environment and get initial state\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # Initialize for this episode\n",
    "            done = False\n",
    "            ow = 0  # Total weight of selected items\n",
    "            ov = 0  # Total value of selected items\n",
    "            \n",
    "            # Create a copy of the problem instance P for modification\n",
    "            n_P_prime = len(P['values'])\n",
    "            W_P_prime = P['capacity']\n",
    "            \n",
    "            # Store states, actions, rewards for batch update\n",
    "            states:List[np.ndarray] = []\n",
    "            actions:List[int] = []\n",
    "            rewards:List[float] = []\n",
    "            next_states:List[np.ndarray] = []\n",
    "            dones:List[bool] = []\n",
    "            \n",
    "            # Track episode rewards\n",
    "            episode_rewards:List[float] = []\n",
    "            \n",
    "            # Solve the knapsack problem for this instance\n",
    "            while ow < W_P_prime and n_P_prime > 0 and not done:\n",
    "                # Process state if needed\n",
    "                processed_state = self.process_state(state)\n",
    "                states.append(processed_state)\n",
    "                \n",
    "                # Get available actions (indices of remaining items)\n",
    "                available_actions = list(range(len(self.env.items)))\n",
    "                \n",
    "                # Get action according to policy (line 12 in pseudocode)\n",
    "                action = self.policy_net.get_action(processed_state, available_actions)\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Take action and observe reward and next state\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                rewards.append(reward)\n",
    "                episode_rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "                \n",
    "                # Check if item fits in knapsack (line 13 in pseudocode)\n",
    "                if action < n_P_prime and info['current_weight'] - ow <= W_P_prime:\n",
    "                    # Update totals (line 14 in pseudocode)\n",
    "                    ow = info['current_weight']\n",
    "                    ov = info['current_value']\n",
    "                    W_P_prime = P['capacity'] - ow\n",
    "                \n",
    "                # Update P_prime (line 16 in pseudocode)\n",
    "                n_P_prime -= 1\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "            \n",
    "            # Update parameters using collected trajectories\n",
    "            self.update_parameters(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            if self.verbose and t % 1000 == 0:\n",
    "                print(f\"Iteration [{t}/{t_max}], Training KP Instance {P_idx}, Reward: {sum(rewards) / len(rewards)}\")\n",
    "\n",
    "\n",
    "            # Update best value if needed (lines 20-22 in pseudocode)\n",
    "            if ov > val[P_idx]:\n",
    "                val[P_idx] = ov\n",
    "            \n",
    "        return val\n",
    "    \n",
    "    def update_parameters(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Update policy and value network parameters using collected trajectories.\n",
    "        \n",
    "        Args:\n",
    "            states (List[numpy.ndarray]): List of states\n",
    "            actions (List[int]): List of actions\n",
    "            rewards (List[float]): List of rewards\n",
    "            next_states (List[numpy.ndarray]): List of next states\n",
    "            dones (List[bool]): List of done flags\n",
    "        \"\"\"\n",
    "        # Convert to tensors\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.LongTensor(actions)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute state values\n",
    "        state_values = self.value_net(states_tensor).squeeze()\n",
    "        next_state_values = self.value_net(next_states_tensor).squeeze()\n",
    "        \n",
    "        # Compute returns and advantages for A2C\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        R = 0\n",
    "        \n",
    "        # Compute returns with bootstrapping\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + self.gamma * R * (1 - dones[i])\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns_tensor = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Compute advantage = returns - state_values\n",
    "        advantages = returns_tensor - state_values.detach()\n",
    "        \n",
    "        # Update value network (equation 3 in pseudocode)\n",
    "        value_loss = F.mse_loss(state_values, returns_tensor)\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Update policy network (equation 2 in pseudocode)\n",
    "        policy_output = self.policy_net(states_tensor)\n",
    "        action_probs = F.softmax(policy_output, dim=1)\n",
    "        \n",
    "        # Extract probabilities of chosen actions\n",
    "        action_log_probs = F.log_softmax(policy_output, dim=1)\n",
    "        selected_action_log_probs = action_log_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        policy_loss = -(selected_action_log_probs * advantages).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "    \n",
    "    def solve(self, problem_instance):\n",
    "        \"\"\"\n",
    "        Solve a single knapsack problem instance using the trained policy.\n",
    "        \n",
    "        Args:\n",
    "            problem_instance (Dict): A problem instance\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[float, List[int]]: Total value and list of selected item indices\n",
    "        \"\"\"\n",
    "        # Set environment to use this problem instance\n",
    "        self.env.change_problem_instance(problem_instance)\n",
    "        \n",
    "        # Reset environment and get initial state\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        done = False\n",
    "        total_value = 0\n",
    "        total_weight = 0\n",
    "        selected_items = []\n",
    "        \n",
    "        while not done:\n",
    "            # Process state if needed\n",
    "            processed_state = self.process_state(state)\n",
    "            \n",
    "            # Get available actions (indices of remaining items)\n",
    "            available_actions = list(range(len(self.env.items)))\n",
    "            \n",
    "            if not available_actions:\n",
    "                break\n",
    "                \n",
    "            # Get action according to policy\n",
    "            action = self.policy_net.get_action(processed_state, available_actions)\n",
    "            \n",
    "            # Get original item index (before any removal)\n",
    "            original_item_idx = self.env.items[action][2]\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            # If item was added (positive reward means item fit)\n",
    "            if reward > 0:\n",
    "                selected_items.append(original_item_idx)\n",
    "            \n",
    "            # Update value and weight\n",
    "            total_value = info['current_value']\n",
    "            total_weight = info['current_weight']\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        return total_value, selected_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_knapsack_dp(problem: Dict[str, Any]) -> Tuple[float, List[int]]:\n",
    "    \"\"\"\n",
    "    Solves the 0-1 Knapsack Problem using Dynamic Programming.\n",
    "    \n",
    "    Args:\n",
    "        problem (Dict): A dictionary containing:\n",
    "            - 'values': List of item values\n",
    "            - 'weights': List of item weights\n",
    "            - 'capacity': Knapsack capacity\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, List[int]]: A tuple containing:\n",
    "            - The maximum value achievable\n",
    "            - A list of indices of the selected items\n",
    "    \"\"\"\n",
    "    values = problem['values']\n",
    "    weights = problem['weights']\n",
    "    capacity = problem['capacity']\n",
    "    n = len(values)\n",
    "    \n",
    "    # Create DP table: rows are items, columns are capacities (0 to W)\n",
    "    dp = np.zeros((n + 1, capacity + 1), dtype=float)\n",
    "    \n",
    "    # Fill the DP table\n",
    "    for i in range(1, n + 1):\n",
    "        for w in range(capacity + 1):\n",
    "            # If item i-1 can fit in knapsack of capacity w\n",
    "            if weights[i-1] <= w:\n",
    "                # Max of (including this item, excluding this item)\n",
    "                dp[i, w] = max(values[i-1] + dp[i-1, w-weights[i-1]], dp[i-1, w])\n",
    "            else:\n",
    "                # Cannot include this item\n",
    "                dp[i, w] = dp[i-1, w]\n",
    "    \n",
    "    # Backtrack to find the selected items\n",
    "    selected_items = []\n",
    "    w = capacity\n",
    "    for i in range(n, 0, -1):\n",
    "        # If this item was included\n",
    "        if dp[i, w] != dp[i-1, w]:\n",
    "            selected_items.append(i-1)  # Item index is i-1\n",
    "            w -= weights[i-1]\n",
    "    \n",
    "    # Reverse the list to get items in original order\n",
    "    selected_items.reverse()\n",
    "    \n",
    "    return dp[n, capacity], selected_items\n",
    "\n",
    "def train_knapsack_solver(env, problem_instances, N=100, use_state_aggregation=False, gamma=0.99, \n",
    "                         lr_policy=0.001, lr_value=0.001, t_max=None, verbose=True, compute_optimal=True):\n",
    "    \"\"\"\n",
    "    Train a DRL-based knapsack solver on multiple problem instances.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment for knapsack problem\n",
    "        problem_instances (List[Dict]): List of problem instances\n",
    "        N (int): Maximum number of items in a problem instance\n",
    "        use_state_aggregation (bool): Whether to use state aggregation\n",
    "        gamma (float): Discount factor\n",
    "        lr_policy (float): Learning rate for policy network\n",
    "        lr_value (float): Learning rate for value network\n",
    "        t_max (int): Maximum number of training steps\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[KnapsackDRLSolver, List[float]]: Trained solver and solution values\n",
    "    \"\"\"\n",
    "    # Initialize solver\n",
    "    solver = KnapsackDRLSolver(\n",
    "        env=env,\n",
    "        N=N,\n",
    "        use_state_aggregation=use_state_aggregation,\n",
    "        gamma=gamma,\n",
    "        lr_policy=lr_policy,\n",
    "        lr_value=lr_value,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Train solver\n",
    "    solution_values = solver.train(problem_instances, t_max)\n",
    "    \n",
    "    optimal_sols = [None] * len(problem_instances)\n",
    "\n",
    "    if compute_optimal:\n",
    "        for i, instance in enumerate(problem_instances):\n",
    "            opt_sol, opt_items = solve_knapsack_dp(instance)\n",
    "            optimal_sols[i] = opt_sol\n",
    "\n",
    "    return solver, solution_values, optimal_sols\n",
    "\n",
    "\n",
    "def evaluate_knapsack_solver(solver:KnapsackDRLSolver, test_instances:list[dict]):\n",
    "    \"\"\"\n",
    "    Evaluate the trained solver on test instances.\n",
    "    \n",
    "    Args:\n",
    "        solver (KnapsackDRLSolver): Trained solver\n",
    "        test_instances (List[Dict]): List of test problem instances\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Evaluation results for each test instance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, instance in enumerate(test_instances):\n",
    "        print(f\"Training on KP Instance {i}\")\n",
    "\n",
    "        # Solve instance\n",
    "        value, selected_items = solver.solve(instance)\n",
    "        \n",
    "        # Calculate actual total weight\n",
    "        total_weight = sum(instance['weights'][idx] for idx in selected_items)\n",
    "        \n",
    "        # Check if solution respects capacity constraint\n",
    "        is_valid = total_weight <= instance['capacity']\n",
    "\n",
    "        optimal_value, optimal_items = solve_knapsack_dp(instance)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'instance_idx': i,\n",
    "            'total_value': value,\n",
    "            'total_weight': total_weight,\n",
    "            'capacity': instance['capacity'],\n",
    "            'selected_items': selected_items,\n",
    "            'is_valid': is_valid,\n",
    "            \"optimal_sol\": optimal_value,\n",
    "            \"optimal_items\": optimal_items,\n",
    "            \"optimality_ratio\": value /  optimal_value \n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2 KP Instances, with N=20, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.19259259259259257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/lykx91p150q4wl3w7w5ypwdh0000gn/T/ipykernel_42447/720289629.py:189: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(state_values, returns_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.29166666666666663\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.29166666666666663\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.29166666666666663\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.29166666666666663\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.32499999999999996\n",
      "Trained solution values: [54.0, 64.0]\n",
      "Optimal solution values: [59.0, 64.0]\n",
      "Training on KP Instance 0\n",
      "Evaluation results: [{'instance_idx': 0, 'total_value': 30.0, 'total_weight': 15, 'capacity': 20, 'selected_items': [0, 1], 'is_valid': True, 'optimal_sol': 40.0, 'optimal_items': [0, 2], 'optimality_ratio': 0.75}]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create problem instances\n",
    "    problem_instances = [\n",
    "        {\n",
    "            'values': [10, 5, 15, 7, 6, 18, 3, 20],\n",
    "            'weights': [2, 3, 5, 7, 1, 4, 1, 5],\n",
    "            'capacity': 15\n",
    "        },\n",
    "        {\n",
    "            'values': [20, 30, 15, 25, 10, 14, 2, 6],\n",
    "            'weights': [5, 10, 8, 12, 4, 2, 5, 20],\n",
    "            'capacity': 20\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    N = 20\n",
    "    t_max = 10000\n",
    "    env:KnapsackEnv = KnapsackEnv(problem_instance=None, N=N)\n",
    "\n",
    "    # Train solver\n",
    "    solver, solution_values, optimal_sol = train_knapsack_solver(\n",
    "        env=env,\n",
    "        problem_instances=problem_instances,\n",
    "        N=N,\n",
    "        use_state_aggregation=False,\n",
    "        t_max=t_max, # Set a smaller value for testing\n",
    "        verbose=True,\n",
    "        compute_optimal=True\n",
    "    )\n",
    "    \n",
    "    print(\"Trained solution values:\", solution_values)\n",
    "    print(\"Optimal solution values:\", optimal_sol)\n",
    "    \n",
    "    # Evaluate on test instances\n",
    "    test_instances = [\n",
    "        {\n",
    "            'values': [10, 20, 30],\n",
    "            'weights': [5, 10, 15],\n",
    "            'capacity': 20\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    solver.verbose = False\n",
    "    results = evaluate_knapsack_solver(solver, test_instances)\n",
    "    print(\"Evaluation results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
