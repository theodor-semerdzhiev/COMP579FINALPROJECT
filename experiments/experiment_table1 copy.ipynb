{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import go here\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "\n",
    "from environment.knapsackgym import KnapsackEnv, _1_positive_reward, _1_negative_reward, v_i_positive_reward, vr_i_positive_reward, w_i_negative_reward, wr_i_negative_reward\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from models.DP_Knapsack import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "from models.Greedy_Knapsack import solve_problem_instances_greedy\n",
    "from models.KnapsackPPO import KnapsackPPOSolver\n",
    "from models.KnapsackA2C import KnapsackA2C\n",
    "from models.KnapsackQLearning import KnapsackDQN\n",
    "from util.instance_gen import KnapsackInstanceGenerator\n",
    "from util.metrics import evaluate_knapsack_performance\n",
    "from models.KnapsackDRLSolver import KnapsackDRLSolver, run_KPSolver\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Tuple, Callable\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_reward_functions_single_run(\n",
    "    KPSolver_A2C: KnapsackA2C,\n",
    "    KPSolver_DQN: KnapsackDQN,\n",
    "    KPSolver_PPO: KnapsackPPOSolver,\n",
    "    M: int = 10,\n",
    "    instance_type: str = \"RI\",\n",
    "    N: int = 50,\n",
    "    r_range: int = 500,\n",
    "    seed: int = 42,\n",
    "    t_max: int = None,\n",
    "    use_state_aggregation: bool = False,\n",
    "    n_test_instances: int = 5,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test different reward function combinations across multiple RL models for the Knapsack problem.\n",
    "    \n",
    "    Args:\n",
    "        M (int): Number of problem instances to generate for training\n",
    "        instance_type (str): Type of knapsack instances to generate ('RI', 'FI', 'HI', 'SS')\n",
    "        N (int): Maximum number of items per problem instance\n",
    "        r_range (int): Range parameter for instance generation\n",
    "        seed (int): Random seed for reproducibility\n",
    "        t_max (int): Maximum training steps (if None, will use default 3*N*10000)\n",
    "        use_state_aggregation (bool): Whether to use state aggregation\n",
    "        n_test_instances (int): Number of test instances to evaluate on\n",
    "        verbose (bool): Whether to print detailed progress information\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results and metrics for all experiments\n",
    "    \"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Define the reward function combinations to test\n",
    "    positive_reward_functions = {\n",
    "        'v_i': v_i_positive_reward,\n",
    "        'vr_i': vr_i_positive_reward,\n",
    "        '_1': _1_positive_reward\n",
    "    }\n",
    "    \n",
    "    negative_reward_functions = {\n",
    "        'w_i': w_i_negative_reward,\n",
    "        'wr_i': wr_i_negative_reward,\n",
    "        '_1': _1_negative_reward\n",
    "    }\n",
    "    \n",
    "    # Generate problem instances\n",
    "    gen = KnapsackInstanceGenerator(seed=seed)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Generating {M} {instance_type} training instances with N={N}, R={r_range}\")\n",
    "    \n",
    "    if instance_type == \"RI\":\n",
    "        training_instances = gen.generate_random_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_random_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    elif instance_type == \"FI\":\n",
    "        training_instances = gen.generate_fixed_instances(M, N, seed=seed)\n",
    "        test_instances = gen.generate_fixed_instances(n_test_instances, N, seed=seed+100)\n",
    "    elif instance_type == \"HI\":\n",
    "        training_instances = gen.generate_hard_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_hard_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    elif instance_type == \"SS\":\n",
    "        training_instances = gen.generate_subset_sum_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_subset_sum_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown instance type: {instance_type}\")\n",
    "    \n",
    "    # Solve instances with DP and Greedy for baselines\n",
    "    if verbose: print(\"Computing DP optimal solutions for training instances...\")\n",
    "    dp_sols_items_train, dp_values_train, dp_weight_train = solve_KP_instances_with_DP(training_instances)\n",
    "\n",
    "    if verbose: print(\"Computing Greedy solutions for training instances...\")\n",
    "    greedy_values_train, greedy_sols_items_train, greedy_weights_train = solve_problem_instances_greedy(training_instances)\n",
    "    \n",
    "    if verbose: print(\"Computing DP optimal solutions for test instances...\")\n",
    "    dp_sols_items_test, dp_values_test, dp_weight_test = solve_KP_instances_with_DP(test_instances)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Computing Greedy solutions for test instances...\")\n",
    "    greedy_values_test, greedy_sols_items_test, greedy_weights_test = solve_problem_instances_greedy(test_instances)\n",
    "    \n",
    "    # Define models to test\n",
    "    model_constructors = {}\n",
    "    if KPSolver_A2C is not None: model_constructors[\"A2C\"] = KPSolver_A2C\n",
    "    if KPSolver_DQN is not None: model_constructors[\"DQN\"] = KPSolver_DQN\n",
    "    if KPSolver_PPO is not None: model_constructors[\"PPO\"] = KPSolver_PPO\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'training': {},\n",
    "        'test': {},\n",
    "        'metrics': {},\n",
    "        'config': {\n",
    "            'num_instances': M,\n",
    "            'instance_type': instance_type,\n",
    "            'n_items': N,\n",
    "            'r_range': r_range,\n",
    "            'seed': seed,\n",
    "            't_max': t_max,\n",
    "            'use_state_aggregation': use_state_aggregation\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create all combinations of reward functions and models\n",
    "    reward_combinations = list(itertools.product(\n",
    "        positive_reward_functions.items(),\n",
    "        negative_reward_functions.items()\n",
    "    ))\n",
    "    \n",
    "    # Total count of experiments\n",
    "    total_experiments = len(reward_combinations) * len(model_constructors)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Running {total_experiments} experiments...\")\n",
    "    \n",
    "    experiment_counter = 0\n",
    "    \n",
    "    # Run experiments for each model and reward function combination\n",
    "    for model_name, model in model_constructors.items():\n",
    "        results['training'][model_name] = {}\n",
    "        results['test'][model_name] = {}\n",
    "        results['metrics'][model_name] = {}\n",
    "        \n",
    "        for (pos_name, pos_func), (neg_name, neg_func) in reward_combinations:\n",
    "            experiment_counter += 1\n",
    "            reward_combo_name = f\"(+{pos_name} -{neg_name})\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nExperiment {experiment_counter}/{total_experiments}: Testing {model_name} with {reward_combo_name}\")\n",
    "                print(f\"Positive reward: {pos_name}, Negative reward: {neg_name}\")\n",
    "            \n",
    "            # Create environment with specific reward functions\n",
    "            env = KnapsackEnv(\n",
    "                problem_instance=None,\n",
    "                N=N,\n",
    "                positive_reward_function=pos_func,\n",
    "                negative_reward_function=neg_func\n",
    "            )\n",
    "            \n",
    "            # Initialize the model\n",
    "            kp_solver = model\n",
    "            \n",
    "            # Train the model\n",
    "            start_time = time.time()\n",
    "            \n",
    "            solver, solution_values = run_KPSolver(\n",
    "                env=env,\n",
    "                KPSolver=kp_solver,\n",
    "                training_problem_instances=training_instances,\n",
    "                t_max=t_max,\n",
    "                use_state_aggregation=use_state_aggregation,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Store training results\n",
    "            results['training'][model_name][reward_combo_name] = {\n",
    "                'solution_values': solution_values,\n",
    "                'training_time': training_time\n",
    "            }\n",
    "            \n",
    "            # Evaluate on test instances\n",
    "            test_values = []\n",
    "            for instance in test_instances:\n",
    "                env.change_problem_instance(instance)\n",
    "                # value, weight, _ = kp_solver.solve(instance)\n",
    "                value, weight, _ = solver.solve(instance)\n",
    "                test_values.append(value)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            \n",
    "            # For training instances\n",
    "            train_best_values = solution_values['instance_best_values']\n",
    "            train_metrics = evaluate_knapsack_performance(\n",
    "                train_best_values, \n",
    "                dp_values_train, \n",
    "                greedy_values_train\n",
    "            )\n",
    "            \n",
    "            # For test instances\n",
    "            test_metrics = evaluate_knapsack_performance(\n",
    "                test_values,\n",
    "                dp_values_test,\n",
    "                greedy_values_test\n",
    "            )\n",
    "            \n",
    "            # Store test results and metrics\n",
    "            results['test'][model_name][reward_combo_name] = {\n",
    "                'values': test_values,\n",
    "                'metrics': test_metrics\n",
    "            }\n",
    "            \n",
    "            results['metrics'][model_name][reward_combo_name] = {\n",
    "                'train': train_metrics,\n",
    "                'test': test_metrics\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Training metrics for {model_name} with {reward_combo_name}:\")\n",
    "                print(f\"  Val/Opt Ratio: {train_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {train_metrics['#opt']}/{M}\")\n",
    "                print(f\"  Mean percentage error: {train_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {train_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "                \n",
    "                print(f\"Test metrics for {model_name} with {reward_combo_name}:\")\n",
    "                print(f\"  Val/Opt Ratio: {test_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {test_metrics['#opt']}/{n_test_instances}\")\n",
    "                print(f\"  Mean percentage error: {test_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {test_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "    \n",
    "    # Generate summary table\n",
    "    summary = create_summary_table(results)\n",
    "    results['summary'] = summary\n",
    "    \n",
    "    # Generate visualizations\n",
    "    visualize_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_summary_table(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a summary table of all experiments.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from test_reward_functions\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Summary table\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name in results['metrics']:\n",
    "        for reward_combo_name, metrics in results['metrics'][model_name].items():\n",
    "            train_metrics = metrics['train']\n",
    "            test_metrics = metrics['test']\n",
    "            \n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Reward': reward_combo_name,\n",
    "                'Train_ValOptRatio': train_metrics['ValOptRatio'],\n",
    "                'Train_#opt': train_metrics['#opt'],\n",
    "                'Train_MAE': train_metrics['mean_absolute_error'],\n",
    "                'Train_MPE': train_metrics['mean_percentage_error'],\n",
    "                'Train_vs_Greedy': train_metrics['mean_improvement_over_greedy'],\n",
    "                'Test_ValOptRatio': test_metrics['ValOptRatio'],\n",
    "                'Test_#opt': test_metrics['#opt'],\n",
    "                'Test_MAE': test_metrics['mean_absolute_error'],\n",
    "                'Test_MPE': test_metrics['mean_percentage_error'],\n",
    "                'Test_vs_Greedy': test_metrics['mean_improvement_over_greedy']\n",
    "            }\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def visualize_results(results: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Generate visualizations for the experiment results.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from test_reward_functions\n",
    "    \"\"\"\n",
    "    # Compare Val/Opt Ratio across models and reward functions\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    summary_df = results['summary']\n",
    "    \n",
    "    # Create data for grouped bar chart\n",
    "    models = summary_df['Model'].unique()\n",
    "    rewards = summary_df['Reward'].unique()\n",
    "    \n",
    "    width = 0.8 / len(rewards)\n",
    "    x = np.arange(len(models))\n",
    "    \n",
    "    # Training ValOptRatio chart\n",
    "    for i, reward in enumerate(rewards):\n",
    "        values = [summary_df[(summary_df['Model'] == model) & (summary_df['Reward'] == reward)]['Train_ValOptRatio'].values[0] \n",
    "                 for model in models]\n",
    "        ax1.bar(x + i*width - 0.4 + width/2, values, width, label=reward)\n",
    "    \n",
    "    ax1.set_ylabel('Val/Opt Ratio (%)')\n",
    "    ax1.set_title('Training Val/Opt Ratio by Model and Reward')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend(title='Reward')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Test ValOptRatio chart\n",
    "    for i, reward in enumerate(rewards):\n",
    "        values = [summary_df[(summary_df['Model'] == model) & (summary_df['Reward'] == reward)]['Test_ValOptRatio'].values[0] \n",
    "                 for model in models]\n",
    "        ax2.bar(x + i*width - 0.4 + width/2, values, width, label=reward)\n",
    "    \n",
    "    ax2.set_ylabel('Val/Opt Ratio (%)')\n",
    "    ax2.set_title('Test Val/Opt Ratio by Model and Reward')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.legend(title='Reward')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('val_opt_ratio_comparison.png')\n",
    "    \n",
    "    # Create heatmap for reward function performance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    pos_rewards = list(set([r.split('_')[0] for r in rewards]))\n",
    "    neg_rewards = list(set([r.split('_')[1] for r in rewards]))\n",
    "    \n",
    "    heatmap_data = np.zeros((len(models), len(pos_rewards), len(neg_rewards)))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        for j, pos in enumerate(pos_rewards):\n",
    "            for k, neg in enumerate(neg_rewards):\n",
    "                reward = f\"{pos}_{neg}\"\n",
    "                try:\n",
    "                    val = summary_df[(summary_df['Model'] == model) & (summary_df['Reward'] == reward)]['Test_ValOptRatio'].values[0]\n",
    "                    heatmap_data[i, j, k] = val\n",
    "                except:\n",
    "                    heatmap_data[i, j, k] = 0\n",
    "    \n",
    "    # Create subplots for each model\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(16, 6))\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        im = axes[i].imshow(heatmap_data[i], cmap='viridis')\n",
    "        axes[i].set_title(f'{model}')\n",
    "        axes[i].set_xticks(np.arange(len(neg_rewards)))\n",
    "        axes[i].set_yticks(np.arange(len(pos_rewards)))\n",
    "        axes[i].set_xticklabels(neg_rewards)\n",
    "        axes[i].set_yticklabels(pos_rewards)\n",
    "        axes[i].set_xlabel('Negative Reward')\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel('Positive Reward')\n",
    "            \n",
    "        # Loop over data dimensions and create text annotations\n",
    "        for j in range(len(pos_rewards)):\n",
    "            for k in range(len(neg_rewards)):\n",
    "                text = axes[i].text(k, j, f\"{heatmap_data[i, j, k]:.1f}\",\n",
    "                            ha=\"center\", va=\"center\", color=\"w\" if heatmap_data[i, j, k] < 70 else \"black\")\n",
    "    \n",
    "    fig.colorbar(im, ax=axes, shrink=0.8, label='Val/Opt Ratio (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reward_function_heatmap.png')\n",
    "    \n",
    "    # Plot convergence over time\n",
    "    for model_name in results['training']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        for reward_combo_name in results['training'][model_name]:\n",
    "            solution_values = results['training'][model_name][reward_combo_name]['solution_values']\n",
    "            best_sum_over_time = solution_values['best_sum_over_time']\n",
    "            t_values = np.arange(len(best_sum_over_time))\n",
    "            \n",
    "            plt.plot(t_values, best_sum_over_time, label=reward_combo_name)\n",
    "        \n",
    "        plt.xlabel('Training Iterations')\n",
    "        plt.ylabel('Sum of Best Values')\n",
    "        plt.title(f'Convergence for {model_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'convergence_{model_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward_functions(\n",
    "    KPSolver_A2C: KnapsackA2C,\n",
    "    KPSolver_DQN: KnapsackDQN,\n",
    "    KPSolver_PPO: KnapsackPPOSolver,\n",
    "    M: int = 10,\n",
    "    instance_type: str = \"RI\",\n",
    "    N: int = 50,\n",
    "    r_range: int = 500,\n",
    "    seed: int = 42,\n",
    "    t_max: int = None,\n",
    "    use_state_aggregation: bool = False,\n",
    "    n_test_instances: int = 5,\n",
    "    verbose: bool = True,\n",
    "    n_runs: int = 10  # Number of runs to average results over\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test different reward function combinations across multiple RL models for the Knapsack problem.\n",
    "    Runs each experiment n_runs times and averages the results.\n",
    "    \n",
    "    Args:\n",
    "        M (int): Number of problem instances to generate for training\n",
    "        instance_type (str): Type of knapsack instances to generate ('RI', 'FI', 'HI', 'SS')\n",
    "        N (int): Maximum number of items per problem instance\n",
    "        r_range (int): Range parameter for instance generation\n",
    "        seed (int): Base random seed for reproducibility\n",
    "        t_max (int): Maximum training steps (if None, will use default 3*N*10000)\n",
    "        use_state_aggregation (bool): Whether to use state aggregation\n",
    "        n_test_instances (int): Number of test instances to evaluate on\n",
    "        verbose (bool): Whether to print detailed progress information\n",
    "        n_runs (int): Number of runs to average results over\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results and metrics for all experiments, including averaged metrics\n",
    "    \"\"\"\n",
    "    # Define the reward function combinations to test\n",
    "    positive_reward_functions = {\n",
    "        'v_i': v_i_positive_reward,\n",
    "        'vr_i': vr_i_positive_reward,\n",
    "        '_1': _1_positive_reward\n",
    "    }\n",
    "    \n",
    "    negative_reward_functions = {\n",
    "        'w_i': w_i_negative_reward,\n",
    "        'wr_i': wr_i_negative_reward,\n",
    "        '_1': _1_negative_reward\n",
    "    }\n",
    "    \n",
    "    # Create all combinations of reward functions and models\n",
    "    # reward_combinations = list(itertools.product(\n",
    "    #     positive_reward_functions.items(),\n",
    "    #     negative_reward_functions.items()\n",
    "    # ))\n",
    "    reward_combinations = [\n",
    "        (('vr_i', positive_reward_functions['vr_i']), ('wr_i', negative_reward_functions['wr_i'])),\n",
    "        (('vr_i', positive_reward_functions['vr_i']), ('w_i', negative_reward_functions['w_i'])),\n",
    "        (('v_i', positive_reward_functions['v_i']), ('wr_i', negative_reward_functions['wr_i'])),\n",
    "        (('v_i', positive_reward_functions['v_i']), ('w_i', negative_reward_functions['w_i'])),\n",
    "        (('_1', positive_reward_functions['_1']), ('_1', negative_reward_functions['_1']))\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Define models to test\n",
    "    model_constructors = {}\n",
    "    if KPSolver_A2C is not None: model_constructors[\"A2C\"] = KPSolver_A2C\n",
    "    if KPSolver_DQN is not None: model_constructors[\"DQN\"] = KPSolver_DQN\n",
    "    if KPSolver_PPO is not None: model_constructors[\"PPO\"] = KPSolver_PPO\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'training': {},\n",
    "        'test': {},\n",
    "        'metrics': {},\n",
    "        'config': {\n",
    "            'num_instances': M,\n",
    "            'instance_type': instance_type,\n",
    "            'n_items': N,\n",
    "            'r_range': r_range,\n",
    "            'base_seed': seed,\n",
    "            't_max': t_max,\n",
    "            'use_state_aggregation': use_state_aggregation,\n",
    "            'n_runs': n_runs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Total count of experiments\n",
    "    total_experiments = len(reward_combinations) * len(model_constructors)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Running {total_experiments} experiments, each with {n_runs} runs...\")\n",
    "    \n",
    "    experiment_counter = 0\n",
    "    \n",
    "    # Run experiments for each model and reward function combination\n",
    "    for model_name, model in model_constructors.items():\n",
    "        results['training'][model_name] = {}\n",
    "        results['test'][model_name] = {}\n",
    "        results['metrics'][model_name] = {}\n",
    "        \n",
    "        for (pos_name, pos_func), (neg_name, neg_func) in reward_combinations:\n",
    "            experiment_counter += 1\n",
    "            reward_combo_name = f\"(+{pos_name} -{neg_name})\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nExperiment {experiment_counter}/{total_experiments}: Testing {model_name} with {reward_combo_name}\")\n",
    "                print(f\"Positive reward: {pos_name}, Negative reward: {neg_name}\")\n",
    "                print(f\"Running {n_runs} times and averaging results...\")\n",
    "            \n",
    "            # Initialize storage for multiple runs\n",
    "            all_training_values = []\n",
    "            all_test_values = []\n",
    "            all_training_times = []\n",
    "            all_train_metrics = []\n",
    "            all_test_metrics = []\n",
    "            \n",
    "            # Run the experiment n_runs times\n",
    "            for run in range(n_runs):\n",
    "                # Set different seed for each run\n",
    "                run_seed = seed + run * 1000\n",
    "                np.random.seed(run_seed)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\nRun {run+1}/{n_runs} (seed={run_seed}):\")\n",
    "                \n",
    "                # Generate problem instances for this run\n",
    "                gen = KnapsackInstanceGenerator(seed=run_seed)\n",
    "                \n",
    "                if instance_type == \"RI\":\n",
    "                    training_instances = gen.generate_random_instances(M, N, r_range, seed=run_seed)\n",
    "                    test_instances = gen.generate_random_instances(n_test_instances, N, r_range, seed=run_seed+100)\n",
    "                elif instance_type == \"FI\":\n",
    "                    training_instances = gen.generate_fixed_instances(M, N, seed=run_seed)\n",
    "                    test_instances = gen.generate_fixed_instances(n_test_instances, N, seed=run_seed+100)\n",
    "                elif instance_type == \"HI\":\n",
    "                    training_instances = gen.generate_hard_instances(M, N, r_range, seed=run_seed)\n",
    "                    test_instances = gen.generate_hard_instances(n_test_instances, N, r_range, seed=run_seed+100)\n",
    "                elif instance_type == \"SS\":\n",
    "                    training_instances = gen.generate_subset_sum_instances(M, N, r_range, seed=run_seed)\n",
    "                    test_instances = gen.generate_subset_sum_instances(n_test_instances, N, r_range, seed=run_seed+100)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown instance type: {instance_type}\")\n",
    "                \n",
    "                # Solve instances with DP and Greedy for baselines\n",
    "                if verbose: print(\"Computing DP optimal solutions for training instances...\")\n",
    "                dp_sols_items_train, dp_values_train, dp_weight_train = solve_KP_instances_with_DP(training_instances)\n",
    "\n",
    "                if verbose: print(\"Computing Greedy solutions for training instances...\")\n",
    "                greedy_values_train, greedy_sols_items_train, greedy_weights_train = solve_problem_instances_greedy(training_instances)\n",
    "                \n",
    "                if verbose: print(\"Computing DP optimal solutions for test instances...\")\n",
    "                dp_sols_items_test, dp_values_test, dp_weight_test = solve_KP_instances_with_DP(test_instances)\n",
    "                \n",
    "                if verbose: print(\"Computing Greedy solutions for test instances...\")\n",
    "                greedy_values_test, greedy_sols_items_test, greedy_weights_test = solve_problem_instances_greedy(test_instances)\n",
    "                \n",
    "                # Create environment with specific reward functions\n",
    "                env = KnapsackEnv(\n",
    "                    problem_instance=None,\n",
    "                    N=N,\n",
    "                    positive_reward_function=pos_func,\n",
    "                    negative_reward_function=neg_func\n",
    "                )\n",
    "                \n",
    "                # Initialize the model\n",
    "                kp_solver = model\n",
    "                \n",
    "                # Train the model\n",
    "                start_time = time.time()\n",
    "                \n",
    "                solver, solution_values = run_KPSolver(\n",
    "                    env=env,\n",
    "                    KPSolver=kp_solver,\n",
    "                    training_problem_instances=training_instances,\n",
    "                    t_max=t_max,\n",
    "                    use_state_aggregation=use_state_aggregation,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "                \n",
    "                training_time = time.time() - start_time\n",
    "                all_training_times.append(training_time)\n",
    "                \n",
    "                # Store training results for this run\n",
    "                all_training_values.append(solution_values)\n",
    "                \n",
    "                # Evaluate on test instances\n",
    "                test_values = []\n",
    "                for instance in test_instances:\n",
    "                    env.change_problem_instance(instance)\n",
    "                    value, weight, _ = solver.solve(instance)\n",
    "                    test_values.append(value)\n",
    "                \n",
    "                all_test_values.append(test_values)\n",
    "                \n",
    "                # Calculate performance metrics for this run\n",
    "                # For training instances\n",
    "                train_best_values = solution_values['instance_best_values']\n",
    "                train_metrics = evaluate_knapsack_performance(\n",
    "                    train_best_values, \n",
    "                    dp_values_train, \n",
    "                    greedy_values_train\n",
    "                )\n",
    "                all_train_metrics.append(train_metrics)\n",
    "                \n",
    "                # For test instances\n",
    "                test_metrics = evaluate_knapsack_performance(\n",
    "                    test_values,\n",
    "                    dp_values_test,\n",
    "                    greedy_values_test\n",
    "                )\n",
    "                all_test_metrics.append(test_metrics)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Run {run+1} training metrics for {model_name} with {reward_combo_name}:\")\n",
    "                    print(f\"  Val/Opt Ratio: {train_metrics['ValOptRatio']:.2f}%\")\n",
    "                    print(f\"  #opt: {train_metrics['#opt']}/{M}\")\n",
    "                    print(f\"  Mean percentage error: {train_metrics['mean_percentage_error']:.4f}\")\n",
    "                    print(f\"  Mean improvement over greedy: {train_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "                    \n",
    "                    print(f\"Run {run+1} test metrics for {model_name} with {reward_combo_name}:\")\n",
    "                    print(f\"  Val/Opt Ratio: {test_metrics['ValOptRatio']:.2f}%\")\n",
    "                    print(f\"  #opt: {test_metrics['#opt']}/{n_test_instances}\")\n",
    "                    print(f\"  Mean percentage error: {test_metrics['mean_percentage_error']:.4f}\")\n",
    "                    print(f\"  Mean improvement over greedy: {test_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "            \n",
    "            # Calculate averages across all runs\n",
    "            avg_training_time = np.mean(all_training_times)\n",
    "            \n",
    "            # Average training metrics\n",
    "            avg_train_metrics = {\n",
    "                'ValOptRatio': np.mean([m['ValOptRatio'] for m in all_train_metrics]),\n",
    "                '#opt': np.mean([m['#opt'] for m in all_train_metrics]),\n",
    "                'mean_percentage_error': np.mean([m['mean_percentage_error'] for m in all_train_metrics]),\n",
    "                'mean_improvement_over_greedy': np.mean([m['mean_improvement_over_greedy'] for m in all_train_metrics])\n",
    "            }\n",
    "            \n",
    "            # Calculate std dev of metrics for error bars\n",
    "            std_train_metrics = {\n",
    "                'ValOptRatio': np.std([m['ValOptRatio'] for m in all_train_metrics]),\n",
    "                '#opt': np.std([m['#opt'] for m in all_train_metrics]),\n",
    "                'mean_percentage_error': np.std([m['mean_percentage_error'] for m in all_train_metrics]),\n",
    "                'mean_improvement_over_greedy': np.std([m['mean_improvement_over_greedy'] for m in all_train_metrics])\n",
    "            }\n",
    "            \n",
    "            # Average test metrics\n",
    "            avg_test_metrics = {\n",
    "                'ValOptRatio': np.mean([m['ValOptRatio'] for m in all_test_metrics]),\n",
    "                '#opt': np.mean([m['#opt'] for m in all_test_metrics]),\n",
    "                'mean_percentage_error': np.mean([m['mean_percentage_error'] for m in all_test_metrics]),\n",
    "                'mean_improvement_over_greedy': np.mean([m['mean_improvement_over_greedy'] for m in all_test_metrics])\n",
    "            }\n",
    "            \n",
    "            # Calculate std dev of test metrics for error bars\n",
    "            std_test_metrics = {\n",
    "                'ValOptRatio': np.std([m['ValOptRatio'] for m in all_test_metrics]),\n",
    "                '#opt': np.std([m['#opt'] for m in all_test_metrics]),\n",
    "                'mean_percentage_error': np.std([m['mean_percentage_error'] for m in all_test_metrics]),\n",
    "                'mean_improvement_over_greedy': np.std([m['mean_improvement_over_greedy'] for m in all_test_metrics])\n",
    "            }\n",
    "            \n",
    "            # Store averaged results\n",
    "            results['training'][model_name][reward_combo_name] = {\n",
    "                'solution_values': all_training_values,  # Store all runs\n",
    "                'avg_training_time': avg_training_time,\n",
    "                'avg_metrics': avg_train_metrics,\n",
    "                'std_metrics': std_train_metrics\n",
    "            }\n",
    "            \n",
    "            results['test'][model_name][reward_combo_name] = {\n",
    "                'values': all_test_values,  # Store all runs\n",
    "                'avg_metrics': avg_test_metrics,\n",
    "                'std_metrics': std_test_metrics\n",
    "            }\n",
    "            \n",
    "            results['metrics'][model_name][reward_combo_name] = {\n",
    "                'train': {\n",
    "                    'avg': avg_train_metrics,\n",
    "                    'std': std_train_metrics,\n",
    "                    'all_runs': all_train_metrics\n",
    "                },\n",
    "                'test': {\n",
    "                    'avg': avg_test_metrics,\n",
    "                    'std': std_test_metrics,\n",
    "                    'all_runs': all_test_metrics\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nAVERAGED RESULTS ({n_runs} runs) for {model_name} with {reward_combo_name}:\")\n",
    "                print(f\"Training metrics:\")\n",
    "                print(f\"  Val/Opt Ratio: {avg_train_metrics['ValOptRatio']:.2f}% ± {std_train_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {avg_train_metrics['#opt']:.2f} ± {std_train_metrics['#opt']:.2f}\")\n",
    "                print(f\"  Mean percentage error: {avg_train_metrics['mean_percentage_error']:.4f} ± {std_train_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {avg_train_metrics['mean_improvement_over_greedy']:.4f} ± {std_train_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "                \n",
    "                print(f\"Test metrics:\")\n",
    "                print(f\"  Val/Opt Ratio: {avg_test_metrics['ValOptRatio']:.2f}% ± {std_test_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {avg_test_metrics['#opt']:.2f} ± {std_test_metrics['#opt']:.2f}\")\n",
    "                print(f\"  Mean percentage error: {avg_test_metrics['mean_percentage_error']:.4f} ± {std_test_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {avg_test_metrics['mean_improvement_over_greedy']:.4f} ± {std_test_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "    \n",
    "    # Generate summary table with averaged results\n",
    "    summary = create_summary_table_with_std(results, n_runs)\n",
    "    results['summary'] = summary\n",
    "    \n",
    "    # Generate visualizations with error bars\n",
    "    visualize_results_with_error_bars(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_summary_table_with_std(results, n_runs):\n",
    "    \"\"\"\n",
    "    Create a summary table of the experiment results across all models and reward functions,\n",
    "    including standard deviations.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing experiment results\n",
    "        n_runs: Number of runs performed for each experiment\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Summary table with metrics and standard deviations\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name in results['metrics']:\n",
    "        for reward_combo in results['metrics'][model_name]:\n",
    "            # Training metrics\n",
    "            train_metrics = results['metrics'][model_name][reward_combo]['train']['avg']\n",
    "            train_std = results['metrics'][model_name][reward_combo]['train']['std']\n",
    "            \n",
    "            # Test metrics\n",
    "            test_metrics = results['metrics'][model_name][reward_combo]['test']['avg']\n",
    "            test_std = results['metrics'][model_name][reward_combo]['test']['std']\n",
    "            \n",
    "            # Average training time\n",
    "            avg_training_time = results['training'][model_name][reward_combo]['avg_training_time']\n",
    "            \n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Reward Function': reward_combo,\n",
    "                'Train Val/Opt (%)': f\"{train_metrics['ValOptRatio']:.2f} ± {train_std['ValOptRatio']:.2f}\",\n",
    "                'Train #opt': f\"{train_metrics['#opt']:.2f} ± {train_std['#opt']:.2f}\",\n",
    "                'Train MPE': f\"{train_metrics['mean_percentage_error']:.4f} ± {train_std['mean_percentage_error']:.4f}\",\n",
    "                'Train Imp/Greedy': f\"{train_metrics['mean_improvement_over_greedy']:.4f} ± {train_std['mean_improvement_over_greedy']:.4f}\",\n",
    "                'Test Val/Opt (%)': f\"{test_metrics['ValOptRatio']:.2f} ± {test_std['ValOptRatio']:.2f}\",\n",
    "                'Test #opt': f\"{test_metrics['#opt']:.2f} ± {test_std['#opt']:.2f}\",\n",
    "                'Test MPE': f\"{test_metrics['mean_percentage_error']:.4f} ± {test_std['mean_percentage_error']:.4f}\",\n",
    "                'Test Imp/Greedy': f\"{test_metrics['mean_improvement_over_greedy']:.4f} ± {test_std['mean_improvement_over_greedy']:.4f}\",\n",
    "                'Training Time (s)': f\"{avg_training_time:.2f}\"\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Add note about number of runs\n",
    "    summary_df.attrs['note'] = f\"Results averaged over {n_runs} runs. ± values indicate standard deviation.\"\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def visualize_results_with_error_bars(results, save_fig:bool=False):\n",
    "    \"\"\"\n",
    "    Generate visualizations of the experiment results with error bars.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing experiment results\n",
    "    \"\"\"\n",
    "    # Create matplotlib figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    models = list(results['metrics'].keys())\n",
    "    reward_combos = []\n",
    "    for model in models:\n",
    "        reward_combos.extend(list(results['metrics'][model].keys()))\n",
    "    reward_combos = list(set(reward_combos))  # Remove duplicates\n",
    "    \n",
    "    # Prepare data for Val/Opt Ratio (test set)\n",
    "    x = np.arange(len(reward_combos))\n",
    "    width = 0.8 / len(models)  # Width of bars\n",
    "    \n",
    "    # Plot Val/Opt Ratio for test set with error bars\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i, model in enumerate(models):\n",
    "        values = []\n",
    "        errors = []\n",
    "        for reward in reward_combos:\n",
    "            if reward in results['metrics'][model]:\n",
    "                values.append(results['metrics'][model][reward]['test']['avg']['ValOptRatio'])\n",
    "                errors.append(results['metrics'][model][reward]['test']['std']['ValOptRatio'])\n",
    "            else:\n",
    "                values.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width - width*(len(models)-1)/2, values, width, label=model, yerr=errors, capsize=3)\n",
    "    \n",
    "    plt.ylabel('Val/Opt Ratio (%)')\n",
    "    plt.title('Test Set Val/Opt Ratio by Model and Reward Function')\n",
    "    plt.xticks(x, reward_combos, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot #opt for test set with error bars\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i, model in enumerate(models):\n",
    "        values = []\n",
    "        errors = []\n",
    "        for reward in reward_combos:\n",
    "            if reward in results['metrics'][model]:\n",
    "                values.append(results['metrics'][model][reward]['test']['avg']['#opt'])\n",
    "                errors.append(results['metrics'][model][reward]['test']['std']['#opt'])\n",
    "            else:\n",
    "                values.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width - width*(len(models)-1)/2, values, width, label=model, yerr=errors, capsize=3)\n",
    "    \n",
    "    plt.ylabel('Number of Optimal Solutions')\n",
    "    plt.title('Test Set #opt by Model and Reward Function')\n",
    "    plt.xticks(x, reward_combos, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot Mean Percentage Error for test set with error bars\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i, model in enumerate(models):\n",
    "        values = []\n",
    "        errors = []\n",
    "        for reward in reward_combos:\n",
    "            if reward in results['metrics'][model]:\n",
    "                values.append(results['metrics'][model][reward]['test']['avg']['mean_percentage_error'])\n",
    "                errors.append(results['metrics'][model][reward]['test']['std']['mean_percentage_error'])\n",
    "            else:\n",
    "                values.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width - width*(len(models)-1)/2, values, width, label=model, yerr=errors, capsize=3)\n",
    "    \n",
    "    plt.ylabel('Mean Percentage Error')\n",
    "    plt.title('Test Set Mean Percentage Error by Model and Reward Function')\n",
    "    plt.xticks(x, reward_combos, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot Mean Improvement over Greedy for test set with error bars\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for i, model in enumerate(models):\n",
    "        values = []\n",
    "        errors = []\n",
    "        for reward in reward_combos:\n",
    "            if reward in results['metrics'][model]:\n",
    "                values.append(results['metrics'][model][reward]['test']['avg']['mean_improvement_over_greedy'])\n",
    "                errors.append(results['metrics'][model][reward]['test']['std']['mean_improvement_over_greedy'])\n",
    "            else:\n",
    "                values.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width - width*(len(models)-1)/2, values, width, label=model, yerr=errors, capsize=3)\n",
    "    \n",
    "    plt.ylabel('Mean Improvement over Greedy')\n",
    "    plt.title('Test Set Mean Improvement over Greedy by Model and Reward Function')\n",
    "    plt.xticks(x, reward_combos, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_fig: plt.savefig('experiment_results_with_error_bars.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create additional visualizations for training performance\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Plot Val/Opt Ratio for training set with error bars\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i, model in enumerate(models):\n",
    "        values = []\n",
    "        errors = []\n",
    "        for reward in reward_combos:\n",
    "            if reward in results['metrics'][model]:\n",
    "                values.append(results['metrics'][model][reward]['train']['avg']['ValOptRatio'])\n",
    "                errors.append(results['metrics'][model][reward]['train']['std']['ValOptRatio'])\n",
    "            else:\n",
    "                values.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width - width*(len(models)-1)/2, values, width, label=model, yerr=errors, capsize=3)\n",
    "    \n",
    "    plt.ylabel('Val/Opt Ratio (%)')\n",
    "    plt.title('Training Set Val/Opt Ratio by Model and Reward Function')\n",
    "    plt.xticks(x, reward_combos, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot Training Times\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i, model in enumerate(models):\n",
    "        values = []\n",
    "        for reward in reward_combos:\n",
    "            if reward in results['training'][model]:\n",
    "                values.append(results['training'][model][reward]['avg_training_time'])\n",
    "            else:\n",
    "                values.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width - width*(len(models)-1)/2, values, width, label=model)\n",
    "    \n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.title('Average Training Time by Model and Reward Function')\n",
    "    plt.xticks(x, reward_combos, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_fig: plt.savefig('training_performance_with_error_bars.png', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "This jupyter notebook will be used to generate relevant plots relating to our experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 15 experiments, each with 10 runs...\n",
      "\n",
      "Experiment 1/15: Testing A2C with (+vr_i -wr_i)\n",
      "Positive reward: vr_i, Negative reward: wr_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Run 1 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 73.28%\n",
      "  #opt: 254/1000\n",
      "  Mean percentage error: 0.2042\n",
      "  Mean improvement over greedy: -0.1875\n",
      "Run 1 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 51.99%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3137\n",
      "  Mean improvement over greedy: -0.2772\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.3316627650814591\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.6774687831513554\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.6774687831513554\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.6274958264779089\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.5113185108123487\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.5906158196743816\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.628437452099288\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6461829649163314\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5951444193824171\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.4685685295305279\n",
      "Run 2 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 68.89%\n",
      "  #opt: 243/1000\n",
      "  Mean percentage error: 0.2392\n",
      "  Mean improvement over greedy: -0.2227\n",
      "Run 2 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 53.99%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2723\n",
      "  Mean improvement over greedy: -0.2723\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.21693583403479233\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.19841203126120963\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.19841203126120963\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.20351719802103274\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.20519851297610256\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.20419231921640735\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.22037468564977702\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.06449103630489765\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.056176009833220866\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.22177808494337917\n",
      "Run 3 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 71.72%\n",
      "  #opt: 231/1000\n",
      "  Mean percentage error: 0.2198\n",
      "  Mean improvement over greedy: -0.2029\n",
      "Run 3 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 62.86%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4129\n",
      "  Mean improvement over greedy: -0.3930\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -1.4901907971147383\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -1.4854437924078487\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.9882378359692334\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -1.354203598426962\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -1.317645966768774\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.7454469507101087\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -1.3830729051392148\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -1.3542035984269618\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -1.3609907645129085\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -1.441909019816151\n",
      "Run 4 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 70.60%\n",
      "  #opt: 223/1000\n",
      "  Mean percentage error: 0.2316\n",
      "  Mean improvement over greedy: -0.2170\n",
      "Run 4 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 53.16%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4599\n",
      "  Mean improvement over greedy: -0.4405\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -1.720717213963311\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -1.635313401521112\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -1.7170731707317075\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.7788530465949821\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -1.6331497508523474\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -1.8092265001734307\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -1.4198756720430108\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -1.6575999400547\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -1.4286162314388122\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -1.6465774980330445\n",
      "Run 5 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 69.70%\n",
      "  #opt: 222/1000\n",
      "  Mean percentage error: 0.2401\n",
      "  Mean improvement over greedy: -0.2263\n",
      "Run 5 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 52.84%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3882\n",
      "  Mean improvement over greedy: -0.3505\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.5214819612964835\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5403819965017491\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.4917135036907012\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5430359299516907\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.5278931012880819\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.49853907867494823\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.5555272493961353\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.5507168825689894\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5318942481884057\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5403819965017491\n",
      "Run 6 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 70.01%\n",
      "  #opt: 220/1000\n",
      "  Mean percentage error: 0.2380\n",
      "  Mean improvement over greedy: -0.2220\n",
      "Run 6 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 52.59%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4757\n",
      "  Mean improvement over greedy: -0.4665\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Run 7 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 70.87%\n",
      "  #opt: 233/1000\n",
      "  Mean percentage error: 0.2309\n",
      "  Mean improvement over greedy: -0.2189\n",
      "Run 7 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 61.87%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2974\n",
      "  Mean improvement over greedy: -0.2844\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.2919037178753565\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.3264924161876502\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.32121188654033034\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.30915118177953\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.2966051319517706\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.2768719628999925\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.33292726956485075\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.3086405670222634\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.29179801735794475\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.20580132459487857\n",
      "Run 8 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.27%\n",
      "  #opt: 228/1000\n",
      "  Mean percentage error: 0.2161\n",
      "  Mean improvement over greedy: -0.2005\n",
      "Run 8 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 65.82%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3546\n",
      "  Mean improvement over greedy: -0.3351\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.153133474374103\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.153133474374103\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.153133474374103\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.153133474374103\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.153133474374103\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.153133474374103\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.17698202532761356\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.14996059640865142\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.17398514475720359\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.19066698007874477\n",
      "Run 9 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 73.50%\n",
      "  #opt: 264/1000\n",
      "  Mean percentage error: 0.2002\n",
      "  Mean improvement over greedy: -0.1846\n",
      "Run 9 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 47.94%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4987\n",
      "  Mean improvement over greedy: -0.4879\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.43056964736652253\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.3825884721339266\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.38477343363707\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.40686049181028033\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.35429198903932946\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.4557590413059163\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.05495408631772269\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.38973218646816327\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.42326802507836986\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.40487522524756575\n",
      "Run 10 training metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 74.16%\n",
      "  #opt: 228/1000\n",
      "  Mean percentage error: 0.2006\n",
      "  Mean improvement over greedy: -0.1879\n",
      "Run 10 test metrics for A2C with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 58.88%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3850\n",
      "  Mean improvement over greedy: -0.3586\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for A2C with (+vr_i -wr_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 71.50% ± 1.68%\n",
      "  #opt: 234.60 ± 13.86\n",
      "  Mean percentage error: 0.2221 ± 0.0153\n",
      "  Mean improvement over greedy: -0.2070 ± 0.0155\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 56.19% ± 5.48%\n",
      "  #opt: 0.60 ± 0.80\n",
      "  Mean percentage error: 0.3858 ± 0.0734\n",
      "  Mean improvement over greedy: -0.3666 ± 0.0747\n",
      "\n",
      "Experiment 2/15: Testing A2C with (+vr_i -w_i)\n",
      "Positive reward: vr_i, Negative reward: w_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -13.199019652464369\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -8.79845082425049\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -8.798621472714654\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Run 1 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 73.41%\n",
      "  #opt: 258/1000\n",
      "  Mean percentage error: 0.1996\n",
      "  Mean improvement over greedy: -0.1829\n",
      "Run 1 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 56.19%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3670\n",
      "  Mean improvement over greedy: -0.3304\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.44540387082793\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.43436907997694\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -43.942867479668934\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -46.52755127254756\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -42.49800388954654\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -44.44540387082793\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.424864979349785\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -14.48072692914242\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -45.31739196439901\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -37.321684272300466\n",
      "Run 2 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.30%\n",
      "  #opt: 287/1000\n",
      "  Mean percentage error: 0.1824\n",
      "  Mean improvement over greedy: -0.1658\n",
      "Run 2 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 60.65%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2328\n",
      "  Mean improvement over greedy: -0.2328\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -41.83023948056939\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -41.829711274303115\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -41.85902624373778\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.0086433014354067\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -41.888315790264805\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -30.49500609384455\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -41.88654613277204\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -41.88503265612785\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -41.93847866661247\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -39.52663575593331\n",
      "Run 3 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.18%\n",
      "  #opt: 284/1000\n",
      "  Mean percentage error: 0.1794\n",
      "  Mean improvement over greedy: -0.1624\n",
      "Run 3 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 52.05%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5470\n",
      "  Mean improvement over greedy: -0.5271\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -51.57979552539886\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -51.57990645242205\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -53.44178574797208\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -51.52915062045357\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -51.6050816828657\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -51.65322473551414\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -49.80395066613903\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -45.860150375939845\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -31.810970282849983\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -51.605694113332106\n",
      "Run 4 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.13%\n",
      "  #opt: 293/1000\n",
      "  Mean percentage error: 0.1838\n",
      "  Mean improvement over greedy: -0.1692\n",
      "Run 4 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 53.89%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3522\n",
      "  Mean improvement over greedy: -0.3329\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -42.995400238948626\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -47.678609831029185\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -50.2485848850424\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -50.241236122038636\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -49.34288786482334\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -50.224613165486495\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -50.29201088381852\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -56.11626253391958\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -50.253986362444266\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -51.61767868437697\n",
      "Run 5 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 73.90%\n",
      "  #opt: 296/1000\n",
      "  Mean percentage error: 0.1893\n",
      "  Mean improvement over greedy: -0.1755\n",
      "Run 5 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 48.90%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5253\n",
      "  Mean improvement over greedy: -0.4875\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.18413184915018\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.308724387806095\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -44.31050179694026\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.370434645366714\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -44.3087243878061\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -44.71019753128094\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.308724387806095\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -45.182313233264324\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.93221126076455\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Run 6 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 74.59%\n",
      "  #opt: 271/1000\n",
      "  Mean percentage error: 0.1866\n",
      "  Mean improvement over greedy: -0.1706\n",
      "Run 6 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 53.80%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4407\n",
      "  Mean improvement over greedy: -0.4316\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -26.74865876122496\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -32.991776677667765\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -26.74865876122496\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -26.74865876122496\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -32.99177667766777\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Run 7 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 74.13%\n",
      "  #opt: 285/1000\n",
      "  Mean percentage error: 0.1920\n",
      "  Mean improvement over greedy: -0.1801\n",
      "Run 7 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 63.69%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2420\n",
      "  Mean improvement over greedy: -0.2290\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -49.98268376307135\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -49.61881027268862\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -49.34776722052458\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -50.07347262372179\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -49.394211435613606\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -49.36062633134061\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -32.66674829931973\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -49.43873236398153\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -49.57288018706793\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -49.34842023442659\n",
      "Run 8 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 73.68%\n",
      "  #opt: 267/1000\n",
      "  Mean percentage error: 0.1971\n",
      "  Mean improvement over greedy: -0.1815\n",
      "Run 8 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 71.28%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.2840\n",
      "  Mean improvement over greedy: -0.2644\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -38.17799452925967\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -38.90496088398338\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -36.632816015190585\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -37.54026103196735\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -36.54466670054905\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -36.177197645497095\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -36.543653760455236\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -36.543653760455236\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -36.54466670054905\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -36.54466670054905\n",
      "Run 9 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.06%\n",
      "  #opt: 298/1000\n",
      "  Mean percentage error: 0.1829\n",
      "  Mean improvement over greedy: -0.1673\n",
      "Run 9 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 62.19%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3221\n",
      "  Mean improvement over greedy: -0.3113\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.32251779155189\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.32749035083274\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -42.12994156381253\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.402369383490075\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -26.973052704464\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -30.73838450573387\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.29069044944989\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -44.330712649753345\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.28436832053612\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.28311079545455\n",
      "Run 10 training metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 74.55%\n",
      "  #opt: 272/1000\n",
      "  Mean percentage error: 0.1929\n",
      "  Mean improvement over greedy: -0.1801\n",
      "Run 10 test metrics for A2C with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 41.89%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.5298\n",
      "  Mean improvement over greedy: -0.5034\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for A2C with (+vr_i -w_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 74.49% ± 0.65%\n",
      "  #opt: 281.10 ± 12.75\n",
      "  Mean percentage error: 0.1886 ± 0.0064\n",
      "  Mean improvement over greedy: -0.1735 ± 0.0070\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 56.45% ± 7.90%\n",
      "  #opt: 0.70 ± 0.78\n",
      "  Mean percentage error: 0.3843 ± 0.1137\n",
      "  Mean improvement over greedy: -0.3650 ± 0.1077\n",
      "\n",
      "Experiment 3/15: Testing A2C with (+v_i -wr_i)\n",
      "Positive reward: v_i, Negative reward: wr_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Run 1 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 54.12%\n",
      "  #opt: 145/1000\n",
      "  Mean percentage error: 0.3897\n",
      "  Mean improvement over greedy: -0.3730\n",
      "Run 1 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 52.93%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3124\n",
      "  Mean improvement over greedy: -0.2758\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Run 2 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 54.30%\n",
      "  #opt: 159/1000\n",
      "  Mean percentage error: 0.3809\n",
      "  Mean improvement over greedy: -0.3644\n",
      "Run 2 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 48.40%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3112\n",
      "  Mean improvement over greedy: -0.3112\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Run 3 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 55.65%\n",
      "  #opt: 193/1000\n",
      "  Mean percentage error: 0.3583\n",
      "  Mean improvement over greedy: -0.3414\n",
      "Run 3 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 66.49%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3861\n",
      "  Mean improvement over greedy: -0.3662\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Run 4 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 53.65%\n",
      "  #opt: 154/1000\n",
      "  Mean percentage error: 0.3928\n",
      "  Mean improvement over greedy: -0.3782\n",
      "Run 4 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 49.59%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3799\n",
      "  Mean improvement over greedy: -0.3605\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.6860739575137689\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 16.290322580645164\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 1.4917387883556266\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 15.53763440860215\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 1.4413847364280095\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 12.631048387096772\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5476003147128248\n",
      "Run 5 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 60.05%\n",
      "  #opt: 197/1000\n",
      "  Mean percentage error: 0.3166\n",
      "  Mean improvement over greedy: -0.3028\n",
      "Run 5 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 46.77%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5054\n",
      "  Mean improvement over greedy: -0.4677\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Run 6 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 59.43%\n",
      "  #opt: 194/1000\n",
      "  Mean percentage error: 0.3235\n",
      "  Mean improvement over greedy: -0.3075\n",
      "Run 6 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 42.94%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5585\n",
      "  Mean improvement over greedy: -0.5494\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Run 7 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 63.23%\n",
      "  #opt: 209/1000\n",
      "  Mean percentage error: 0.2918\n",
      "  Mean improvement over greedy: -0.2799\n",
      "Run 7 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 49.61%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3968\n",
      "  Mean improvement over greedy: -0.3838\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 10.913392857142858\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 7.068993506493505\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 7.068993506493505\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 11.65503246753247\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 11.33400974025974\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 7.3441558441558445\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 6.839691558441558\n",
      "Run 8 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 66.52%\n",
      "  #opt: 200/1000\n",
      "  Mean percentage error: 0.2652\n",
      "  Mean improvement over greedy: -0.2495\n",
      "Run 8 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 72.19%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.2766\n",
      "  Mean improvement over greedy: -0.2571\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Run 9 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 67.72%\n",
      "  #opt: 228/1000\n",
      "  Mean percentage error: 0.2463\n",
      "  Mean improvement over greedy: -0.2307\n",
      "Run 9 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 49.86%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4576\n",
      "  Mean improvement over greedy: -0.4468\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 4.991287878787878\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 5.159469696969697\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 16.349783549783552\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 10.709469696969705\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Run 10 training metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 64.16%\n",
      "  #opt: 188/1000\n",
      "  Mean percentage error: 0.2844\n",
      "  Mean improvement over greedy: -0.2717\n",
      "Run 10 test metrics for A2C with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 48.05%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4325\n",
      "  Mean improvement over greedy: -0.4061\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for A2C with (+v_i -wr_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 59.88% ± 5.06%\n",
      "  #opt: 186.70 ± 24.81\n",
      "  Mean percentage error: 0.3249 ± 0.0506\n",
      "  Mean improvement over greedy: -0.3099 ± 0.0499\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 52.68% ± 8.76%\n",
      "  #opt: 0.80 ± 0.75\n",
      "  Mean percentage error: 0.4017 ± 0.0848\n",
      "  Mean improvement over greedy: -0.3825 ± 0.0853\n",
      "\n",
      "Experiment 4/15: Testing A2C with (+v_i -w_i)\n",
      "Positive reward: v_i, Negative reward: w_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 15.2\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 5.4\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 15.2\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 19.6\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 19.6\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Run 1 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 74.14%\n",
      "  #opt: 287/1000\n",
      "  Mean percentage error: 0.1916\n",
      "  Mean improvement over greedy: -0.1749\n",
      "Run 1 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 72.93%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2667\n",
      "  Mean improvement over greedy: -0.2302\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -39.63157894736842\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -36.31578947368421\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -39.63157894736842\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -36.31578947368421\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -37.55263157894737\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -36.30434782608695\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -40.76315789473684\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -36.55263157894737\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -35.26315789473684\n",
      "Run 2 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 74.91%\n",
      "  #opt: 262/1000\n",
      "  Mean percentage error: 0.1887\n",
      "  Mean improvement over greedy: -0.1721\n",
      "Run 2 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 49.57%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3143\n",
      "  Mean improvement over greedy: -0.3143\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -16.842105263157894\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -29.583333333333332\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -33.138888888888886\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -32.44444444444444\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 27.714285714285715\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -33.72222222222222\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -33.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -30.5\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -29.27777777777778\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -34.0\n",
      "Run 3 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.49%\n",
      "  #opt: 265/1000\n",
      "  Mean percentage error: 0.1822\n",
      "  Mean improvement over greedy: -0.1653\n",
      "Run 3 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 67.86%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2714\n",
      "  Mean improvement over greedy: -0.2516\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -40.638888888888886\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -45.09756097560975\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -45.09756097560975\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.55555555555556\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -45.24390243902439\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -44.41463414634146\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -45.24390243902439\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -36.78260869565217\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -45.09756097560975\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -45.55555555555556\n",
      "Run 4 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.40%\n",
      "  #opt: 270/1000\n",
      "  Mean percentage error: 0.1824\n",
      "  Mean improvement over greedy: -0.1678\n",
      "Run 4 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 53.12%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4754\n",
      "  Mean improvement over greedy: -0.4561\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -46.073170731707314\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -47.90243902439025\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -47.146341463414636\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 16.333333333333332\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -47.0\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -46.75609756097561\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -37.72\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -49.146341463414636\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -48.25\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -47.48780487804878\n",
      "Run 5 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 74.29%\n",
      "  #opt: 275/1000\n",
      "  Mean percentage error: 0.1888\n",
      "  Mean improvement over greedy: -0.1750\n",
      "Run 5 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 53.92%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4538\n",
      "  Mean improvement over greedy: -0.4161\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -36.0\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -39.25\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -33.9375\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -39.25\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -39.25\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -38.75\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -33.25\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -37.25\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -36.0\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -39.25\n",
      "Run 6 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.28%\n",
      "  #opt: 269/1000\n",
      "  Mean percentage error: 0.1794\n",
      "  Mean improvement over greedy: -0.1634\n",
      "Run 6 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 56.63%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4116\n",
      "  Mean improvement over greedy: -0.4024\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -13.0\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -17.25\n",
      "Run 7 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 73.30%\n",
      "  #opt: 274/1000\n",
      "  Mean percentage error: 0.1987\n",
      "  Mean improvement over greedy: -0.1868\n",
      "Run 7 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 50.23%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3820\n",
      "  Mean improvement over greedy: -0.3689\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -41.54545454545455\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -39.95454545454545\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -40.95454545454545\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -41.77272727272727\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -42.86363636363637\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -41.09090909090909\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -38.68181818181818\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -40.72727272727273\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -40.81818181818182\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -29.6\n",
      "Run 8 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 73.74%\n",
      "  #opt: 249/1000\n",
      "  Mean percentage error: 0.1977\n",
      "  Mean improvement over greedy: -0.1821\n",
      "Run 8 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 68.61%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3309\n",
      "  Mean improvement over greedy: -0.3114\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -21.727272727272727\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -23.90909090909091\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -24.727272727272727\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -24.272727272727273\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -27.0\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -29.454545454545453\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -19.727272727272727\n",
      "Run 9 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.05%\n",
      "  #opt: 299/1000\n",
      "  Mean percentage error: 0.1829\n",
      "  Mean improvement over greedy: -0.1673\n",
      "Run 9 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 53.49%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4323\n",
      "  Mean improvement over greedy: -0.4215\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -25.166666666666668\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -6.222222222222222\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -32.625\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -38.75\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -37.875\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -28.916666666666668\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -35.666666666666664\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -26.833333333333332\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -24.791666666666668\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -33.541666666666664\n",
      "Run 10 training metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 74.61%\n",
      "  #opt: 256/1000\n",
      "  Mean percentage error: 0.1901\n",
      "  Mean improvement over greedy: -0.1773\n",
      "Run 10 test metrics for A2C with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 46.92%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4332\n",
      "  Mean improvement over greedy: -0.4068\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for A2C with (+v_i -w_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 74.62% ± 0.70%\n",
      "  #opt: 270.60 ± 13.76\n",
      "  Mean percentage error: 0.1882 ± 0.0063\n",
      "  Mean improvement over greedy: -0.1732 ± 0.0071\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 57.33% ± 8.63%\n",
      "  #opt: 0.70 ± 0.64\n",
      "  Mean percentage error: 0.3772 ± 0.0724\n",
      "  Mean improvement over greedy: -0.3579 ± 0.0730\n",
      "\n",
      "Experiment 5/15: Testing A2C with (+_1 -_1)\n",
      "Positive reward: _1, Negative reward: _1\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Run 1 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 69.34%\n",
      "  #opt: 235/1000\n",
      "  Mean percentage error: 0.2390\n",
      "  Mean improvement over greedy: -0.2223\n",
      "Run 1 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 56.35%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3301\n",
      "  Mean improvement over greedy: -0.2935\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.4444444444444444\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5555555555555556\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.14285714285714285\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.7894736842105263\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7368421052631579\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7894736842105263\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.4117647058823529\n",
      "Run 2 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 74.73%\n",
      "  #opt: 276/1000\n",
      "  Mean percentage error: 0.1883\n",
      "  Mean improvement over greedy: -0.1717\n",
      "Run 2 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 56.60%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2645\n",
      "  Mean improvement over greedy: -0.2645\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.29411764705882354\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5555555555555556\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.44\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.6111111111111112\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.3333333333333333\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Run 3 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.51%\n",
      "  #opt: 265/1000\n",
      "  Mean percentage error: 0.1809\n",
      "  Mean improvement over greedy: -0.1640\n",
      "Run 3 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 64.41%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3932\n",
      "  Mean improvement over greedy: -0.3733\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.9024390243902439\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5384615384615384\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.7560975609756098\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.9024390243902439\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.8181818181818182\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7560975609756098\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.8125\n",
      "Run 4 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.19%\n",
      "  #opt: 260/1000\n",
      "  Mean percentage error: 0.1877\n",
      "  Mean improvement over greedy: -0.1732\n",
      "Run 4 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 60.09%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3476\n",
      "  Mean improvement over greedy: -0.3282\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.45454545454545453\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7391304347826086\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.5714285714285714\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.1111111111111111\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7837837837837838\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.7419354838709677\n",
      "Run 5 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 73.75%\n",
      "  #opt: 267/1000\n",
      "  Mean percentage error: 0.1956\n",
      "  Mean improvement over greedy: -0.1819\n",
      "Run 5 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 46.54%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5215\n",
      "  Mean improvement over greedy: -0.4838\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Run 6 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 74.02%\n",
      "  #opt: 250/1000\n",
      "  Mean percentage error: 0.1972\n",
      "  Mean improvement over greedy: -0.1812\n",
      "Run 6 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 46.72%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4999\n",
      "  Mean improvement over greedy: -0.4907\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Run 7 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 73.21%\n",
      "  #opt: 253/1000\n",
      "  Mean percentage error: 0.2043\n",
      "  Mean improvement over greedy: -0.1924\n",
      "Run 7 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 50.60%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3683\n",
      "  Mean improvement over greedy: -0.3553\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5714285714285714\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Run 8 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 73.72%\n",
      "  #opt: 232/1000\n",
      "  Mean percentage error: 0.2023\n",
      "  Mean improvement over greedy: -0.1866\n",
      "Run 8 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 65.03%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3638\n",
      "  Mean improvement over greedy: -0.3443\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.09090909090909091\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.1111111111111111\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.45454545454545453\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Run 9 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 74.85%\n",
      "  #opt: 263/1000\n",
      "  Mean percentage error: 0.1880\n",
      "  Mean improvement over greedy: -0.1724\n",
      "Run 9 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 51.49%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4413\n",
      "  Mean improvement over greedy: -0.4304\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.6190476190476191\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.8333333333333334\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.8333333333333334\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Run 10 training metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 73.55%\n",
      "  #opt: 238/1000\n",
      "  Mean percentage error: 0.2039\n",
      "  Mean improvement over greedy: -0.1912\n",
      "Run 10 test metrics for A2C with (+_1 -_1):\n",
      "  Val/Opt Ratio: 42.26%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4507\n",
      "  Mean improvement over greedy: -0.4243\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for A2C with (+_1 -_1):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 73.79% ± 1.65%\n",
      "  #opt: 253.90 ± 14.17\n",
      "  Mean percentage error: 0.1987 ± 0.0154\n",
      "  Mean improvement over greedy: -0.1837 ± 0.0155\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 54.01% ± 7.37%\n",
      "  #opt: 0.70 ± 0.78\n",
      "  Mean percentage error: 0.3981 ± 0.0757\n",
      "  Mean improvement over greedy: -0.3788 ± 0.0728\n",
      "\n",
      "Experiment 6/15: Testing DQN with (+vr_i -wr_i)\n",
      "Positive reward: vr_i, Negative reward: wr_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.04680122243024061\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.004517794822580917\n",
      "Run 1 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 75.23%\n",
      "  #opt: 259/1000\n",
      "  Mean percentage error: 0.1847\n",
      "  Mean improvement over greedy: -0.1680\n",
      "Run 1 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 69.06%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.1849\n",
      "  Mean improvement over greedy: -0.1483\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6353799023650097\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.610695976213789\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.635025482982075\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.7141809188464117\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.5791539410344669\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.6324614352783364\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.6102782847757338\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.580262131912858\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5936327695037852\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5951444193824171\n",
      "Run 2 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 73.12%\n",
      "  #opt: 244/1000\n",
      "  Mean percentage error: 0.2070\n",
      "  Mean improvement over greedy: -0.1904\n",
      "Run 2 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 47.34%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3248\n",
      "  Mean improvement over greedy: -0.3248\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.19408512875661837\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.21862325121509224\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.20224592254864954\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.1902991084255584\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.22144800585462565\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.1652899063367464\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.18800510752772745\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.16528990633674642\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.16590432735305527\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.19057520015011825\n",
      "Run 3 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.57%\n",
      "  #opt: 220/1000\n",
      "  Mean percentage error: 0.2141\n",
      "  Mean improvement over greedy: -0.1972\n",
      "Run 3 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 61.68%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4662\n",
      "  Mean improvement over greedy: -0.4463\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -1.2948658410732716\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -1.3542035984269616\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -1.485443792407849\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.9872291021671827\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -1.3542035984269618\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -1.3542035984269618\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -1.1628590707538076\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -1.354203598426962\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -1.4901907971147381\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -1.3284676535087718\n",
      "Run 4 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.39%\n",
      "  #opt: 228/1000\n",
      "  Mean percentage error: 0.2144\n",
      "  Mean improvement over greedy: -0.1998\n",
      "Run 4 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 70.28%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2253\n",
      "  Mean improvement over greedy: -0.2059\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -1.655228822449515\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -1.6446586240055947\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -1.1647765456989247\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -1.7180260512282544\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -1.2880451015531664\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -1.7180260512282544\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -1.6626777662157473\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -1.5932118192149667\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -1.6007518139697527\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -1.5704301075268816\n",
      "Run 5 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.58%\n",
      "  #opt: 233/1000\n",
      "  Mean percentage error: 0.2095\n",
      "  Mean improvement over greedy: -0.1958\n",
      "Run 5 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 51.22%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4411\n",
      "  Mean improvement over greedy: -0.4034\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.5555272493961353\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5555272493961352\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.5507168825689894\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5734740802675585\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.5022281236275802\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.5625654052960223\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.5125069058998869\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.5125069058998868\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5214819612964835\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5493165059195754\n",
      "Run 6 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 73.18%\n",
      "  #opt: 225/1000\n",
      "  Mean percentage error: 0.2067\n",
      "  Mean improvement over greedy: -0.1907\n",
      "Run 6 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 46.89%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4710\n",
      "  Mean improvement over greedy: -0.4618\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.20132940260318166\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Run 7 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 71.49%\n",
      "  #opt: 236/1000\n",
      "  Mean percentage error: 0.2198\n",
      "  Mean improvement over greedy: -0.2079\n",
      "Run 7 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 36.52%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.5099\n",
      "  Mean improvement over greedy: -0.4969\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.3135991907156433\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.2919037178753565\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.2886077612863327\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.3504273958799821\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.32500053944958124\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.3327494789945866\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.32326397189794914\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.005843267315487837\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.3114029768583746\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.2919037178753565\n",
      "Run 8 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.35%\n",
      "  #opt: 235/1000\n",
      "  Mean percentage error: 0.2128\n",
      "  Mean improvement over greedy: -0.1971\n",
      "Run 8 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 53.31%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4432\n",
      "  Mean improvement over greedy: -0.4236\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.14696371583824147\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.17046732679085622\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.1500504635214843\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.15845973135674832\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.15845973135674832\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.15845973135674832\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.18536948417787516\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.1704673267908562\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.12534478471978472\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.17698202532761356\n",
      "Run 9 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 73.66%\n",
      "  #opt: 240/1000\n",
      "  Mean percentage error: 0.1991\n",
      "  Mean improvement over greedy: -0.1835\n",
      "Run 9 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 48.54%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4577\n",
      "  Mean improvement over greedy: -0.4468\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.35041090250329376\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.3956556637806639\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.4144905956112854\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.4315243989187652\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.42023200757575757\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.4832138785978911\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.37572843822843827\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.43242375679875683\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.458779059883548\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.34075011195700855\n",
      "Run 10 training metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.95%\n",
      "  #opt: 219/1000\n",
      "  Mean percentage error: 0.2136\n",
      "  Mean improvement over greedy: -0.2008\n",
      "Run 10 test metrics for DQN with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 46.38%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4078\n",
      "  Mean improvement over greedy: -0.3814\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for DQN with (+vr_i -wr_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 72.95% ± 0.94%\n",
      "  #opt: 233.90 ± 11.42\n",
      "  Mean percentage error: 0.2082 ± 0.0095\n",
      "  Mean improvement over greedy: -0.1931 ± 0.0105\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 53.12% ± 10.20%\n",
      "  #opt: 0.70 ± 0.64\n",
      "  Mean percentage error: 0.3932 ± 0.1051\n",
      "  Mean improvement over greedy: -0.3739 ± 0.1088\n",
      "\n",
      "Experiment 7/15: Testing DQN with (+vr_i -w_i)\n",
      "Positive reward: vr_i, Negative reward: w_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -13.199019652464369\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Run 1 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 64.99%\n",
      "  #opt: 202/1000\n",
      "  Mean percentage error: 0.2817\n",
      "  Mean improvement over greedy: -0.2650\n",
      "Run 1 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 49.89%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3873\n",
      "  Mean improvement over greedy: -0.3508\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.52244526794329\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.52244526794328\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -41.21990778984759\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.52244526794328\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -45.65704835071032\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -44.52244526794328\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.51936516092836\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -44.52244526794328\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.52244526794328\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.52244526794328\n",
      "Run 2 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 62.32%\n",
      "  #opt: 204/1000\n",
      "  Mean percentage error: 0.3030\n",
      "  Mean improvement over greedy: -0.2864\n",
      "Run 2 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 57.08%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2655\n",
      "  Mean improvement over greedy: -0.2655\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -36.21137469505834\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -34.87421282272599\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -41.910438981845445\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -34.87421282272599\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -41.91043898184545\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -41.93936687534878\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -34.657441199089455\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -41.857096099424666\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -35.936756554796766\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -33.85055323744485\n",
      "Run 3 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 64.69%\n",
      "  #opt: 208/1000\n",
      "  Mean percentage error: 0.2816\n",
      "  Mean improvement over greedy: -0.2646\n",
      "Run 3 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 58.23%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4083\n",
      "  Mean improvement over greedy: -0.3884\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -54.12317251461988\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -51.59874788597511\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -53.21375487329434\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -54.381970551378444\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -51.59874788597511\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -51.59874788597511\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -51.598747885975094\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -51.59874788597511\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -51.598747885975094\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -60.45733928246221\n",
      "Run 4 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 66.24%\n",
      "  #opt: 210/1000\n",
      "  Mean percentage error: 0.2724\n",
      "  Mean improvement over greedy: -0.2578\n",
      "Run 4 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 61.29%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3650\n",
      "  Mean improvement over greedy: -0.3457\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -50.215871142582394\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -33.57912186379928\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -50.276010640290735\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -50.28053463714361\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -50.58876170655567\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -50.23189089955416\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.37443548387096\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -50.215871142582394\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -50.215871142582394\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -50.23189089955416\n",
      "Run 5 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 63.29%\n",
      "  #opt: 205/1000\n",
      "  Mean percentage error: 0.2900\n",
      "  Mean improvement over greedy: -0.2763\n",
      "Run 5 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 49.83%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4776\n",
      "  Mean improvement over greedy: -0.4399\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.31050179694026\n",
      "Run 6 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 63.49%\n",
      "  #opt: 189/1000\n",
      "  Mean percentage error: 0.2928\n",
      "  Mean improvement over greedy: -0.2767\n",
      "Run 6 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 66.58%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2738\n",
      "  Mean improvement over greedy: -0.2646\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Run 7 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 64.08%\n",
      "  #opt: 208/1000\n",
      "  Mean percentage error: 0.2843\n",
      "  Mean improvement over greedy: -0.2724\n",
      "Run 7 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 66.35%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2196\n",
      "  Mean improvement over greedy: -0.2066\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -50.07347262372179\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -50.07347262372179\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -49.3941507711039\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -49.438732363981536\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -50.07347262372179\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -49.43873236398153\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -49.438732363981536\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -49.43873236398153\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -49.70889139850785\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -50.07347262372179\n",
      "Run 8 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 64.01%\n",
      "  #opt: 182/1000\n",
      "  Mean percentage error: 0.2923\n",
      "  Mean improvement over greedy: -0.2766\n",
      "Run 8 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 63.51%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3628\n",
      "  Mean improvement over greedy: -0.3433\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -38.3586066132541\n",
      "Run 9 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 66.23%\n",
      "  #opt: 237/1000\n",
      "  Mean percentage error: 0.2614\n",
      "  Mean improvement over greedy: -0.2458\n",
      "Run 9 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 60.70%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4236\n",
      "  Mean improvement over greedy: -0.4128\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.36419662930243\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.36223776487703\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -31.411655020703932\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.442357005240034\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -44.402182166492516\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.017704691142191142\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -24.813597524154588\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -44.32602419282107\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.45272873827562\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.442357005240034\n",
      "Run 10 training metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 63.83%\n",
      "  #opt: 188/1000\n",
      "  Mean percentage error: 0.2913\n",
      "  Mean improvement over greedy: -0.2785\n",
      "Run 10 test metrics for DQN with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 58.15%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3862\n",
      "  Mean improvement over greedy: -0.3598\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for DQN with (+vr_i -w_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 64.32% ± 1.19%\n",
      "  #opt: 203.30 ± 14.57\n",
      "  Mean percentage error: 0.2851 ± 0.0111\n",
      "  Mean improvement over greedy: -0.2700 ± 0.0112\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 59.16% ± 5.58%\n",
      "  #opt: 0.80 ± 0.60\n",
      "  Mean percentage error: 0.3570 ± 0.0759\n",
      "  Mean improvement over greedy: -0.3377 ± 0.0687\n",
      "\n",
      "Experiment 8/15: Testing DQN with (+v_i -wr_i)\n",
      "Positive reward: v_i, Negative reward: wr_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Run 1 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 66.32%\n",
      "  #opt: 189/1000\n",
      "  Mean percentage error: 0.2735\n",
      "  Mean improvement over greedy: -0.2568\n",
      "Run 1 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 52.93%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3124\n",
      "  Mean improvement over greedy: -0.2758\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 3.0811712379540404\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 3.081171237954041\n",
      "Run 2 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 55.04%\n",
      "  #opt: 145/1000\n",
      "  Mean percentage error: 0.3831\n",
      "  Mean improvement over greedy: -0.3666\n",
      "Run 2 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 48.40%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3112\n",
      "  Mean improvement over greedy: -0.3112\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 8.90909090909091\n",
      "Run 3 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 55.30%\n",
      "  #opt: 148/1000\n",
      "  Mean percentage error: 0.3829\n",
      "  Mean improvement over greedy: -0.3660\n",
      "Run 3 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 65.78%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3950\n",
      "  Mean improvement over greedy: -0.3751\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 12.538011695906432\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 12.578947368421053\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 13.421052631578947\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 12.612573099415206\n",
      "Run 4 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 55.20%\n",
      "  #opt: 151/1000\n",
      "  Mean percentage error: 0.3820\n",
      "  Mean improvement over greedy: -0.3674\n",
      "Run 4 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 49.59%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3799\n",
      "  Mean improvement over greedy: -0.3605\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 2.800944138473642\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 2.8009441384736427\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 2.8009441384736427\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 1.3658536585365866\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 2.800944138473643\n",
      "Run 5 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 56.92%\n",
      "  #opt: 166/1000\n",
      "  Mean percentage error: 0.3598\n",
      "  Mean improvement over greedy: -0.3460\n",
      "Run 5 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 46.77%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5054\n",
      "  Mean improvement over greedy: -0.4677\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 10.246603260869565\n",
      "Run 6 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 56.17%\n",
      "  #opt: 164/1000\n",
      "  Mean percentage error: 0.3640\n",
      "  Mean improvement over greedy: -0.3480\n",
      "Run 6 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 49.67%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4965\n",
      "  Mean improvement over greedy: -0.4873\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 15.556930693069306\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 15.556930693069306\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Run 7 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 55.72%\n",
      "  #opt: 168/1000\n",
      "  Mean percentage error: 0.3685\n",
      "  Mean improvement over greedy: -0.3565\n",
      "Run 7 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 59.99%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3219\n",
      "  Mean improvement over greedy: -0.3089\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 10.646103896103895\n",
      "Run 8 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 55.70%\n",
      "  #opt: 157/1000\n",
      "  Mean percentage error: 0.3721\n",
      "  Mean improvement over greedy: -0.3564\n",
      "Run 8 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 72.13%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2537\n",
      "  Mean improvement over greedy: -0.2342\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 8.249197860962568\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 16.38770053475936\n",
      "Run 9 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 56.31%\n",
      "  #opt: 175/1000\n",
      "  Mean percentage error: 0.3590\n",
      "  Mean improvement over greedy: -0.3434\n",
      "Run 9 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 58.26%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4248\n",
      "  Mean improvement over greedy: -0.4140\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 10.709469696969704\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 10.37310606060606\n",
      "Run 10 training metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 54.43%\n",
      "  #opt: 150/1000\n",
      "  Mean percentage error: 0.3852\n",
      "  Mean improvement over greedy: -0.3725\n",
      "Run 10 test metrics for DQN with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 56.43%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3040\n",
      "  Mean improvement over greedy: -0.2776\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for DQN with (+v_i -wr_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 56.71% ± 3.27%\n",
      "  #opt: 161.30 ± 13.13\n",
      "  Mean percentage error: 0.3630 ± 0.0313\n",
      "  Mean improvement over greedy: -0.3480 ± 0.0318\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 56.00% ± 7.81%\n",
      "  #opt: 0.90 ± 0.83\n",
      "  Mean percentage error: 0.3705 ± 0.0806\n",
      "  Mean improvement over greedy: -0.3512 ± 0.0806\n",
      "\n",
      "Experiment 9/15: Testing DQN with (+v_i -w_i)\n",
      "Positive reward: v_i, Negative reward: w_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 19.6\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 21.8\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 19.6\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Run 1 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 61.60%\n",
      "  #opt: 197/1000\n",
      "  Mean percentage error: 0.3071\n",
      "  Mean improvement over greedy: -0.2905\n",
      "Run 1 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 50.77%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3248\n",
      "  Mean improvement over greedy: -0.2883\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -40.68421052631579\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -40.68421052631579\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -40.68421052631579\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -40.68421052631579\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -36.026315789473685\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -37.1578947368421\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -40.68421052631579\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -40.68421052631579\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -33.973684210526315\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -36.31578947368421\n",
      "Run 2 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 64.08%\n",
      "  #opt: 206/1000\n",
      "  Mean percentage error: 0.2804\n",
      "  Mean improvement over greedy: -0.2638\n",
      "Run 2 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 55.75%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2600\n",
      "  Mean improvement over greedy: -0.2600\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -32.75\n",
      "Run 3 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 65.23%\n",
      "  #opt: 217/1000\n",
      "  Mean percentage error: 0.2683\n",
      "  Mean improvement over greedy: -0.2514\n",
      "Run 3 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 71.36%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2514\n",
      "  Mean improvement over greedy: -0.2315\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.25\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -41.78378378378378\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -45.8235294117647\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.675\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -45.18421052631579\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -45.9\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.17948717948718\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -44.41463414634146\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -43.4\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.888888888888886\n",
      "Run 4 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 60.79%\n",
      "  #opt: 191/1000\n",
      "  Mean percentage error: 0.3114\n",
      "  Mean improvement over greedy: -0.2968\n",
      "Run 4 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 48.43%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4818\n",
      "  Mean improvement over greedy: -0.4624\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -45.829268292682926\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -45.829268292682926\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -45.51219512195122\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -45.829268292682926\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -45.21212121212121\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -45.829268292682926\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -47.638888888888886\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -45.51219512195122\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -45.51219512195122\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -45.51219512195122\n",
      "Run 5 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 62.16%\n",
      "  #opt: 183/1000\n",
      "  Mean percentage error: 0.3027\n",
      "  Mean improvement over greedy: -0.2889\n",
      "Run 5 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 51.37%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3808\n",
      "  Mean improvement over greedy: -0.3431\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -37.6875\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -34.5625\n",
      "Run 6 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 61.14%\n",
      "  #opt: 209/1000\n",
      "  Mean percentage error: 0.3052\n",
      "  Mean improvement over greedy: -0.2892\n",
      "Run 6 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 46.54%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5173\n",
      "  Mean improvement over greedy: -0.5081\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -17.25\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -17.25\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -17.25\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -17.25\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -17.25\n",
      "Run 7 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 59.10%\n",
      "  #opt: 196/1000\n",
      "  Mean percentage error: 0.3309\n",
      "  Mean improvement over greedy: -0.3190\n",
      "Run 7 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 59.99%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3219\n",
      "  Mean improvement over greedy: -0.3089\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -41.13636363636363\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -41.13636363636363\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -38.54545454545455\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -41.13636363636363\n",
      "Run 8 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 59.38%\n",
      "  #opt: 179/1000\n",
      "  Mean percentage error: 0.3310\n",
      "  Mean improvement over greedy: -0.3153\n",
      "Run 8 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 61.75%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3513\n",
      "  Mean improvement over greedy: -0.3318\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Run 9 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 59.87%\n",
      "  #opt: 193/1000\n",
      "  Mean percentage error: 0.3202\n",
      "  Mean improvement over greedy: -0.3046\n",
      "Run 9 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 58.26%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4248\n",
      "  Mean improvement over greedy: -0.4140\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -28.416666666666668\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -35.25\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -28.416666666666668\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -33.583333333333336\n",
      "Run 10 training metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 58.15%\n",
      "  #opt: 170/1000\n",
      "  Mean percentage error: 0.3421\n",
      "  Mean improvement over greedy: -0.3294\n",
      "Run 10 test metrics for DQN with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 50.86%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3408\n",
      "  Mean improvement over greedy: -0.3144\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for DQN with (+v_i -w_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 61.15% ± 2.11%\n",
      "  #opt: 194.10 ± 13.58\n",
      "  Mean percentage error: 0.3099 ± 0.0217\n",
      "  Mean improvement over greedy: -0.2949 ± 0.0229\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 55.51% ± 7.14%\n",
      "  #opt: 0.80 ± 0.87\n",
      "  Mean percentage error: 0.3655 ± 0.0830\n",
      "  Mean improvement over greedy: -0.3463 ± 0.0842\n",
      "\n",
      "Experiment 10/15: Testing DQN with (+_1 -_1)\n",
      "Positive reward: _1, Negative reward: _1\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Run 1 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 60.02%\n",
      "  #opt: 187/1000\n",
      "  Mean percentage error: 0.3202\n",
      "  Mean improvement over greedy: -0.3035\n",
      "Run 1 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 54.75%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3260\n",
      "  Mean improvement over greedy: -0.2895\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.7368421052631579\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.7368421052631579\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.7297297297297297\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.8421052631578947\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.6842105263157895\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.7894736842105263\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.7894736842105263\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.52\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.6774193548387096\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.7894736842105263\n",
      "Run 2 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 69.81%\n",
      "  #opt: 253/1000\n",
      "  Mean percentage error: 0.2284\n",
      "  Mean improvement over greedy: -0.2118\n",
      "Run 2 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 70.55%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.1748\n",
      "  Mean improvement over greedy: -0.1748\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.7222222222222222\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.29411764705882354\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.6111111111111112\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.07692307692307693\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.6111111111111112\n",
      "Run 3 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 68.27%\n",
      "  #opt: 228/1000\n",
      "  Mean percentage error: 0.2432\n",
      "  Mean improvement over greedy: -0.2262\n",
      "Run 3 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 70.11%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3647\n",
      "  Mean improvement over greedy: -0.3449\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.14285714285714285\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.14285714285714285\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.5714285714285714\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.85\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.3333333333333333\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Run 4 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 70.31%\n",
      "  #opt: 241/1000\n",
      "  Mean percentage error: 0.2281\n",
      "  Mean improvement over greedy: -0.2135\n",
      "Run 4 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 49.51%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3168\n",
      "  Mean improvement over greedy: -0.2975\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.7142857142857143\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.7142857142857143\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.7\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.7\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7142857142857143\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.9024390243902439\n",
      "Run 5 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 71.90%\n",
      "  #opt: 246/1000\n",
      "  Mean percentage error: 0.2149\n",
      "  Mean improvement over greedy: -0.2011\n",
      "Run 5 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 44.30%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5353\n",
      "  Mean improvement over greedy: -0.4976\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Run 6 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 73.37%\n",
      "  #opt: 243/1000\n",
      "  Mean percentage error: 0.1995\n",
      "  Mean improvement over greedy: -0.1835\n",
      "Run 6 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 35.85%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.6127\n",
      "  Mean improvement over greedy: -0.6035\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Run 7 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 72.39%\n",
      "  #opt: 248/1000\n",
      "  Mean percentage error: 0.2104\n",
      "  Mean improvement over greedy: -0.1985\n",
      "Run 7 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 52.01%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3535\n",
      "  Mean improvement over greedy: -0.3405\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5454545454545454\n",
      "Run 8 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 71.60%\n",
      "  #opt: 219/1000\n",
      "  Mean percentage error: 0.2226\n",
      "  Mean improvement over greedy: -0.2070\n",
      "Run 8 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 60.17%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3703\n",
      "  Mean improvement over greedy: -0.3507\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 1.0\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Run 9 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 71.52%\n",
      "  #opt: 261/1000\n",
      "  Mean percentage error: 0.2089\n",
      "  Mean improvement over greedy: -0.1933\n",
      "Run 9 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 60.16%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3747\n",
      "  Mean improvement over greedy: -0.3639\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackQLearning.KnapsackDQN'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.5\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.4166666666666667\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.375\n",
      "Run 10 training metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 73.70%\n",
      "  #opt: 211/1000\n",
      "  Mean percentage error: 0.2061\n",
      "  Mean improvement over greedy: -0.1934\n",
      "Run 10 test metrics for DQN with (+_1 -_1):\n",
      "  Val/Opt Ratio: 63.90%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2433\n",
      "  Mean improvement over greedy: -0.2169\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for DQN with (+_1 -_1):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 70.29% ± 3.76%\n",
      "  #opt: 233.70 ± 21.35\n",
      "  Mean percentage error: 0.2282 ± 0.0330\n",
      "  Mean improvement over greedy: -0.2132 ± 0.0323\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 56.13% ± 10.54%\n",
      "  #opt: 1.00 ± 0.89\n",
      "  Mean percentage error: 0.3672 ± 0.1206\n",
      "  Mean improvement over greedy: -0.3480 ± 0.1188\n",
      "\n",
      "Experiment 11/15: Testing PPO with (+vr_i -wr_i)\n",
      "Positive reward: vr_i, Negative reward: wr_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.012955943704415804\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.0348938319928081\n",
      "Run 1 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 62.36%\n",
      "  #opt: 146/1000\n",
      "  Mean percentage error: 0.3277\n",
      "  Mean improvement over greedy: -0.3110\n",
      "Run 1 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 52.93%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3124\n",
      "  Mean improvement over greedy: -0.2758\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6774687831513554\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.6774687831513554\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.6774687831513554\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5030537083694738\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.22892423880726578\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.4123539894481332\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.6786481137277387\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6263469394192703\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5118616793352708\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.6536063100672361\n",
      "Run 2 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 72.95%\n",
      "  #opt: 238/1000\n",
      "  Mean percentage error: 0.2108\n",
      "  Mean improvement over greedy: -0.1942\n",
      "Run 2 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 53.78%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2726\n",
      "  Mean improvement over greedy: -0.2726\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.2013746156643301\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.1938483213242294\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.194322939009448\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.21587560839782047\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.21235640343355217\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.18755677686942762\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.13157389652914644\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.19871617341415357\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.18327056126882674\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.20870169119753515\n",
      "Run 3 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 76.77%\n",
      "  #opt: 290/1000\n",
      "  Mean percentage error: 0.1702\n",
      "  Mean improvement over greedy: -0.1533\n",
      "Run 3 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 60.25%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4312\n",
      "  Mean improvement over greedy: -0.4114\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -1.3542035984269618\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -1.5074912891986065\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -1.373958333333333\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -1.3542035984269616\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -1.3485562228742587\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -1.3373108015135877\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.9578216374269006\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -1.4342068375311707\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7605219946417768\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -1.3377786915204681\n",
      "Run 4 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 76.02%\n",
      "  #opt: 280/1000\n",
      "  Mean percentage error: 0.1784\n",
      "  Mean improvement over greedy: -0.1638\n",
      "Run 4 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 55.96%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3282\n",
      "  Mean improvement over greedy: -0.3088\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -1.3426004527447652\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -1.6446586240055947\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -1.6737726824272852\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -1.5856587114258238\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -1.6446586240055947\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -1.7180260512282544\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.9509472606246798\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -1.720717213963311\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -1.6753343823760818\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -1.6331497508523474\n",
      "Run 5 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 75.65%\n",
      "  #opt: 284/1000\n",
      "  Mean percentage error: 0.1778\n",
      "  Mean improvement over greedy: -0.1641\n",
      "Run 5 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 58.37%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3656\n",
      "  Mean improvement over greedy: -0.3279\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.48824688878386896\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5734740802675585\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.5257436208757323\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.48530236369910273\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.5493165059195754\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.5734740802675585\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.2716338411990586\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.5278818491501758\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.46735620793591814\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.5734740802675584\n",
      "Run 6 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 76.15%\n",
      "  #opt: 283/1000\n",
      "  Mean percentage error: 0.1735\n",
      "  Mean improvement over greedy: -0.1575\n",
      "Run 6 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 47.98%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4910\n",
      "  Mean improvement over greedy: -0.4818\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.18484598459845986\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.20132940260318166\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.20132940260318166\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.18484598459845983\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.18484598459845986\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.2882627216209993\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.1493463299818354\n",
      "Run 7 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 75.24%\n",
      "  #opt: 271/1000\n",
      "  Mean percentage error: 0.1838\n",
      "  Mean improvement over greedy: -0.1719\n",
      "Run 7 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 48.04%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3577\n",
      "  Mean improvement over greedy: -0.3447\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.28012692619277435\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.31490907491523257\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.24271783660498095\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.2802712960596236\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.3321155812531625\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.3101421732021006\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.3226682526943362\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.3044556820861149\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.22795220822704085\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.30915118177953\n",
      "Run 8 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 75.07%\n",
      "  #opt: 269/1000\n",
      "  Mean percentage error: 0.1854\n",
      "  Mean improvement over greedy: -0.1698\n",
      "Run 8 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 68.61%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3050\n",
      "  Mean improvement over greedy: -0.2854\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.1505257549894228\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.1657736138760807\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.1657736138760807\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.07642202202371368\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.16306863267647584\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.16306863267647584\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.12534478471978472\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.12534478471978472\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.1590112019449836\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.2037902210299846\n",
      "Run 9 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 76.50%\n",
      "  #opt: 303/1000\n",
      "  Mean percentage error: 0.1707\n",
      "  Mean improvement over greedy: -0.1551\n",
      "Run 9 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 47.94%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4877\n",
      "  Mean improvement over greedy: -0.4769\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.37067933736188863\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.16519803449238935\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.4557590413059163\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.35978861192570877\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.3078302611367128\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.460655561294766\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.4691219008264464\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.3393571834950293\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.41293512043512054\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.38482265840220387\n",
      "Run 10 training metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 75.43%\n",
      "  #opt: 276/1000\n",
      "  Mean percentage error: 0.1824\n",
      "  Mean improvement over greedy: -0.1697\n",
      "Run 10 test metrics for PPO with (+vr_i -wr_i):\n",
      "  Val/Opt Ratio: 45.88%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.5038\n",
      "  Mean improvement over greedy: -0.4774\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for PPO with (+vr_i -wr_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 74.21% ± 4.08%\n",
      "  #opt: 264.00 ± 42.46\n",
      "  Mean percentage error: 0.1961 ± 0.0452\n",
      "  Mean improvement over greedy: -0.1810 ± 0.0447\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 53.97% ± 6.72%\n",
      "  #opt: 0.80 ± 0.75\n",
      "  Mean percentage error: 0.3855 ± 0.0817\n",
      "  Mean improvement over greedy: -0.3663 ± 0.0829\n",
      "\n",
      "Experiment 12/15: Testing PPO with (+vr_i -w_i)\n",
      "Positive reward: vr_i, Negative reward: w_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -8.798621472714654\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -8.798621472714654\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -8.79845082425049\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -8.79845082425049\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -13.199019652464369\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -15.598374450112683\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -17.19871635759008\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -8.79845082425049\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -8.798621472714654\n",
      "Run 1 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.62%\n",
      "  #opt: 266/1000\n",
      "  Mean percentage error: 0.1825\n",
      "  Mean improvement over greedy: -0.1658\n",
      "Run 1 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 70.77%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2682\n",
      "  Mean improvement over greedy: -0.2317\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.445403870827924\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.42714226262469\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -44.451750267688\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -42.65410901078989\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -44.52717329848852\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -41.693424517694396\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -43.27843328461482\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -42.47924205852173\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.55134574215157\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -50.78444302176697\n",
      "Run 2 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.68%\n",
      "  #opt: 281/1000\n",
      "  Mean percentage error: 0.1808\n",
      "  Mean improvement over greedy: -0.1642\n",
      "Run 2 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 58.63%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2555\n",
      "  Mean improvement over greedy: -0.2555\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -41.80097789800777\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -41.885701839757886\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -41.968111918789845\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -41.85839874393284\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -41.88793677978726\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -41.8278441114336\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -41.856876268435116\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -41.86039329213652\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -41.8576970598729\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.008028856135332916\n",
      "Run 3 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 76.73%\n",
      "  #opt: 302/1000\n",
      "  Mean percentage error: 0.1693\n",
      "  Mean improvement over greedy: -0.1524\n",
      "Run 3 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 55.08%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5236\n",
      "  Mean improvement over greedy: -0.5037\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -49.65394254867939\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.0546157059315\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -54.08182401817351\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -51.57705483220246\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -53.4234414160401\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -50.11326268256849\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -51.57733905832706\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -51.453722447143505\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -46.87612549893252\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -50.54131449312938\n",
      "Run 4 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 76.20%\n",
      "  #opt: 290/1000\n",
      "  Mean percentage error: 0.1759\n",
      "  Mean improvement over greedy: -0.1613\n",
      "Run 4 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 61.51%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2945\n",
      "  Mean improvement over greedy: -0.2752\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -50.24654690095288\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -50.2426435877262\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -50.25213742460004\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -50.215871142582394\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -50.2426435877262\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -34.92986559139785\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -51.20938416422287\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -50.24654690095288\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -50.23945712037765\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.05619146722164412\n",
      "Run 5 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.56%\n",
      "  #opt: 288/1000\n",
      "  Mean percentage error: 0.1774\n",
      "  Mean improvement over greedy: -0.1636\n",
      "Run 5 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 49.59%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4495\n",
      "  Mean improvement over greedy: -0.4118\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -45.06018607113697\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.87052321024771\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -44.24881374642342\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.021897064288368635\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -44.93522775213421\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -44.18339528152591\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -44.93522775213421\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -44.93522775213421\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.18339528152591\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.6230664715719\n",
      "Run 6 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.76%\n",
      "  #opt: 276/1000\n",
      "  Mean percentage error: 0.1768\n",
      "  Mean improvement over greedy: -0.1608\n",
      "Run 6 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 55.93%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3526\n",
      "  Mean improvement over greedy: -0.3434\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -26.74865876122496\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -32.991776677667765\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -26.74865876122496\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -34.74093048839768\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -27.748359105573478\n",
      "Run 7 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.25%\n",
      "  #opt: 267/1000\n",
      "  Mean percentage error: 0.1850\n",
      "  Mean improvement over greedy: -0.1731\n",
      "Run 7 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 42.51%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4613\n",
      "  Mean improvement over greedy: -0.4483\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -49.61726163580731\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -44.81965093729799\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -49.618649626043606\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -49.709492822552754\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -49.61864962604359\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -49.2997119955937\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -49.3941507711039\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -49.39051653658239\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -49.800030964580195\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -29.531092436974784\n",
      "Run 8 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.12%\n",
      "  #opt: 271/1000\n",
      "  Mean percentage error: 0.1868\n",
      "  Mean improvement over greedy: -0.1712\n",
      "Run 8 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 56.77%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4132\n",
      "  Mean improvement over greedy: -0.3937\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -36.63279930396064\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -36.176681683115504\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -36.451352568999624\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -37.54136482390159\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -38.90496088398338\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -36.176681683115504\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -36.722023013750956\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -36.722023013750956\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -36.54466670054905\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -36.904147288990856\n",
      "Run 9 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 76.35%\n",
      "  #opt: 290/1000\n",
      "  Mean percentage error: 0.1717\n",
      "  Mean improvement over greedy: -0.1561\n",
      "Run 9 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 43.87%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5866\n",
      "  Mean improvement over greedy: -0.5758\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -44.32290945165945\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -35.511873825099634\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -39.56047842334637\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -44.57183626958737\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -27.78242191142191\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -44.28903236914601\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -40.4900544235458\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -44.28636048905477\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.60456389392369\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -44.607092648513095\n",
      "Run 10 training metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 75.64%\n",
      "  #opt: 274/1000\n",
      "  Mean percentage error: 0.1814\n",
      "  Mean improvement over greedy: -0.1686\n",
      "Run 10 test metrics for PPO with (+vr_i -w_i):\n",
      "  Val/Opt Ratio: 44.84%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4433\n",
      "  Mean improvement over greedy: -0.4169\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for PPO with (+vr_i -w_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 75.79% ± 0.47%\n",
      "  #opt: 280.50 ± 11.16\n",
      "  Mean percentage error: 0.1788 ± 0.0053\n",
      "  Mean improvement over greedy: -0.1637 ± 0.0061\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 53.95% ± 8.42%\n",
      "  #opt: 0.70 ± 0.64\n",
      "  Mean percentage error: 0.4048 ± 0.1047\n",
      "  Mean improvement over greedy: -0.3856 ± 0.1051\n",
      "\n",
      "Experiment 13/15: Testing PPO with (+v_i -wr_i)\n",
      "Positive reward: v_i, Negative reward: wr_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 30.593174061433444\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 28.385665529010236\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 18.55221843003413\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 30.793856655290103\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 30.793856655290103\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 30.593174061433444\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 30.593174061433444\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 21.96382252559727\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 28.385665529010236\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 30.593174061433444\n",
      "Run 1 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 75.11%\n",
      "  #opt: 259/1000\n",
      "  Mean percentage error: 0.1855\n",
      "  Mean improvement over greedy: -0.1689\n",
      "Run 1 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 63.87%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2837\n",
      "  Mean improvement over greedy: -0.2472\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 6.817272053372872\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 30.390845070422536\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 6.577094143810228\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 5.883246849518161\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 3.828391401037807\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 25.438380281690144\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 3.0250880281690145\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 5.376204595997035\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 5.296145292809489\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 6.870644922164567\n",
      "Run 2 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 76.05%\n",
      "  #opt: 296/1000\n",
      "  Mean percentage error: 0.1767\n",
      "  Mean improvement over greedy: -0.1601\n",
      "Run 2 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 46.81%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3300\n",
      "  Mean improvement over greedy: -0.3300\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 25.459545454545456\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 17.782575757575763\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 20.325974025974027\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 17.610984848484843\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 9.802020202020204\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 17.838383838383844\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 21.85397727272727\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 19.406493506493508\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 8.713762626262628\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 7.541792929292929\n",
      "Run 3 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 76.41%\n",
      "  #opt: 287/1000\n",
      "  Mean percentage error: 0.1705\n",
      "  Mean improvement over greedy: -0.1536\n",
      "Run 3 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 53.59%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5205\n",
      "  Mean improvement over greedy: -0.5007\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 3.9480609418282553\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 9.425207756232686\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 5.326059050064186\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 1.1957637997432602\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 6.938122332859177\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 4.235380116959064\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 1.8465982028241332\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 5.526315789473686\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 5.837837837837839\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 12.629186602870815\n",
      "Run 4 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 75.38%\n",
      "  #opt: 268/1000\n",
      "  Mean percentage error: 0.1839\n",
      "  Mean improvement over greedy: -0.1693\n",
      "Run 4 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 51.05%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3578\n",
      "  Mean improvement over greedy: -0.3384\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.6860739575137684\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.6860739575137689\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.6860739575137684\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 1.6176239181746663\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 10.89516129032258\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.28324154209284014\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.5476003147128246\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 4.150201612903227\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 1.0889063729346973\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 5.580645161290322\n",
      "Run 5 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 75.77%\n",
      "  #opt: 290/1000\n",
      "  Mean percentage error: 0.1762\n",
      "  Mean improvement over greedy: -0.1624\n",
      "Run 5 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 61.23%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3214\n",
      "  Mean improvement over greedy: -0.2837\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 6.203125\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 8.730298913043478\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 11.067934782608694\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 6.455842391304348\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 10.752038043478262\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 5.192255434782608\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 5.634510869565218\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 5.634510869565218\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 3.5495923913043486\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 7.656249999999999\n",
      "Run 6 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 76.16%\n",
      "  #opt: 270/1000\n",
      "  Mean percentage error: 0.1761\n",
      "  Mean improvement over greedy: -0.1601\n",
      "Run 6 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 36.94%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5883\n",
      "  Mean improvement over greedy: -0.5791\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 5.71039603960396\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 19.09158415841584\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 14.547029702970297\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 5.71039603960396\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 14.547029702970297\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 15.556930693069306\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 15.556930693069306\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 14.547029702970297\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 15.556930693069306\n",
      "Run 7 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 75.11%\n",
      "  #opt: 284/1000\n",
      "  Mean percentage error: 0.1842\n",
      "  Mean improvement over greedy: -0.1723\n",
      "Run 7 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 49.45%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4105\n",
      "  Mean improvement over greedy: -0.3975\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 6.931412337662336\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 10.422619047619046\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 6.977272727272726\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 9.08685064935065\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 8.122321428571428\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 6.197646103896104\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 8.582386363636365\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 7.298295454545454\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 7.0689935064935066\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 5.9683441558441555\n",
      "Run 8 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 75.46%\n",
      "  #opt: 266/1000\n",
      "  Mean percentage error: 0.1831\n",
      "  Mean improvement over greedy: -0.1674\n",
      "Run 8 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 64.24%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3679\n",
      "  Mean improvement over greedy: -0.3484\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 16.753475935828877\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 15.656149732620321\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 15.656149732620321\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 16.753475935828877\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 11.358288770053475\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 37.20294117647059\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 37.247058823529414\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 10.992513368983957\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 10.26096256684492\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 15.930481283422461\n",
      "Run 9 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 76.01%\n",
      "  #opt: 307/1000\n",
      "  Mean percentage error: 0.1743\n",
      "  Mean improvement over greedy: -0.1587\n",
      "Run 9 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 46.01%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5215\n",
      "  Mean improvement over greedy: -0.5107\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 13.358333333333334\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 6.3367424242424235\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 16.301515151515158\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 7.303787878787877\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 13.694696969696976\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 8.691287878787879\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 13.232196969696973\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 19.20265151515152\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 13.778787878787883\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 12.601515151515159\n",
      "Run 10 training metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 75.37%\n",
      "  #opt: 260/1000\n",
      "  Mean percentage error: 0.1838\n",
      "  Mean improvement over greedy: -0.1710\n",
      "Run 10 test metrics for PPO with (+v_i -wr_i):\n",
      "  Val/Opt Ratio: 58.61%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3372\n",
      "  Mean improvement over greedy: -0.3108\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for PPO with (+v_i -wr_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 75.68% ± 0.44%\n",
      "  #opt: 278.70 ± 15.54\n",
      "  Mean percentage error: 0.1794 ± 0.0050\n",
      "  Mean improvement over greedy: -0.1644 ± 0.0059\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 53.18% ± 8.40%\n",
      "  #opt: 0.60 ± 0.66\n",
      "  Mean percentage error: 0.4039 ± 0.0980\n",
      "  Mean improvement over greedy: -0.3847 ± 0.1041\n",
      "\n",
      "Experiment 14/15: Testing PPO with (+v_i -w_i)\n",
      "Positive reward: v_i, Negative reward: w_i\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 5.4\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 5.4\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 5.4\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 4.8\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 21.8\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 15.2\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 5.4\n",
      "Run 1 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.38%\n",
      "  #opt: 261/1000\n",
      "  Mean percentage error: 0.1840\n",
      "  Mean improvement over greedy: -0.1674\n",
      "Run 1 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 67.68%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2314\n",
      "  Mean improvement over greedy: -0.1948\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -37.21052631578947\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -38.1578947368421\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -39.0\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -32.3448275862069\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -36.27777777777778\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -36.083333333333336\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -36.86842105263158\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -34.69565217391305\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -28.136363636363637\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -41.10526315789474\n",
      "Run 2 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 76.22%\n",
      "  #opt: 298/1000\n",
      "  Mean percentage error: 0.1742\n",
      "  Mean improvement over greedy: -0.1576\n",
      "Run 2 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 56.34%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2730\n",
      "  Mean improvement over greedy: -0.2730\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -34.916666666666664\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -30.666666666666668\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -28.97222222222222\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -33.94444444444444\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -37.138888888888886\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -17.210526315789473\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -31.083333333333332\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -37.611111111111114\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 59.333333333333336\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -27.666666666666668\n",
      "Run 3 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 76.00%\n",
      "  #opt: 288/1000\n",
      "  Mean percentage error: 0.1745\n",
      "  Mean improvement over greedy: -0.1575\n",
      "Run 3 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 64.23%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3285\n",
      "  Mean improvement over greedy: -0.3087\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -36.61904761904762\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -47.3125\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -48.26829268292683\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -49.170731707317074\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -44.64102564102564\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -38.333333333333336\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -45.24390243902439\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -46.170731707317074\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -39.75\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -48.90243902439025\n",
      "Run 4 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.89%\n",
      "  #opt: 296/1000\n",
      "  Mean percentage error: 0.1769\n",
      "  Mean improvement over greedy: -0.1623\n",
      "Run 4 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 56.69%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3289\n",
      "  Mean improvement over greedy: -0.3095\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -46.65853658536585\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -23.333333333333332\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -38.833333333333336\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -21.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -40.81818181818182\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -47.75609756097561\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -47.146341463414636\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -47.24390243902439\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -44.0\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -41.4\n",
      "Run 5 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.98%\n",
      "  #opt: 306/1000\n",
      "  Mean percentage error: 0.1734\n",
      "  Mean improvement over greedy: -0.1597\n",
      "Run 5 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 38.15%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5916\n",
      "  Mean improvement over greedy: -0.5539\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -38.75\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -36.0\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -39.25\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -38.75\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -35.9375\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -39.25\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -35.9375\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -39.3125\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -38.75\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -33.25\n",
      "Run 6 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.83%\n",
      "  #opt: 282/1000\n",
      "  Mean percentage error: 0.1761\n",
      "  Mean improvement over greedy: -0.1600\n",
      "Run 6 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 40.11%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5889\n",
      "  Mean improvement over greedy: -0.5798\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -13.0\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -17.25\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -13.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -15.5\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -20.75\n",
      "Run 7 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.79%\n",
      "  #opt: 282/1000\n",
      "  Mean percentage error: 0.1783\n",
      "  Mean improvement over greedy: -0.1664\n",
      "Run 7 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 56.91%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.2973\n",
      "  Mean improvement over greedy: -0.2843\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -40.72727272727273\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -42.54545454545455\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -38.142857142857146\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -37.68181818181818\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -42.54545454545455\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -43.95454545454545\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -39.09090909090909\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -43.72727272727273\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -40.72727272727273\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -41.36363636363637\n",
      "Run 8 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.55%\n",
      "  #opt: 277/1000\n",
      "  Mean percentage error: 0.1806\n",
      "  Mean improvement over greedy: -0.1650\n",
      "Run 8 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 73.89%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.2726\n",
      "  Mean improvement over greedy: -0.2531\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -21.545454545454547\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -21.363636363636363\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -23.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -21.727272727272727\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -19.636363636363637\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -23.0\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -19.818181818181817\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -21.363636363636363\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -20.454545454545453\n",
      "Run 9 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.99%\n",
      "  #opt: 295/1000\n",
      "  Mean percentage error: 0.1769\n",
      "  Mean improvement over greedy: -0.1613\n",
      "Run 9 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 58.26%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3786\n",
      "  Mean improvement over greedy: -0.3678\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 1.375\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -35.75\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -33.541666666666664\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -34.666666666666664\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -39.5\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -32.0\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -24.708333333333332\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -31.125\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -29.666666666666668\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -32.625\n",
      "Run 10 training metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 75.26%\n",
      "  #opt: 260/1000\n",
      "  Mean percentage error: 0.1860\n",
      "  Mean improvement over greedy: -0.1732\n",
      "Run 10 test metrics for PPO with (+v_i -w_i):\n",
      "  Val/Opt Ratio: 39.45%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4747\n",
      "  Mean improvement over greedy: -0.4483\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for PPO with (+v_i -w_i):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 75.79% ± 0.29%\n",
      "  #opt: 284.50 ± 14.56\n",
      "  Mean percentage error: 0.1781 ± 0.0040\n",
      "  Mean improvement over greedy: -0.1630 ± 0.0047\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 55.17% ± 11.70%\n",
      "  #opt: 0.60 ± 0.66\n",
      "  Mean percentage error: 0.3766 ± 0.1243\n",
      "  Mean improvement over greedy: -0.3573 ± 0.1228\n",
      "\n",
      "Experiment 15/15: Testing PPO with (+_1 -_1)\n",
      "Positive reward: _1, Negative reward: _1\n",
      "Running 10 times and averaging results...\n",
      "\n",
      "Run 1/10 (seed=42):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.6\n",
      "Run 1 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.81%\n",
      "  #opt: 275/1000\n",
      "  Mean percentage error: 0.1794\n",
      "  Mean improvement over greedy: -0.1627\n",
      "Run 1 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 54.86%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3109\n",
      "  Mean improvement over greedy: -0.2744\n",
      "\n",
      "Run 2/10 (seed=1042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.7368421052631579\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.4666666666666667\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.7368421052631579\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.7368421052631579\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.42857142857142855\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.6842105263157895\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.7894736842105263\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.7647058823529411\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.3333333333333333\n",
      "Run 2 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 76.03%\n",
      "  #opt: 289/1000\n",
      "  Mean percentage error: 0.1753\n",
      "  Mean improvement over greedy: -0.1587\n",
      "Run 2 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 46.43%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.3406\n",
      "  Mean improvement over greedy: -0.3406\n",
      "\n",
      "Run 3/10 (seed=2042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6111111111111112\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.5555555555555556\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.6111111111111112\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.7222222222222222\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7777777777777778\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.25\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.7222222222222222\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.6111111111111112\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.7777777777777778\n",
      "Run 3 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 76.72%\n",
      "  #opt: 296/1000\n",
      "  Mean percentage error: 0.1697\n",
      "  Mean improvement over greedy: -0.1527\n",
      "Run 3 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 58.88%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4574\n",
      "  Mean improvement over greedy: -0.4375\n",
      "\n",
      "Run 4/10 (seed=3042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.7435897435897436\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7419354838709677\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.7948717948717948\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.8235294117647058\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.7560975609756098\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Run 4 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.84%\n",
      "  #opt: 284/1000\n",
      "  Mean percentage error: 0.1789\n",
      "  Mean improvement over greedy: -0.1643\n",
      "Run 4 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 54.41%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.4342\n",
      "  Mean improvement over greedy: -0.4148\n",
      "\n",
      "Run 5/10 (seed=4042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.8536585365853658\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.7560975609756098\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.6\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.8048780487804879\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.2\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.5384615384615384\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.7777777777777778\n",
      "Run 5 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.47%\n",
      "  #opt: 283/1000\n",
      "  Mean percentage error: 0.1791\n",
      "  Mean improvement over greedy: -0.1654\n",
      "Run 5 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 53.27%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3614\n",
      "  Mean improvement over greedy: -0.3237\n",
      "\n",
      "Run 6/10 (seed=5042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.14285714285714285\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.625\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Run 6 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 76.14%\n",
      "  #opt: 275/1000\n",
      "  Mean percentage error: 0.1758\n",
      "  Mean improvement over greedy: -0.1598\n",
      "Run 6 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 49.20%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5062\n",
      "  Mean improvement over greedy: -0.4971\n",
      "\n",
      "Run 7/10 (seed=6042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Run 7 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 74.68%\n",
      "  #opt: 278/1000\n",
      "  Mean percentage error: 0.1860\n",
      "  Mean improvement over greedy: -0.1741\n",
      "Run 7 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 54.98%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3610\n",
      "  Mean improvement over greedy: -0.3480\n",
      "\n",
      "Run 8/10 (seed=7042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5454545454545454\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.6363636363636364\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.7272727272727273\n",
      "Run 8 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.14%\n",
      "  #opt: 269/1000\n",
      "  Mean percentage error: 0.1872\n",
      "  Mean improvement over greedy: -0.1715\n",
      "Run 8 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 78.57%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.2432\n",
      "  Mean improvement over greedy: -0.2236\n",
      "\n",
      "Run 9/10 (seed=8042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: 0.0\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Run 9 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 76.42%\n",
      "  #opt: 299/1000\n",
      "  Mean percentage error: 0.1711\n",
      "  Mean improvement over greedy: -0.1555\n",
      "Run 9 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 39.23%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5542\n",
      "  Mean improvement over greedy: -0.5434\n",
      "\n",
      "Run 10/10 (seed=9042):\n",
      "Computing DP optimal solutions for training instances...\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing DP optimal solutions for test instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running Model <class 'models.KnapsackPPO.KnapsackPPOSolver'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=10000\n",
      "Iteration [0/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Iteration [1000/10000], Training KP Instance 0, Reward: -0.09090909090909091\n",
      "Iteration [2000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [3000/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Iteration [4000/10000], Training KP Instance 0, Reward: -0.1111111111111111\n",
      "Iteration [5000/10000], Training KP Instance 0, Reward: -0.2727272727272727\n",
      "Iteration [6000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Iteration [7000/10000], Training KP Instance 0, Reward: -0.5833333333333334\n",
      "Iteration [8000/10000], Training KP Instance 0, Reward: -0.6666666666666666\n",
      "Iteration [9000/10000], Training KP Instance 0, Reward: -0.75\n",
      "Run 10 training metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 75.62%\n",
      "  #opt: 252/1000\n",
      "  Mean percentage error: 0.1828\n",
      "  Mean improvement over greedy: -0.1700\n",
      "Run 10 test metrics for PPO with (+_1 -_1):\n",
      "  Val/Opt Ratio: 49.05%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4745\n",
      "  Mean improvement over greedy: -0.4481\n",
      "\n",
      "AVERAGED RESULTS (10 runs) for PPO with (+_1 -_1):\n",
      "Training metrics:\n",
      "  Val/Opt Ratio: 75.79% ± 0.57%\n",
      "  #opt: 280.00 ± 12.97\n",
      "  Mean percentage error: 0.1785 ± 0.0055\n",
      "  Mean improvement over greedy: -0.1635 ± 0.0066\n",
      "Test metrics:\n",
      "  Val/Opt Ratio: 53.89% ± 9.76%\n",
      "  #opt: 0.70 ± 0.78\n",
      "  Mean percentage error: 0.4044 ± 0.0916\n",
      "  Mean improvement over greedy: -0.3851 ± 0.0952\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Results:\n",
      "   Model Reward Function Train Val/Opt (%)      Train #opt        Train MPE  \\\n",
      "0    A2C   (+vr_i -wr_i)      71.50 ± 1.68  234.60 ± 13.86  0.2221 ± 0.0153   \n",
      "1    A2C    (+vr_i -w_i)      74.49 ± 0.65  281.10 ± 12.75  0.1886 ± 0.0064   \n",
      "2    A2C    (+v_i -wr_i)      59.88 ± 5.06  186.70 ± 24.81  0.3249 ± 0.0506   \n",
      "3    A2C     (+v_i -w_i)      74.62 ± 0.70  270.60 ± 13.76  0.1882 ± 0.0063   \n",
      "4    A2C       (+_1 -_1)      73.79 ± 1.65  253.90 ± 14.17  0.1987 ± 0.0154   \n",
      "5    DQN   (+vr_i -wr_i)      72.95 ± 0.94  233.90 ± 11.42  0.2082 ± 0.0095   \n",
      "6    DQN    (+vr_i -w_i)      64.32 ± 1.19  203.30 ± 14.57  0.2851 ± 0.0111   \n",
      "7    DQN    (+v_i -wr_i)      56.71 ± 3.27  161.30 ± 13.13  0.3630 ± 0.0313   \n",
      "8    DQN     (+v_i -w_i)      61.15 ± 2.11  194.10 ± 13.58  0.3099 ± 0.0217   \n",
      "9    DQN       (+_1 -_1)      70.29 ± 3.76  233.70 ± 21.35  0.2282 ± 0.0330   \n",
      "10   PPO   (+vr_i -wr_i)      74.21 ± 4.08  264.00 ± 42.46  0.1961 ± 0.0452   \n",
      "11   PPO    (+vr_i -w_i)      75.79 ± 0.47  280.50 ± 11.16  0.1788 ± 0.0053   \n",
      "12   PPO    (+v_i -wr_i)      75.68 ± 0.44  278.70 ± 15.54  0.1794 ± 0.0050   \n",
      "13   PPO     (+v_i -w_i)      75.79 ± 0.29  284.50 ± 14.56  0.1781 ± 0.0040   \n",
      "14   PPO       (+_1 -_1)      75.79 ± 0.57  280.00 ± 12.97  0.1785 ± 0.0055   \n",
      "\n",
      "    Train Imp/Greedy Test Val/Opt (%)    Test #opt         Test MPE  \\\n",
      "0   -0.2070 ± 0.0155     56.19 ± 5.48  0.60 ± 0.80  0.3858 ± 0.0734   \n",
      "1   -0.1735 ± 0.0070     56.45 ± 7.90  0.70 ± 0.78  0.3843 ± 0.1137   \n",
      "2   -0.3099 ± 0.0499     52.68 ± 8.76  0.80 ± 0.75  0.4017 ± 0.0848   \n",
      "3   -0.1732 ± 0.0071     57.33 ± 8.63  0.70 ± 0.64  0.3772 ± 0.0724   \n",
      "4   -0.1837 ± 0.0155     54.01 ± 7.37  0.70 ± 0.78  0.3981 ± 0.0757   \n",
      "5   -0.1931 ± 0.0105    53.12 ± 10.20  0.70 ± 0.64  0.3932 ± 0.1051   \n",
      "6   -0.2700 ± 0.0112     59.16 ± 5.58  0.80 ± 0.60  0.3570 ± 0.0759   \n",
      "7   -0.3480 ± 0.0318     56.00 ± 7.81  0.90 ± 0.83  0.3705 ± 0.0806   \n",
      "8   -0.2949 ± 0.0229     55.51 ± 7.14  0.80 ± 0.87  0.3655 ± 0.0830   \n",
      "9   -0.2132 ± 0.0323    56.13 ± 10.54  1.00 ± 0.89  0.3672 ± 0.1206   \n",
      "10  -0.1810 ± 0.0447     53.97 ± 6.72  0.80 ± 0.75  0.3855 ± 0.0817   \n",
      "11  -0.1637 ± 0.0061     53.95 ± 8.42  0.70 ± 0.64  0.4048 ± 0.1047   \n",
      "12  -0.1644 ± 0.0059     53.18 ± 8.40  0.60 ± 0.66  0.4039 ± 0.0980   \n",
      "13  -0.1630 ± 0.0047    55.17 ± 11.70  0.60 ± 0.66  0.3766 ± 0.1243   \n",
      "14  -0.1635 ± 0.0066     53.89 ± 9.76  0.70 ± 0.78  0.4044 ± 0.0916   \n",
      "\n",
      "     Test Imp/Greedy Training Time (s)  \n",
      "0   -0.3666 ± 0.0747             31.63  \n",
      "1   -0.3650 ± 0.1077             30.55  \n",
      "2   -0.3825 ± 0.0853             33.01  \n",
      "3   -0.3579 ± 0.0730             30.39  \n",
      "4   -0.3788 ± 0.0728             31.19  \n",
      "5   -0.3739 ± 0.1088             36.75  \n",
      "6   -0.3377 ± 0.0687             37.98  \n",
      "7   -0.3512 ± 0.0806            106.00  \n",
      "8   -0.3463 ± 0.0842             37.26  \n",
      "9   -0.3480 ± 0.1188             36.92  \n",
      "10  -0.3663 ± 0.0829             57.03  \n",
      "11  -0.3856 ± 0.1051             56.15  \n",
      "12  -0.3847 ± 0.1041             56.09  \n",
      "13  -0.3573 ± 0.1228             56.33  \n",
      "14  -0.3851 ± 0.0952             56.37  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO integrate the instance generator in this code\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    N = 50\n",
    "    M = 1000\n",
    "    gamma = 0.99\n",
    "    t_max = 10**4\n",
    "    # t_max = None\n",
    "\n",
    "    env:KnapsackEnv = KnapsackEnv(problem_instance=None, N=N, pad_meta_data_at_back=False)\n",
    "    gen = KnapsackInstanceGenerator(seed=42)\n",
    "\n",
    "    problem_instances = gen.generate('RI', M=M, N=N, R=100)\n",
    "    # print(problem_instances)\n",
    "\n",
    "    KPSolver_A2C = KnapsackA2C(N=N, gamma=gamma, lr_policy=0.001, lr_value=0.001, verbose=False)\n",
    "    KPSolver_PPO = KnapsackPPOSolver(N=N, gamma=gamma, policy_lr=0.001, value_lr=0.001, verbose=False)\n",
    "    KPSolver_DQN = KnapsackDQN(N=N, gamma=gamma, lr=0.001, verbose=False)\n",
    "\n",
    "\n",
    "    results = test_reward_functions(\n",
    "        KPSolver_A2C=KPSolver_A2C,\n",
    "        # KPSolver_A2C=None,\n",
    "        KPSolver_PPO=KPSolver_PPO,\n",
    "        KPSolver_DQN=KPSolver_DQN,\n",
    "        # KPSolver_DQN=None,\n",
    "        M=M,\n",
    "        instance_type=\"RI\",\n",
    "        N=N,\n",
    "        r_range=100,\n",
    "        seed=42,\n",
    "        t_max=t_max,\n",
    "        use_state_aggregation=False,\n",
    "        n_test_instances=5,\n",
    "        verbose=True,\n",
    "        n_runs=10\n",
    "    )\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(results['summary'])\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_results_with_error_bars(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
