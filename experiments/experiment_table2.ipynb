{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import go here\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "\n",
    "from environment.knapsackgym import KnapsackEnv, _1_positive_reward, _1_negative_reward, v_i_positive_reward, vr_i_positive_reward, w_i_negative_reward, wr_i_negative_reward\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from models.DP_Knapsack_float import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "from models.Greedy_Knapsack import solve_problem_instances_greedy\n",
    "from models.KnapsackPPO import KnapsackPPOSolver\n",
    "from models.KnapsackA2C import KnapsackA2C\n",
    "from models.KnapsackQLearning import KnapsackDQN\n",
    "from util.instance_gen import KnapsackInstanceGenerator\n",
    "from util.metrics import evaluate_knapsack_performance\n",
    "from models.KnapsackDRLSolver import KnapsackDRLSolver, run_KPSolver\n",
    "from models.StateAggregator import StateAggregator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Tuple, Callable\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    KPSolver_A2C: KnapsackA2C,\n",
    "    KPSolver_DQN: KnapsackDQN,\n",
    "    KPSolver_PPO: KnapsackPPOSolver,\n",
    "    instance_type,\n",
    "    N,\n",
    "    t_max,\n",
    "    verbose=False):\n",
    "\n",
    "    M = 1000\n",
    "    seed = 42\n",
    "    r_range = 100\n",
    "    n_test_instances = 5\n",
    "\n",
    "    # Generate problem instances\n",
    "    gen = KnapsackInstanceGenerator(seed=seed)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Generating {M} {instance_type} training instances with N={N}, R={r_range}\")\n",
    "\n",
    "    if instance_type == \"RI\":\n",
    "        training_instances = gen.generate_random_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_random_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    elif instance_type == \"FI\":\n",
    "        training_instances = gen.generate_fixed_instances(M, N, seed=seed)\n",
    "        test_instances = gen.generate_fixed_instances(n_test_instances, N, seed=seed+100)\n",
    "    elif instance_type == \"HI\":\n",
    "        training_instances = gen.generate_hard_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_hard_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    elif instance_type == \"SS\":\n",
    "        training_instances = gen.generate_subset_sum_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_subset_sum_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown instance type: {instance_type}\")\n",
    "    \n",
    "    # Solve instances with DP and Greedy for baselines\n",
    "    if verbose: print(\"Computing DP optimal solutions for training instances...\")\n",
    "    dp_sols_items_train, dp_values_train, dp_weight_train = solve_KP_instances_with_DP(training_instances)\n",
    "\n",
    "    if verbose: print(\"Computing Greedy solutions for training instances...\")\n",
    "    greedy_values_train, greedy_sols_items_train, greedy_weights_train = solve_problem_instances_greedy(training_instances)\n",
    "    \n",
    "    if verbose: print(\"Computing DP optimal solutions for test instances...\")\n",
    "    dp_sols_items_test, dp_values_test, dp_weight_test = solve_KP_instances_with_DP(test_instances)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Computing Greedy solutions for test instances...\")\n",
    "    greedy_values_test, greedy_sols_items_test, greedy_weights_test = solve_problem_instances_greedy(test_instances)\n",
    "    \n",
    "    # Define models to test\n",
    "    model_constructors = {}\n",
    "    if KPSolver_A2C is not None: model_constructors[\"A2C\"] = KPSolver_A2C\n",
    "    if KPSolver_DQN is not None: model_constructors[\"DQN\"] = KPSolver_DQN\n",
    "    if KPSolver_PPO is not None: model_constructors[\"PPO\"] = KPSolver_PPO\n",
    "\n",
    "    aggregation_constructors = [True, False]\n",
    "\n",
    "    results = {\n",
    "        'training': {},\n",
    "        'test': {},\n",
    "        'metrics': {},\n",
    "        'config': {\n",
    "            'num_instances': M,\n",
    "            'instance_type': instance_type,\n",
    "            'n_items': N,\n",
    "            'r_range': r_range,\n",
    "            'seed': seed,\n",
    "            't_max': t_max,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Total count of experiments\n",
    "    total_experiments = len(aggregation_constructors) * len(model_constructors)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Running {total_experiments} experiments...\")\n",
    "\n",
    "    experiment_counter = 0\n",
    "    \n",
    "    # Run experiments for each model and reward function combination\n",
    "    for model_name, model in model_constructors.items():\n",
    "        results['training'][model_name] = {}\n",
    "        results['test'][model_name] = {}\n",
    "        results['metrics'][model_name] = {}\n",
    "        \n",
    "        for aggr_policy in aggregation_constructors:\n",
    "            experiment_counter += 1\n",
    "            aggr_desc = \"with state aggregation\" if aggr_policy else \"without state aggregation\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nExperiment {experiment_counter}/{total_experiments}: Testing {model_name} {aggr_desc}\")\n",
    "            \n",
    "            # Create environment with specific reward functions\n",
    "            env = KnapsackEnv(\n",
    "                problem_instance=None,\n",
    "                N=N\n",
    "            )\n",
    "            \n",
    "            # Initialize the model\n",
    "            kp_solver = model\n",
    "            \n",
    "            # Train the model\n",
    "            start_time = time.time()\n",
    "            \n",
    "            solver, solution_values = run_KPSolver(\n",
    "                env=env,\n",
    "                KPSolver=kp_solver,\n",
    "                training_problem_instances=training_instances,\n",
    "                t_max=t_max,\n",
    "                use_state_aggregation=aggr_policy,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Store training results\n",
    "            results['training'][model_name][aggr_desc] = {\n",
    "                'solution_values': solution_values,\n",
    "                'training_time': training_time\n",
    "            }\n",
    "            \n",
    "            # Evaluate on test instances\n",
    "            test_values = []\n",
    "            for instance in test_instances:\n",
    "                env.change_problem_instance(instance)\n",
    "                # value, weight, _ = kp_solver.solve(instance)\n",
    "                value, weight, _ = solver.solve(instance)\n",
    "                test_values.append(value)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            \n",
    "            # For training instances\n",
    "            train_best_values = solution_values['instance_best_values']\n",
    "            train_metrics = evaluate_knapsack_performance(\n",
    "                train_best_values, \n",
    "                dp_values_train, \n",
    "                greedy_values_train\n",
    "            )\n",
    "            \n",
    "            # For test instances\n",
    "            test_metrics = evaluate_knapsack_performance(\n",
    "                test_values,\n",
    "                dp_values_test,\n",
    "                greedy_values_test\n",
    "            )\n",
    "            \n",
    "            # Store test results and metrics\n",
    "            results['test'][model_name][aggr_desc] = {\n",
    "                'values': test_values,\n",
    "                'metrics': test_metrics\n",
    "            }\n",
    "            \n",
    "            results['metrics'][model_name][aggr_desc] = {\n",
    "                'train': train_metrics,\n",
    "                'test': test_metrics\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Training metrics for {model_name} with {aggr_desc}:\")\n",
    "                print(f\"  Val/Opt Ratio: {train_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {train_metrics['#opt']}/{M}\")\n",
    "                print(f\"  Mean percentage error: {train_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {train_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "                \n",
    "                print(f\"Test metrics for {model_name} with {aggr_desc}:\")\n",
    "                print(f\"  Val/Opt Ratio: {test_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {test_metrics['#opt']}/{n_test_instances}\")\n",
    "                print(f\"  Mean percentage error: {test_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {test_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "\n",
    "        # Generate summary table\n",
    "        summary = create_summary_table(results)\n",
    "        results['summary'] = summary\n",
    "        \n",
    "        # Generate visualizations\n",
    "        # visualize_results(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def create_summary_table(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a summary table of all experiments.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from test_reward_functions\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Summary table\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name in results['metrics']:\n",
    "        for reward_combo_name, metrics in results['metrics'][model_name].items():\n",
    "            train_metrics = metrics['train']\n",
    "            test_metrics = metrics['test']\n",
    "            \n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Method': reward_combo_name,\n",
    "                'Train_ValOptRatio': train_metrics['ValOptRatio'],\n",
    "                'GreedyOptRatio': train_metrics['ValGROptRatio'],\n",
    "                'AveragedVal': train_metrics['AveragedVal'],\n",
    "                'AveragedGrVal': train_metrics['AveragedGrVal'],\n",
    "                'Train_#opt': train_metrics['#opt'],\n",
    "                'Test_#opt': test_metrics['#opt'],\n",
    "                'Greedy_#opt': test_metrics['#gr_opt'],\n",
    "                'Train_#highest': test_metrics['#highest'],\n",
    "                'Test_ValOptRatio': test_metrics['ValOptRatio']\n",
    "            }\n",
    "\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\"RI\", \"FI\", \"HI\", \"SS\"]\n",
    "Ns = [50, 300, 500]\n",
    "t_max = 10000\n",
    "\n",
    "# results = dict()\n",
    "# for instance in instances:\n",
    "#     results[instance] = dict()\n",
    "#     for N in Ns:\n",
    "#         KPSolver_A2C = KnapsackA2C(N=N, gamma=0.99, lr_policy=0.001, lr_value=0.001, verbose=True)\n",
    "#         results[instance][N] = run_experiment(\n",
    "#             KPSolver_A2C,\n",
    "#             None,\n",
    "#             None,\n",
    "#             instance,\n",
    "#             N,\n",
    "#             t_max,\n",
    "#             verbose=True\n",
    "#         )\n",
    "\n",
    "i, j = 3, 2\n",
    "\n",
    "KPSolver_A2C = KnapsackA2C(N=Ns[j], gamma=0.99, lr_policy=0.001, lr_value=0.001, verbose=True)\n",
    "results = run_experiment(\n",
    "    KPSolver_A2C,\n",
    "    None,\n",
    "    None,\n",
    "    instances[i],\n",
    "    Ns[j],\n",
    "    t_max,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instances[i], Ns[j])\n",
    "print(results['summary'])\n",
    "plt.title('Random Instances', weight='bold')\n",
    "sum_aggr = results['training']['A2C']['with state aggregation']['solution_values']['best_sum_over_time']\n",
    "sum_no_aggr = results['training']['A2C']['without state aggregation']['solution_values']['best_sum_over_time']\n",
    "plt.plot(sum_aggr, label='DRL w/ Aggregation', color=\"dodgerblue\")\n",
    "plt.plot(sum_no_aggr, label='DRL w/o Aggregation', color=\"red\")\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Sum of values of the solutions')\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
