{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import go here\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "\n",
    "from environment.knapsackgym import KnapsackEnv, _1_positive_reward, _1_negative_reward, v_i_positive_reward, vr_i_positive_reward, w_i_negative_reward, wr_i_negative_reward\n",
    "from typing import List, Callable, Optional, Union, Tuple, Dict, Any\n",
    "from models.DP_Knapsack import solve_knapsack_dp, solve_KP_instances_with_DP\n",
    "from models.Greedy_Knapsack import solve_problem_instances_greedy\n",
    "from models.KnapsackPPO import KnapsackPPOSolver\n",
    "from models.KnapsackA2C import KnapsackA2C\n",
    "from models.KnapsackQLearning import KnapsackDQN\n",
    "from util.instance_gen import KnapsackInstanceGenerator\n",
    "from util.metrics import evaluate_knapsack_performance\n",
    "from models.KnapsackDRLSolver import KnapsackDRLSolver, run_KPSolver\n",
    "from models.StateAggregator import StateAggregator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Tuple, Callable\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    KPSolver_A2C: KnapsackA2C,\n",
    "    KPSolver_DQN: KnapsackDQN,\n",
    "    KPSolver_PPO: KnapsackPPOSolver,\n",
    "    instance_type,\n",
    "    N,\n",
    "    t_max,\n",
    "    verbose=False):\n",
    "\n",
    "    M = 1000\n",
    "    seed = 42\n",
    "    r_range = 100\n",
    "    n_test_instances = 200\n",
    "\n",
    "    # Generate problem instances\n",
    "    gen = KnapsackInstanceGenerator(seed=seed)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Generating {M} {instance_type} training instances with N={N}, R={r_range}\")\n",
    "\n",
    "    if instance_type == \"RI\":\n",
    "        training_instances = gen.generate_random_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_random_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    elif instance_type == \"FI\":\n",
    "        training_instances = gen.generate_fixed_instances(M, N, seed=seed)\n",
    "        test_instances = gen.generate_fixed_instances(n_test_instances, N, seed=seed+100)\n",
    "    elif instance_type == \"HI\":\n",
    "        training_instances = gen.generate_hard_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_hard_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    elif instance_type == \"SS\":\n",
    "        training_instances = gen.generate_subset_sum_instances(M, N, r_range, seed=seed)\n",
    "        test_instances = gen.generate_subset_sum_instances(n_test_instances, N, r_range, seed=seed+100)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown instance type: {instance_type}\")\n",
    "    \n",
    "    # Solve instances with DP and Greedy for baselines\n",
    "    # if verbose: print(\"Computing DP optimal solutions for training instances...\")\n",
    "    # dp_sols_items_train, dp_values_train, dp_weight_train = solve_KP_instances_with_DP(training_instances)\n",
    "\n",
    "    if verbose: print(\"Computing Greedy solutions for training instances...\")\n",
    "    greedy_values_train, greedy_sols_items_train, greedy_weights_train = solve_problem_instances_greedy(training_instances)\n",
    "    \n",
    "    # if verbose: print(\"Computing DP optimal solutions for test instances...\")\n",
    "    # dp_sols_items_test, dp_values_test, dp_weight_test = solve_KP_instances_with_DP(test_instances)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Computing Greedy solutions for test instances...\")\n",
    "    greedy_values_test, greedy_sols_items_test, greedy_weights_test = solve_problem_instances_greedy(test_instances)\n",
    "    \n",
    "    # Define models to test\n",
    "    model_constructors = {}\n",
    "    if KPSolver_A2C is not None: model_constructors[\"A2C\"] = KPSolver_A2C\n",
    "    if KPSolver_DQN is not None: model_constructors[\"DQN\"] = KPSolver_DQN\n",
    "    if KPSolver_PPO is not None: model_constructors[\"PPO\"] = KPSolver_PPO\n",
    "\n",
    "    aggregation_constructors = [True, False]\n",
    "\n",
    "    results = {\n",
    "        'training': {},\n",
    "        'test': {},\n",
    "        'metrics': {},\n",
    "        'config': {\n",
    "            'num_instances': M,\n",
    "            'instance_type': instance_type,\n",
    "            'n_items': N,\n",
    "            'r_range': r_range,\n",
    "            'seed': seed,\n",
    "            't_max': t_max,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Total count of experiments\n",
    "    total_experiments = len(aggregation_constructors) * len(model_constructors)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Running {total_experiments} experiments...\")\n",
    "\n",
    "    experiment_counter = 0\n",
    "    \n",
    "    # Run experiments for each model and reward function combination\n",
    "    for model_name, model in model_constructors.items():\n",
    "        results['training'][model_name] = {}\n",
    "        results['test'][model_name] = {}\n",
    "        results['metrics'][model_name] = {}\n",
    "        \n",
    "        for aggr_policy in aggregation_constructors:\n",
    "            experiment_counter += 1\n",
    "            aggr_desc = \"with state aggregation\" if aggr_policy else \"without state aggregation\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nExperiment {experiment_counter}/{total_experiments}: Testing {model_name} {aggr_desc}\")\n",
    "            \n",
    "            # Create environment with specific reward functions\n",
    "            env = KnapsackEnv(\n",
    "                problem_instance=None,\n",
    "                N=N\n",
    "            )\n",
    "            \n",
    "            # Initialize the model\n",
    "            kp_solver = model\n",
    "            \n",
    "            # Train the model\n",
    "            start_time = time.time()\n",
    "            \n",
    "            solver, solution_values = run_KPSolver(\n",
    "                env=env,\n",
    "                KPSolver=kp_solver,\n",
    "                training_problem_instances=training_instances,\n",
    "                t_max=t_max,\n",
    "                use_state_aggregation=aggr_policy,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Store training results\n",
    "            results['training'][model_name][aggr_desc] = {\n",
    "                'solution_values': solution_values,\n",
    "                'training_time': training_time\n",
    "            }\n",
    "            \n",
    "            # Evaluate on test instances\n",
    "            test_values = []\n",
    "            for instance in test_instances:\n",
    "                env.change_problem_instance(instance)\n",
    "                # value, weight, _ = kp_solver.solve(instance)\n",
    "                value, weight, _ = solver.solve(instance)\n",
    "                test_values.append(value)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            \n",
    "            # For training instances\n",
    "            train_best_values = solution_values['instance_best_values']\n",
    "            train_metrics = evaluate_knapsack_performance(\n",
    "                train_best_values, \n",
    "                # dp_values_train, \n",
    "                greedy_values_train, \n",
    "                greedy_values_train\n",
    "            )\n",
    "            \n",
    "            # For test instances\n",
    "            test_metrics = evaluate_knapsack_performance(\n",
    "                test_values,\n",
    "                # dp_values_test,\n",
    "                greedy_values_test,\n",
    "                greedy_values_test\n",
    "            )\n",
    "            \n",
    "            # Store test results and metrics\n",
    "            results['test'][model_name][aggr_desc] = {\n",
    "                'values': test_values,\n",
    "                'metrics': test_metrics\n",
    "            }\n",
    "            \n",
    "            results['metrics'][model_name][aggr_desc] = {\n",
    "                'train': train_metrics,\n",
    "                'test': test_metrics\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Training metrics for {model_name} with {aggr_desc}:\")\n",
    "                print(f\"  Val/Opt Ratio: {train_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {train_metrics['#opt']}/{M}\")\n",
    "                print(f\"  Mean percentage error: {train_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {train_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "                \n",
    "                print(f\"Test metrics for {model_name} with {aggr_desc}:\")\n",
    "                print(f\"  Val/Opt Ratio: {test_metrics['ValOptRatio']:.2f}%\")\n",
    "                print(f\"  #opt: {test_metrics['#opt']}/{n_test_instances}\")\n",
    "                print(f\"  Mean percentage error: {test_metrics['mean_percentage_error']:.4f}\")\n",
    "                print(f\"  Mean improvement over greedy: {test_metrics['mean_improvement_over_greedy']:.4f}\")\n",
    "\n",
    "        # Generate summary table\n",
    "        summary = create_summary_table(results)\n",
    "        results['summary'] = summary\n",
    "        \n",
    "        # Generate visualizations\n",
    "        # visualize_results(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def create_summary_table(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a summary table of all experiments.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from test_reward_functions\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Summary table\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name in results['metrics']:\n",
    "        for reward_combo_name, metrics in results['metrics'][model_name].items():\n",
    "            train_metrics = metrics['train']\n",
    "            test_metrics = metrics['test']\n",
    "            \n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Reward': reward_combo_name,\n",
    "                'Train_ValOptRatio': train_metrics['ValOptRatio'],\n",
    "                'Train_#opt': train_metrics['#opt'],\n",
    "                'Train_vs_Greedy': train_metrics['mean_improvement_over_greedy'],\n",
    "                'Test_ValOptRatio': test_metrics['ValOptRatio'],\n",
    "                'Test_#opt': test_metrics['#opt'],\n",
    "                'Test_MAE': test_metrics['mean_absolute_error'],\n",
    "                'Test_MPE': test_metrics['mean_percentage_error'],\n",
    "                'Test_vs_Greedy': test_metrics['mean_improvement_over_greedy']\n",
    "            }\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 RI training instances with N=50, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/m1ke-l/Data/McGill/COMP579/Project/COMP579FINALPROJECT/models/StateAggregator.py:143: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  top = (bucket_size ** action) * (self.features.shape[0] - (bucket_size * action))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 4.99%\n",
      "  #opt: 20/1000\n",
      "  Mean percentage error: 0.9337\n",
      "  Mean improvement over greedy: -0.9328\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 45.57%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.3432\n",
      "  Mean improvement over greedy: -0.3432\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.005276762817043792\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 4.94%\n",
      "  #opt: 20/1000\n",
      "  Mean percentage error: 0.9348\n",
      "  Mean improvement over greedy: -0.9348\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 75.52%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.1675\n",
      "  Mean improvement over greedy: -0.1062\n",
      "Generating 1000 RI training instances with N=300, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -2.3580246913580245\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 2.74%\n",
      "  #opt: 2/1000\n",
      "  Mean percentage error: 0.9666\n",
      "  Mean improvement over greedy: -0.9666\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 36.14%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4938\n",
      "  Mean improvement over greedy: -0.4938\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -1.7530550257822985\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 2.66%\n",
      "  #opt: 2/1000\n",
      "  Mean percentage error: 0.9679\n",
      "  Mean improvement over greedy: -0.9679\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 42.01%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.4323\n",
      "  Mean improvement over greedy: -0.4323\n",
      "Generating 1000 RI training instances with N=500, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.16066079528300647\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 2.08%\n",
      "  #opt: 1/1000\n",
      "  Mean percentage error: 0.9727\n",
      "  Mean improvement over greedy: -0.9727\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 22.43%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.6584\n",
      "  Mean improvement over greedy: -0.6584\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.19915406483470394\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 2.07%\n",
      "  #opt: 2/1000\n",
      "  Mean percentage error: 0.9716\n",
      "  Mean improvement over greedy: -0.9716\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 19.73%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.7076\n",
      "  Mean improvement over greedy: -0.7076\n",
      "Generating 1000 FI training instances with N=50, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: 0.061486049350840795\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 6.92%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9312\n",
      "  Mean improvement over greedy: -0.9312\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 67.22%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3292\n",
      "  Mean improvement over greedy: -0.3292\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: 0.05249908818934364\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 6.89%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9316\n",
      "  Mean improvement over greedy: -0.9316\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 69.74%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.3022\n",
      "  Mean improvement over greedy: -0.3022\n",
      "Generating 1000 FI training instances with N=300, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: 0.0051466887161265755\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 4.67%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9533\n",
      "  Mean improvement over greedy: -0.9533\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 41.81%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5816\n",
      "  Mean improvement over greedy: -0.5816\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: 0.005547630124920178\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 4.60%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9541\n",
      "  Mean improvement over greedy: -0.9541\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 45.21%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.5476\n",
      "  Mean improvement over greedy: -0.5476\n",
      "Generating 1000 FI training instances with N=500, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: 0.008252322981599742\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 3.60%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9642\n",
      "  Mean improvement over greedy: -0.9642\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 38.22%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.6177\n",
      "  Mean improvement over greedy: -0.6177\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: 0.002145705461228302\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 3.59%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9642\n",
      "  Mean improvement over greedy: -0.9642\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 34.77%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.6520\n",
      "  Mean improvement over greedy: -0.6520\n",
      "Generating 1000 HI training instances with N=50, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -23.683982430453888\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 1.11%\n",
      "  #opt: 4/1000\n",
      "  Mean percentage error: 0.9195\n",
      "  Mean improvement over greedy: -0.9172\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 97.89%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0551\n",
      "  Mean improvement over greedy: -0.0453\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -23.68398243045388\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 1.12%\n",
      "  #opt: 5/1000\n",
      "  Mean percentage error: 0.9178\n",
      "  Mean improvement over greedy: -0.9161\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 97.58%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0621\n",
      "  Mean improvement over greedy: -0.0534\n",
      "Generating 1000 HI training instances with N=300, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -3.98158269032922\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 1.00%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9344\n",
      "  Mean improvement over greedy: -0.9344\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 95.18%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0756\n",
      "  Mean improvement over greedy: -0.0756\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -3.9892654320987657\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 1.00%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9347\n",
      "  Mean improvement over greedy: -0.9347\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 95.15%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0750\n",
      "  Mean improvement over greedy: -0.0750\n",
      "Generating 1000 HI training instances with N=500, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -2.3936216473575613\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 0.98%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9367\n",
      "  Mean improvement over greedy: -0.9367\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 94.94%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0756\n",
      "  Mean improvement over greedy: -0.0756\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -2.392410784663809\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 0.98%\n",
      "  #opt: 0/1000\n",
      "  Mean percentage error: 0.9366\n",
      "  Mean improvement over greedy: -0.9366\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 94.69%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0781\n",
      "  Mean improvement over greedy: -0.0781\n",
      "Generating 1000 SS training instances with N=50, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.019619326500732063\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 9.98%\n",
      "  #opt: 13/1000\n",
      "  Mean percentage error: 0.9003\n",
      "  Mean improvement over greedy: -0.9000\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 100.13%\n",
      "  #opt: 0/5\n",
      "  Mean percentage error: 0.0028\n",
      "  Mean improvement over greedy: 0.0011\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=50, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.019677891654465592\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 9.98%\n",
      "  #opt: 40/1000\n",
      "  Mean percentage error: 0.9002\n",
      "  Mean improvement over greedy: -0.9000\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 100.44%\n",
      "  #opt: 1/5\n",
      "  Mean percentage error: 0.0049\n",
      "  Mean improvement over greedy: 0.0042\n",
      "Generating 1000 SS training instances with N=300, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.0028646301287810726\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 9.97%\n",
      "  #opt: 64/1000\n",
      "  Mean percentage error: 0.9000\n",
      "  Mean improvement over greedy: -0.9000\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 99.99%\n",
      "  #opt: 2/5\n",
      "  Mean percentage error: 0.0001\n",
      "  Mean improvement over greedy: -0.0001\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=300, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.003263227513227513\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 9.97%\n",
      "  #opt: 62/1000\n",
      "  Mean percentage error: 0.9000\n",
      "  Mean improvement over greedy: -0.9000\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 99.99%\n",
      "  #opt: 3/5\n",
      "  Mean percentage error: 0.0001\n",
      "  Mean improvement over greedy: -0.0001\n",
      "Generating 1000 SS training instances with N=500, R=100\n",
      "Computing Greedy solutions for training instances...\n",
      "Computing Greedy solutions for test instances...\n",
      "Running 2 experiments...\n",
      "\n",
      "Experiment 1/2: Testing A2C with state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.00031471494491739873\n",
      "Training metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 9.99%\n",
      "  #opt: 82/1000\n",
      "  Mean percentage error: 0.9000\n",
      "  Mean improvement over greedy: -0.9000\n",
      "Test metrics for A2C with with state aggregation:\n",
      "  Val/Opt Ratio: 100.00%\n",
      "  #opt: 4/5\n",
      "  Mean percentage error: 0.0000\n",
      "  Mean improvement over greedy: -0.0000\n",
      "\n",
      "Experiment 2/2: Testing A2C without state aggregation\n",
      "Running Model <class 'models.KnapsackA2C.KnapsackA2C'>\n",
      "Training on 1000 KP Instances, with N=500, t_max=100\n",
      "Iteration [0/100], Training KP Instance 0, Reward: -0.0019592128233613714\n",
      "Training metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 9.99%\n",
      "  #opt: 82/1000\n",
      "  Mean percentage error: 0.9000\n",
      "  Mean improvement over greedy: -0.9000\n",
      "Test metrics for A2C with without state aggregation:\n",
      "  Val/Opt Ratio: 100.00%\n",
      "  #opt: 5/5\n",
      "  Mean percentage error: 0.0000\n",
      "  Mean improvement over greedy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "instances = [\"RI\", \"FI\", \"HI\", \"SS\"]\n",
    "Ns = [50, 300, 500]\n",
    "t_max = 100\n",
    "\n",
    "results = dict()\n",
    "for instance in instances:\n",
    "    results[instance] = dict()\n",
    "    for N in Ns:\n",
    "        KPSolver_A2C = KnapsackA2C(N=N, gamma=0.99, lr_policy=0.001, lr_value=0.001, verbose=True)\n",
    "        results[instance][N] = run_experiment(\n",
    "            KPSolver_A2C,\n",
    "            None,\n",
    "            None,\n",
    "            instance,\n",
    "            N,\n",
    "            t_max,\n",
    "            verbose=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI 50\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           4.990742          20    404.425   \n",
      "1   A2C  without state aggregation           4.943981          20    404.528   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.933705        -0.932848         45.572476          1     193.0   \n",
      "1   0.934765        -0.934765         75.521715          1      96.8   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.343203       -0.343203  \n",
      "1  0.167463       -0.106228  \n",
      "RI 300\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           2.736754           2   1033.386   \n",
      "1   A2C  without state aggregation           2.662399           2   1034.176   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.966648        -0.966648         36.144578          1     328.6   \n",
      "1   0.967859        -0.967859         42.013214          1     298.4   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.493769       -0.493769  \n",
      "1  0.432308       -0.432308  \n",
      "RI 500\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           2.079398           1   1306.441   \n",
      "1   A2C  without state aggregation           2.074976           2   1306.500   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.972750        -0.972750         22.426712          0    1180.2   \n",
      "1   0.971581        -0.971581         19.731826          0    1221.2   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.658370       -0.658370  \n",
      "1  0.707631       -0.707631  \n",
      "FI 50\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           6.922766           0  18.628726   \n",
      "1   A2C  without state aggregation           6.886453           0  18.635994   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.931207        -0.931207         67.217723          0  6.612173   \n",
      "1   0.931606        -0.931606         69.743295          0  6.102766   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.329157       -0.329157  \n",
      "1  0.302191       -0.302191  \n",
      "FI 300\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           4.671853           0  82.327277   \n",
      "1   A2C  without state aggregation           4.595938           0  82.392839   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt   Test_MAE  \\\n",
      "0   0.953301        -0.953301         41.809467          0  48.777780   \n",
      "1   0.954078        -0.954078         45.214083          0  45.923886   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.581553       -0.581553  \n",
      "1  0.547616       -0.547616  \n",
      "FI 500\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt   Train_MAE  \\\n",
      "0   A2C     with state aggregation           3.599382           0  107.421641   \n",
      "1   A2C  without state aggregation           3.591879           0  107.430001   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt   Test_MAE  \\\n",
      "0   0.964152        -0.964152         38.221824          0  69.691277   \n",
      "1   0.964240        -0.964240         34.765193          0  73.590664   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.617745       -0.617745  \n",
      "1  0.651961       -0.651961  \n",
      "HI 50\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           1.105886           4   1537.564   \n",
      "1   A2C  without state aggregation           1.122610           5   1537.290   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.919532        -0.917208         97.885235          0      52.4   \n",
      "1   0.917849        -0.916054         97.575758          0      55.6   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.055110       -0.045307  \n",
      "1  0.062057       -0.053390  \n",
      "HI 300\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           0.995429           0    9432.91   \n",
      "1   A2C  without state aggregation           0.996688           0    9432.79   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.934423        -0.934423         95.182589          0     472.8   \n",
      "1   0.934661        -0.934661         95.154059          0     475.6   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.075557       -0.075557  \n",
      "1  0.075037       -0.075037  \n",
      "HI 500\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           0.980858           0  15754.503   \n",
      "1   A2C  without state aggregation           0.978281           0  15754.913   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.936693        -0.936693         94.938881          0     819.8   \n",
      "1   0.936645        -0.936645         94.691937          0     859.8   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.075613       -0.075613  \n",
      "1  0.078103       -0.078103  \n",
      "SS 50\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           9.979493          13   1132.417   \n",
      "1   A2C  without state aggregation           9.980844          40   1132.360   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.900259        -0.900010        100.129011          0       3.6   \n",
      "1   0.900212        -0.899995        100.435414          1       6.2   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.002781        0.001105  \n",
      "1  0.004852        0.004165  \n",
      "SS 300\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           9.974738          64   6819.801   \n",
      "1   A2C  without state aggregation           9.974711          62   6819.801   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.900006             -0.9         99.992248          2       0.6   \n",
      "1   0.900006             -0.9         99.994832          3       0.4   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.000077       -0.000077  \n",
      "1  0.000053       -0.000053  \n",
      "SS 500\n",
      "  Model                     Reward  Train_ValOptRatio  Train_#opt  Train_MAE  \\\n",
      "0   A2C     with state aggregation           9.987864          82  11364.675   \n",
      "1   A2C  without state aggregation           9.987848          82  11364.677   \n",
      "\n",
      "   Train_MPE  Train_vs_Greedy  Test_ValOptRatio  Test_#opt  Test_MAE  \\\n",
      "0   0.900001        -0.900001         99.996884          4       0.4   \n",
      "1   0.900002        -0.900001        100.000000          5       0.0   \n",
      "\n",
      "   Test_MPE  Test_vs_Greedy  \n",
      "0  0.000032       -0.000032  \n",
      "1  0.000000        0.000000  \n"
     ]
    }
   ],
   "source": [
    "for instance in instances:\n",
    "    for N in Ns:\n",
    "        print(instance, N)\n",
    "        print(results[instance][N][\"summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
